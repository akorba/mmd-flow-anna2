

\section{Introduction}

%OLD INTRO ON LANGEVIN MONTE CARLO
%This paper deals with the problem of sampling from a probability measure $\mu$ on $(\R^d,\mathcal{B}(\R^d))$ which admits a density, still denoted by $\mu$, with respect to the Lebesgue measure.
%This problem appears in machine learning, Bayesian inference, computational physics... Classical methods to tackle this issue are Markov Chain Monte Carlo methods, for instance Metropolis-Hastings algorithm, Gibbs sampling. The main drawback of these methods is that one needs to choose an appropriate proposal distribution, which is not trivial. Consequently, other algorithms based on continuous dynamics have been proposed, such as the over-damped Langevin diffusion:
%\begin{equation}\label{eq:langevin_diffusion}
%dX_t= -\nabla \log \mu (X_t)dt+\sqrt{2}dB_t
%\end{equation}
%where $(B_t)_{t\ge0}$ is a $d$-dimensional Brownian motion. The Langevin Monte-Carlo (LMC) algorithm, or Unadjusted Langevin algorithm (ULA) considers the Markov chain $(X_k)_{k\ge1 }$ given by the Euler-Maruyama discretization of the diffusion \eqref{eq:langevin_diffusion}:
%\begin{equation}\label{eq:langevin_algorithm}
%X_{k+1} = X_k - \gamma_{k+1}\nabla \log \mu(X_k) + \sqrt{2\gamma_{k+1}G_{k+1}}
%\end{equation}
%where $(\gamma_k)_{k\ge1}$ is a sequence of step sizes (constant or convergent to zero), and
%$(G_k)_{k \ge 1}$ is a sequence of i.i.d. standard $d$-dimensional Gaussian random variables. This algorithm has attracted a lot of attention... But....\aknote{say something about the requirement of the knowledge of gradient of log target and how it is difficult to estimate?}

%Neural networks with a large number of parameters Theoretical explanation of 
%gradient descent is a solution of a PDE that converges to a globally optimal solution for networks with a single hidden layer under appropriate assumptions. \cite{rotskoff2019global} propose a Birth-Death dynamics that leads to a modified PDE with the same
%minimizer.


%Recently, using mathematical tools from optimal transport theory and interacting particle systems, it was shown that gradient descent [RVE18, MMN18, SS18, CB18b] and stochastic gradient descent converge asymptotically to the target function in the large data limit


Optimal transport theory provides a
powerful conceptual and mathematical framework for gradient flows on the space of distributions, and has thus found numerous applications in statistics and machine learning. A seminal work is surely the one of \cite{jordan1998variational}, who revealed that the Fokkerâ€“Planck equation is a gradient
flow equation for a the relative entropy functional (also known as the KL-divergence) with respect to the Wassertein metric. This consideration has spawned a range of sampling
algorithms, based on time discretizations of the gradient flow equation. Indeed, under appropriate conditions on the coefficient of the equation, it can be shown that its stationary distribution is unique and is the target distribution $\pi$ (see for instance \cite{pavliotis2011stochastic}, Chapter 4). In particular, the Unadjusted
Langevin Algorithm (ULA) and its Metropolis adjusted counterpart MALA have received much
attention \cite{durmus2018analysis,bernton2018langevin}). Recently, gradient flows and interacting particle systems were also used to explain the convergence of gradient descent to a globally optimal solution for networks with a single hidden layer under appropriate assumptions (see \cite{chizat2018global}). Indeed, gradient descent optimization dynamics is described by a partial differential equation (PDE), corresponding to a Wasserstein gradient flow of a convex energy functional. In this setting, the parameters of the neural networks can be seen as particles propagating in the direction of this gradient flow. \cite{rotskoff2019global} propose a Birth-Death dynamics that leads to a modified PDE which is shown to accelerate this convergence.


In this paper, we study the gradient flow equation in the Wasserstein space of order 2 associated with another free energy functional, the Maximum Mean Discrepancy (MMD), introduced in \cite{gretton2012kernel}. \aknote{to complete. state our contribution.}
%To the best of our knowledge, this work is the first one which investigates the flow of a discrepancy between distributions different from the Kullback-Leibler divergence. 
\asnote{We will probably reviewed by Mroueh so we need to compare their results to ours}

This paper is organized as follows. In \cref{sec:preliminaries}, definitions and mathematical background needed in the paper are introduced, and \cref{sec:mmd_flow} is devoted to deriving the MMD flow and reviewing its theoretical properties. 
\cref{sec:discretized_flow} investigates at length the discretized versions (in time and space) of this flow. 