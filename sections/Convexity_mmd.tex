


\section{Lambda displacement convexity of the MMD}
In all what follows we consider a fixed target distribution $\mu$ and define the following functional 
\begin{align}\label{eq:MMD_functional}
	\nu \mapsto \F(\nu):=\frac{1}{2} MMD^2(\mu,\nu)
\end{align}. 

We investigate some theoretical properties of the MMD flow. In particular, we are interested in characterizing the convergence of the time discretized flow:
	\begin{align}\label{eq:discretized_flow}
		\nu_{n+1} = (I -\gamma \phi_n)_{\#}\nu_n
	\end{align}
where $\gamma$ is some fixed step-size, $X \mapsto \phi_n(X):=\nabla f_{n}(X)$ is the gradient of the witness function between $\mu$ and $\nu_n$. It is easy to see that the particle version of \cref{eq:discretized_flow_particles} is given by:
\begin{align}
	X_{n+1} = X_n - \gamma \phi_n(X_n) \forall n\in \mathbb{N}.
\end{align}. 
One important criterion to characterize the convergence of the gradient flow of a functional $\F$ is the notion of displacement convexity of such functional. Displacement convexity states that the functional evaluated at any distribution in a geodesic path between two distributions $\nu$ and $\nu'$ will be upper-bounded by a convex mixture of $\F(\nu)$ and $\F(\nu')$:
\begin{definition}
(displacement convexity \cite{Villani:2004} Definition 1 ). Let $\mu$
and $\nu$ be two probabilities densities. There exists a $\mu-a.e.$
unique gradient of a convex function ,$\nabla\phi$, such that $\nu$
is equal to $\nabla\phi_{\#}\mu$ and one can define $\rho_{t}=((1-t)Id+t\nabla\phi)_{\#}\mu$
for $0\leq t\leq1$. We say that a functional $\nu\mapsto\mathcal{F}(\nu)$
is displacement convex if 
\[
t\mapsto\mathcal{F}(\rho_{t})
\]
 is convex for any $\mu$ and $\nu$.Moreover, we say that $\mathcal{F}$
is convex in a neighborhood of $\mu$ if there exists a radius $r>0$
such that the above property holds for any $\nu$ with $W_{2}(\mu,\nu)\leq r$
.
\end{definition}


This notion of convexity is to be related to the more widely used notion of convexity called mixture convexity:
\begin{align}
	\F(t\nu +(1-t)\nu')\leq t\F(\nu)+(1-t)\F(\nu') \qquad t\in [0,1]
\end{align}
Unlike, mixture convexity, displacement convexity is compatible with the $W_2$ metric and is therefore the natural notion to use for characterizing convergence of gradient flows in the $W_2$ metric.
Although mixture convexity holds for $\F$ (\cref{lem:mixture_convexity}), this property is less critical for characterizing convergence of gradient flows in the $W_2$ metric. On the other hand, displacement convexity is compatible with the $W_2$ metric \cite{Bottou:2017} and is therefore the natural notion to use in our setting. Unfortunately, $\F$ fails to be displacement convex in general. Instead we will show that $\F$ satisfies some weaker notion of convexity called $\Lambda$-displacement convexity:
%
\begin{definition}
($\Lambda$-convexity \cite{Villani:2009} Definition 16.4). Let $(\mu,v)\mapsto\Lambda(\mu,v)$
be a function that defines for each probability distribution $\mu$
a quadratic form on the set of square integrable vectors valued functions
$v$ , i.e: $v\in L_{2}(\mathbb{R}^{d},\mathbb{R}^{d},\mu)$ . We
further assume that:
\[
\inf_{\mu,v}\frac{\Lambda(\mu,v)}{\Vert v\Vert_{L_{2}(\mu)}^{2}}>-\infty.
\]

We say that a functional $\mu\mapsto\mathcal{F}(\mu)$ is $\Lambda$-convex
if for any $\mu$ and $\nu$ and a minimizing geodesic $\text{\ensuremath{\rho_{t}}}$
between $\mu$ and $\nu$ with velocity vector field $v_{t}$, i.e:
$\partial_{t}\rho_{t}+div(\rho_{t}v_{t})=0;\rho_{0}=\mu;\rho_{1}=\nu$
the following holds:
\end{definition}
\[
\frac{d^{2}\mathcal{F}(\rho_{t})}{dt^{2}}\geq\Lambda(\rho_{t},v_{t})\qquad\forall t\in[0,1].
\]

To show the $\Lambda$-convexity of the functional defined in \cref{eq:MMD_functional} we first make the following assumptions on the kernel:
\begin{assumplist} 
\item \label{assump:bounded_trace} $ \vert \sum_{1\leq i\leq d} \partial_i\partial_ik(x,x) \vert\leq \frac{L}{3}  $ for all $x\in \mathbb{R}^d$.
\item \label{assump:bounded_hessian} $\Vert H_xk(x,y) \Vert_{op} \leq \frac{L}{3}$ for all $x,y\in \mathbb{R}^d$, where $H_xk(x,y)$ is the hessian of $x\mapsto k(x,y)$.
\item \label{assump:bounded_fourth_oder} $\Vert Dk(x,y) \Vert\leq \gamma  $ for all $x,y\in \mathbb{R}$, where $Dk(x,y)$ is an $\mathbb{R}^{d^2}\times \mathbb{R}^{d^2}$ matrix with entries given by $\partial_{x_{i}}\partial_{x_{j}}\partial_{x'_{i}}\partial_{x'j}k(x,x')$.
\end{assumplist}
The next proposition states that the functional defined in \cref{eq:MMD_functional} is $\Lambda$-displacement convex and provide and explicit expression for the functional $\Lambda$:

\begin{proposition}
\label{prop:lambda_convexity} Under \cref{assump:bounded_fourth_oder}, the functional $\nu\mapsto \F(\nu)$ is $\text{\ensuremath{\Lambda}}$-convex
with $\Lambda$ given by:
\begin{equation}
\Lambda(\rho,v)=\langle v,(C_{\rho}-\lambda \F(\rho)^{\frac{1}{2}}I)v\rangle_{L_{2}(\rho)}\label{eq:Lambda}
\end{equation}

where $C_{\rho}$ is the operator defined by:
\[
(C_{\rho}v)(x)=\int\nabla_{x}\nabla_{x'}k(x,x')v(x')d\rho(x')
\] and $\lambda$ is defined in \cref{assump:bounded_fourth_oder}.
\end{proposition}
%
%
It is worth noting that $\nu_{0}=\mu$ and at time $t=0$ we have
that $\F(\rho_{0})=0$ and hence we get:
\[
\frac{d^{2}\F(\rho_{t})}{dt^{2}}\vert_{t=0}=\langle v_{t},C_{\rho_{t}}v_{t}\rangle_{L_{2}(\rho_{t})}\geq0.
\]
This shows that $\nu\mapsto \F(\nu)$ has a non-negative
hessian at $\mu$ which is not surprising since $\mu$ is the global
minimum of this functional.
\begin{corollary}\label{cor:integral_lambda_convexity}
For any geodesic $\rho_{t}$ between two probability distributions
$\rho_{0}$ and $\rho_{1}$ the following holds:
\begin{equation}
\F(\rho_{t})\leq(1-t)\F(\rho_{0})+t\F(\rho_{1})-\int_{0}^{1}\Lambda_{\mu}(\rho_{s},v_{s})G(s,t)ds\label{eq:integral_lambda_convexity}
\end{equation}

where $\Lambda_{\mu}$ is given by \cref{eq:Lambda} and $G$ is given
by:
\[
G(s,t)=\begin{cases}
s(1-t) & s\leq t\\
t(1-s) & s\geq t
\end{cases}
\]
\end{corollary}
%
\begin{proof}
This is a direct consequence of the general identity (\cite{Villani:2009},
proposition 16.2). Indeed, for any continuous function $\phi$ on
$[0,1]$ with second derivative $\ddot{\phi}$ that is bounded below
in distribution sense the following identity holds:
\[
\phi(t)=(1-t)\phi(0)+t\phi(1)-\int_{0}^{1}\ddot{\phi}(s)G(s,t)ds
\]
Hence, one can choose $\phi(t)=\F(\rho_{t})$ therefore,
it follows that:
\[
\F(\rho_{t})=(1-t)\F(\rho_{0})+t\F(\rho_{1})-\int_{0}^{1}\frac{d^{2}\F(\rho_{s})}{ds^{2}}G(s,t)ds
\]
Now using the inequality from \cref{prop:lambda_convexity}, \cref{eq:integral_lambda_convexity}
follows directly. 
\end{proof}
%
\begin{corollary}
\label{cor:loser_bound}Assume the distributions are supported on
$\mathcal{X}$ and the kernel is bounded, i.e: $\sup_{x,y\in\mathcal{X}}\vert k(x,y)\vert<\infty$.
Then the following holds:

\[
\F(\rho_{t})\leq(1-t)\F(\rho_{0})+t\F(\rho_{1})+t(1-t)K
\]
\end{corollary}
%
\begin{proof}
Recall the expression of $\Lambda_{\mu}(\rho_{s},v_{s}):$

\[
\Lambda_{\mu}(\rho_{s},v_{s})=\langle v_{t},(C_{\rho_{t}}-\lambda \F(\rho_{t})^\frac{1}{2} I)v_{t}\rangle_{L_{2}(\rho_{t})}\geq-\lambda \F(\rho_{t})^\frac{1}{2}\Vert v_{t}\Vert_{L_{2}(\rho_{t})}^{2}
\]
However, $\F(\rho_{t})^\frac{1}{2}\leq4C$ where $C=\sup_{x,y\in\mathcal{X}}\vert k(x,y)\vert$
. Moreover, if $\rho_{t}$ is a constant speed geodesic then $\Vert v_{t}\Vert_{L_{2}(\rho_{t})}^{2}=W_{2}^{2}(\rho_{0},\rho_{1})$,
hence: 
\[
-\int_{0}^{1}\Lambda_{\mu}(\rho_{s},v_{s})G(s,t)ds\leq4\lambda CW_{2}^{2}(\rho_{0},\rho_{1})\leq2t(1-t)\lambda Cdiam(\mathcal{X})^{2}
\]
where $diam(\mathcal{X})$ is the diameter of $\mathcal{X}$. The rest of the proof follows by directly using \cref{cor:integral_lambda_convexity}
and by setting $K=2\lambda Cdiam(\mathcal{X})$.
\end{proof}
%
\cref{cor:loser_bound}, is a loser bound and doesn't account for local
convexity of the MMD. However, it allows to state the following result,
which is inspired from (\cite{Bottou:2017}, Theorem 6.3) but generalizes
it to the case of 'almost convex' functionals.
\begin{proposition}
\label{prop:almost_convex_optimization}
(Almost convex optimization). Let $\mathcal{P}$ be a closed subset
of $\mathcal{P}(\mathcal{X})$ which is displacement convex. Then
for all $M>\inf_{\rho\in\mathcal{P}}\F(\rho)+K$, the following
holds:
\end{proposition}
\begin{enumerate}
\item The level set $L(\mathcal{P},M)=\{\rho\in\mathcal{P}:\F(\rho)\leq M\}$
is connected
\item For all $\rho_{0}\in\mathcal{P}$ such that $\F(\rho_0)>M$
and all $\epsilon>0$, there exists $\rho\in\mathcal{P}$ such that
$W_{2}(\rho,\rho_{0})=\mathcal{O}(\epsilon)$ and
\[
\F(\rho)\leq \F(\rho_{0})-\epsilon(\F(\rho_{0})-M).
\]
\end{enumerate}
%
\begin{remark}
The result in \cref{prop:almost_convex_optimization} means that it is possible to optimize the cost function $\rho\mapsto \F(\rho)$
on $\mathcal{P}$ as long as the barrier $\inf_{\rho\in\mathcal{P}}\F(\rho)+K$
is not reached. We provide now a simple proof of this result:
\end{remark}
%
\begin{proof}
The proof is very similar to (\cite{Bottou:2017}, Theorem 6.3 and
Theorem 6.9): 
\end{proof}
\begin{enumerate}
\item First choose $\rho_{1}\in\mathcal{P}$ such that $\F(\rho_{1})<M-K$.
For any $\rho_{0},\rho_{0}'\in L(\mathcal{P},M)$ there exist a displacement
geodesic joining $\rho_{1}$ and $\rho_{0}$ without leaving $\mathcal{P}$,
since $\mathcal{P}$ is by assumption discplacement convex. By \cref{cor:loser_bound}
we have:
\begin{align*}
\F(\rho_{t}) & \leq(1-t)\F(\rho_{0})+t\F(\rho_{1})+t(1-t)K\\
 & \leq(1-t)M+t(M-K)+t(1-t)K\leq M-t^{2}K\leq M
\end{align*}
Hence $\rho_{t}\in L(\mathcal{P},M)$. The same can be done for a
path joining $\rho_{0}'$ and $\rho_{1}$. Hence we can find a path
in $L(\mathcal{P},M)$ joining $\rho_{0}$ and $\rho_{0}'$ , which
means that the level set $L(\mathcal{P},M)$ is connected.
\item Consider now $\rho_{1}\in L(\mathcal{P},M-K)$, note that such an
element exists since $M>\inf_{\rho\in\mathcal{P}}\F(\rho)+K$.
By convexity of $\mathcal{P}$ there exists a constant speed geodesic
$\rho_{t}$ connecting $\rho_{0}$ and $\rho_{1}$. Since it is a
constant speed curve then one has:
\[
W_{2}(\rho_{0},\rho_{t})\leq tW_{2}(\rho_{0},\rho_{1}).
\]
But we also have by \cref{cor:loser_bound}:
\begin{align*}
\F(\rho_{t}) & \leq(1-t)\F(\rho_{0})+t\F(\rho_{1})+t(1-t)K\\
 & \leq \F(\rho_{0})-t(\F(\rho_{0})-M+tK)\\
 & \leq \F(\rho_{0})-t(\F(\rho_{0})-M)
\end{align*}
Here we simply used the fact that $\rho_{1}\in L(\mathcal{P},M-K)$. 
\end{enumerate}
%

\begin{remark}
	A possible direction would be to directly leverage the tighter inequality in \cref{eq:integral_lambda_convexity} to get a better description of the loss landscape.
\end{remark}

\cref{prop:almost_convex_optimization} guarantees the existence of a direction of descent that minimizes the $\F$ provided that the starting point $\rho_1$ has a potential greater than the barrier $K$, i.e:
\begin{align}\label{eq:barrier_condition}
	\F(\rho_1)> \inf_{\rho\in \mathcal{P}} \F(\rho) + K
\end{align}
One natural question to ask is whether the  discretized gradient flow algorithm provides such way to reach the barrier $K$ and at what speed this happens. The next result answers that question:   

\begin{theorem}
	Consider the sequence of distributions $\nu_n$ obtained from \cref{eq:discretized_flow}. Then the following holds:
	\begin{align}
		\F(\bar{\nu})\leq \frac{1}{2\gamma N}W_2^2(\nu_0,\mu) +K
	\end{align}
	where $\bar{\nu}=\frac{1}{N}\sum_{n=1}^N \nu_n$.
\end{theorem}
\begin{proof}
Let $\Pi^n$ be the optimal coupling between $\nu_n$ and $\mu$, then the optimal transport between $\nu_n$ and $\mu$ is given by:

\begin{align}
	W_2^2(\mu,\nu_n)=\int \Vert X-Y \Vert^2 d\Pi^n(\nu_n,\mu)
\end{align}

Moreover, consider $Z=X-\gamma \phi_n(X)$ where $(X,Y)$ are samples from $\Pi^n$. It is easy to see that $(Z,Y)$ is a coupling between $\nu_{n+1}$ and $\mu$, therefore, by definition of the optimal transport map between $\nu_{n+1}$ and $\mu$ it follows that:
\begin{align}\label{eq:optimal_upper-bound}
	W_2^2(\nu_{n+1},\mu)\leq \int \Vert X-\gamma \phi_{n}(X)-Y\Vert^2 \Pi^n(\nu_n,\mu)
\end{align}
By expanding the r.h.s in \cref{eq:optimal_upper-bound}, the following inequality holds:
\begin{align}\label{eq:main_inequality}
	W_2^2(\nu_{n+1},\mu)\leq W_2^2(\nu_{n},\mu) -2\gamma \int \langle \phi_n(X), X-Y \rangle d\Pi^n(\nu_n,\mu)+ \gamma^2D(\nu_n)
\end{align}
where $D(\nu_n) = \int \Vert \Phi_n(X)\vert^2 d\nu_n $.
An upper-bound on $-2\gamma \int \langle \phi_n(X), X-Y \rangle d\Pi^n(\nu_n,\mu) $ in terms of the loss functional can be obtained using the $\Lambda$ displacement convexity of $\nu\mapsto \F(\nu)$. Indeed, by \cref{lem:grad_flow_lambda_version} it holds that:
\begin{align}\label{eq:flow_upper-bound}
	-2\gamma \int \langle \phi(X),X-Y \rangle d\Pi(\nu,\mu)
	\leq
	-2\gamma\left(\F(\nu)- \F(\mu) +K(\rho^n)\right)
\end{align}
where $(\rho^n_t)_{0\leq t \leq 1}$ is a constant-speed geodesic from $\nu_n$ to $\mu$ and $K(\rho^n):=\int_0^1 \Lambda(\rho^n_s,\dot{\rho^n}_s)(1-s)ds$. Note that when $K(\rho^n)\leq 0$ it falls back to the convex setting.
Therefore, the following inequality holds:
\begin{align}
	W_2^2(\nu_{n+1},\mu)\leq W_2^2(\nu_{n},\mu) - 2\gamma\left(\F(\nu_n)- \F(\mu) +K(\rho^n)\right) +\gamma^2 D(\nu_n)
\end{align}
Now let's introduce a term involving $\F(\nu_{n+1})$. The above inequality becomes:
\begin{align}
	W_2^2(\nu_{n+1},\mu)\leq & W_2^2(\nu_{n},\mu) - 2\gamma\left(\F(\nu_{n+1})- \F(\mu) +K(\rho^n)\right) \\
		&+\gamma^2 D(\nu_n) -2\gamma (\F(\nu_n)-\F(\nu_{n+1}))
	\label{eq:main_ineq_2}
\end{align}
 
It is possible to upper-bound the last two terms by a negative quantity when the step-size is small enough. This is mainly a consequence of the smoothness of the functional $\F$ and the fact that $\nu_{n+1}$ is obtained by following the steepest direction of $\F$ starting from $\nu_n$. \cref{lem:decreasing_functional} makes this statement more precise and allows to get the following inequality:
\begin{align}
	\gamma^2 D(\nu_n) -2\gamma (\F(\nu_n)-\F(\nu_{n+1})\leq -\gamma (1-\gamma L)D(\nu_n).
	\label{eq:decreasing_functional}
\end{align}
Here $L$ is a constant that depends only on the choice of the kernel $k$ in $\F$. Combining \cref{eq:decreasing_functional} and \cref{eq:main_ineq_2} it follows that:
\begin{align}
\F(\nu_{n+1})-\F(\nu)+\frac{\gamma}{2}(1-\gamma L)D(\nu_n)
\leq 
\frac{1}{2\gamma} (W_2^2(\nu_n,\mu)-W_2^2(\nu_{n+1},\mu)-K(\rho^n)
\label{eq:main_final}
\end{align}
Averaging \cref{eq:main_final} for $1\leq n\leq N$ it follows that:
\begin{align}
	\bar{\F}-\F(\mu)+\frac{\gamma}{2}(1-\gamma L)\bar{D} \leq \frac{W_2^2(\nu_0,\mu)}{2 \gamma N} -\bar{K}
\end{align}
where $\bar{\F}$,$\bar{D}$ and $\bar{K}$ denotes the averaged values of $(\F(\nu_n))_{1\leq n \leq N}$, $(D(\nu_n))_{1\leq n \leq N}$  and $(K(\rho^n))_{1\leq n \leq N}$ over iterations from $1$ to $N$. Finally, by \cref{lem:mixture_convexity} it follows that:
\begin{align}
\F(\bar{\nu})-\F(\mu)\leq  \frac{W_2^2(\nu_0,\mu)}{2 \gamma N} -\bar{K}
\end{align}

\end{proof}




