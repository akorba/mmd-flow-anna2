\section{Theoretical properties of the MMD flow}\label{sec:theory}


In all what follows we consider a fixed target distribution $\mu$ and define the following functional 
\begin{align}\label{eq:MMD_functional}
	\nu \mapsto \F(\nu):=\frac{1}{2} MMD^2(\mu,\nu)\textbf{}
\end{align}
We investigate some theoretical properties of the MMD flow. In particular, we are interested in characterizing the convergence of the time discretized flow:
	\begin{align}\label{eq:discretized_flow}
		\nu_{n+1} = (I -\gamma \phi_n)_{\#}\nu_n
	\end{align}
where $\gamma$ is some fixed step-size and $X \mapsto \phi_n(X):=\nabla f_{n}(X)$ is the gradient of the witness function between $\mu$ and $\nu_n$. It is easy to see that the particle version of \cref{eq:discretized_flow} is given by:
\begin{align}
	X_{n+1} = X_n - \gamma \phi_n(X_n) \quad\forall n\in \mathbb{N}.
\end{align} 

\subsection{Lambda displacement convexity of the MMD}

One important criterion to characterize the convergence of the gradient flow of a functional $\F$ is the notion of \textit{displacement convexity} of such a functional. Displacement convexity (see \cite{Villani:2004}, Definition 1). states that the functional evaluated at any distribution in a geodesic path between two distributions $\nu$ and $\nu'$ will be upper-bounded by a convex mixture of $\F(\nu)$ and $\F(\nu')$, as explained formally in the following definition.
\begin{definition}\label{def:displacement_convexity}
 Let $\mu$
and $\nu$ be two probabilities densities. There exists a $\mu-a.e.$
unique gradient of a convex function, denoted by $\nabla\phi$, such that $\nu$
is equal to $\nabla\phi_{\#}\mu$ and one can define \aknote{the displacement geodesic?} $\rho_{t}=((1-t)Id+t\nabla\phi)_{\#}\mu$
for $0\leq t\leq1$. We say that a functional $\nu\mapsto\mathcal{F}(\nu)$
is displacement convex if 
\[
t\mapsto\mathcal{F}(\rho_{t})
\]
 is convex for any $\mu$ and $\nu$. Moreover, we say that $\mathcal{F}$
is displacement convex in a neighborhood of $\mu$ if there exists a radius $r>0$
such that the above property holds for any $\nu$ with $W_{2}(\mu,\nu)\leq r$.
\end{definition}


This notion of convexity is to be related to the more widely used notion of convexity called \textit{mixture convexity}:
\begin{align}
	\F(t\nu +(1-t)\nu')\leq t\F(\nu)+(1-t)\F(\nu') \qquad t\in [0,1]
\end{align}
%Unlike mixture convexity, displacement convexity is compatible with the $W_2$ metric and is therefore the natural notion to use for characterizing convergence of gradient flows in the $W_2$ metric.
Although mixture convexity holds for $\F$ (see \cref{lem:mixture_convexity}), this property is less critical for characterizing convergence of gradient flows in the $W_2$ metric. On the other hand, displacement convexity is compatible with the $W_2$ metric \cite{Bottou:2017} and is therefore the natural notion to use in our setting. Unfortunately, $\F$ fails to be displacement convex in general. Instead we will show that $\F$ satisfies some weaker notion of convexity called $\Lambda$-displacement convexity:
%
\begin{definition}\label{def:lambda-convexity}
($\Lambda$-convexity \cite{Villani:2009} Definition 16.4). Let $(\mu,v)\mapsto\Lambda(\mu,v)$
be a function that defines for each probability distribution $\mu$
a quadratic form on the set of square integrable vectors valued functions
$v$ , i.e: $v\in L_{2}(\mathbb{R}^{d},\mathbb{R}^{d},\mu)$ . We
further assume that:
\[
\inf_{\mu,v}\frac{\Lambda(\mu,v)}{\Vert v\Vert_{L_{2}(\mu)}^{2}}>-\infty.
\]
We say that a functional $\mu\mapsto\mathcal{F}(\mu)$ is $\Lambda$-convex
if for any $\mu$ and $\nu$ and a minimizing geodesic $\text{\ensuremath{\rho_{t}}}$
between $\mu$ and $\nu$ with velocity vector field $v_{t}$, i.e:
$\partial_{t}\rho_{t}+div(\rho_{t}v_{t})=0;\rho_{0}=\mu;\rho_{1}=\nu$
the following holds:
\begin{equation*}
\frac{d^{2}\mathcal{F}(\rho_{t})}{dt^{2}}\geq\Lambda(\rho_{t},v_{t})\qquad\forall t\in[0,1].
\end{equation*}
\end{definition}

To show the $\Lambda$-convexity of the functional defined in \cref{eq:MMD_functional} we first make the following assumptions on the kernel:
\begin{assumplist} 
\item \label{assump:bounded_trace} $ \vert \sum_{1\leq i\leq d} \partial_i\partial_ik(x,x) \vert\leq \frac{L}{3}  $ for all $x\in \mathbb{R}^d$.
\item \label{assump:bounded_hessian} $\Vert H_xk(x,y) \Vert_{op} \leq \frac{L}{3}$ for all $x,y\in \mathbb{R}^d$, where $H_xk(x,y)$ is the hessian of $x\mapsto k(x,y)$ and $\Vert.\Vert_{op}$ is the operator norm.
\item \label{assump:bounded_fourth_oder} $\Vert Dk(x,y) \Vert\leq \lambda  $ for all $x,y\in \mathbb{R}$, where $Dk(x,y)$ is an $\mathbb{R}^{d^2}\times \mathbb{R}^{d^2}$ matrix with entries given by $\partial_{x_{i}}\partial_{x_{j}}\partial_{x'_{i}}\partial_{x_{j}'}k(x,x')$.
\end{assumplist}\aknote{do we have an order of magnitude for lambda? or just we put a remark to say it's satisfied by the gaussian kernel}
The next proposition states that the functional defined in \cref{eq:MMD_functional} is $\Lambda$-displacement convex and provide and explicit expression for the functional $\Lambda$.

\begin{proposition}
\label{prop:lambda_convexity} Suppose \cref{assump:bounded_fourth_oder} is satisfied for some $\lambda \in \R^+$. The functional $\nu\mapsto \F(\nu)$ is $\text{\ensuremath{\Lambda}}$-convex
with $\Lambda$ given by:
\begin{equation}
\Lambda(\rho,v)=\langle v,(C_{\rho}-\lambda \F(\rho)^{\frac{1}{2}}I)v\rangle_{L_{2}(\rho)}\label{eq:Lambda}
\end{equation}
where $C_{\rho}$ is the (positive) operator defined by:
\begin{align}\label{eq:positive_operator_C}
	(C_{\rho}v)(x)=\int\nabla_{x}\nabla_{x'}k(x,x')v(x')d\rho(x')
\end{align}
\end{proposition}
%
%
Consider the geodesic \aknote{path/geodesic/curve?}$\rho_{t}=((1-t)Id+t\nabla\phi)_{\#}\mu$ of \cref{def:displacement_convexity}. It is worth noting that $\rho_{0}=\mu$ and at time $t=0$ we have
that $\F(\rho_{0})=0$, hence we get:
\[
\frac{d^{2}\F(\rho_{t})}{dt^{2}}\vert_{t=0}=\langle v_{t},C_{\rho_{t}}v_{t}\rangle_{L_{2}(\rho_{t})}\geq0.
\]
This shows that $\nu\mapsto \F(\nu)$ has a non-negative
hessian at $\mu$ which is not surprising since $\mu$ is the global
minimum of this functional.
\begin{corollary}\label{cor:integral_lambda_convexity}
For any geodesic $\rho_{t}$ between two probability distributions
$\rho_{0}$ and $\rho_{1}$ the following holds:
\begin{equation}
\F(\rho_{t})\leq(1-t)\F(\rho_{0})+t\F(\rho_{1})-\int_{0}^{1}\Lambda(\rho_{s},v_{s})G(s,t)ds\label{eq:integral_lambda_convexity}
\end{equation}

where $\Lambda$ is given by \cref{eq:Lambda} and $G$ is given
by:
\[
G(s,t)=\begin{cases}
s(1-t) & s\leq t\\
t(1-s) & s\geq t
\end{cases}
\]
\end{corollary}
%
\begin{proof}
This is a direct consequence of the general identity (\cite{Villani:2009},
Proposition 16.2). Indeed, for any continuous function $\phi$ on
$[0,1]$ with second derivative $\ddot{\phi}$ that is bounded below
in distribution sense the following identity holds:
\[
\phi(t)=(1-t)\phi(0)+t\phi(1)-\int_{0}^{1}\ddot{\phi}(s)G(s,t)ds
\]
Hence, one can choose $\phi(t)=\F(\rho_{t})$ therefore, \aknote{do we have second derivative bounded below for mmd?}
it follows that:
\[
\F(\rho_{t})=(1-t)\F(\rho_{0})+t\F(\rho_{1})-\int_{0}^{1}\frac{d^{2}\F(\rho_{s})}{ds^{2}}G(s,t)ds
\]
Now using the inequality from \cref{prop:lambda_convexity}, \cref{eq:integral_lambda_convexity}
follows directly. 
\end{proof}
%
\begin{corollary}
\label{cor:loser_bound}Assume the distributions are supported on
$\mathcal{X}$ and the kernel is bounded, i.e: $\sup_{x,y\in\mathcal{X}}\vert k(x,y)\vert<\infty$.
Then the following holds:
\begin{equation}
\F(\rho_{t})\leq(1-t)\F(\rho_{0})+t\F(\rho_{1})+t(1-t)K
\end{equation}
where $K$ is a constant depending on $\X$ and the kernel $k$ in $\F$.
\end{corollary}
%
\begin{proof}
Recall the expression of $\Lambda(\rho_{s},v_{s}):$

\[
\Lambda(\rho_{s},v_{s})=\langle v_{t},(C_{\rho_{t}}-\lambda \F(\rho_{t})^\frac{1}{2} I)v_{t}\rangle_{L_{2}(\rho_{t})}\geq-\lambda \F(\rho_{t})^\frac{1}{2}\Vert v_{t}\Vert_{L_{2}(\rho_{t})}^{2}
\]
However, $\F(\rho_{t})^\frac{1}{2}\leq4C$ where $C=\sup_{x,y\in\mathcal{X}}\vert k(x,y)\vert$
. Moreover, if $\rho_{t}$ is a constant speed geodesic then $\Vert v_{t}\Vert_{L_{2}(\rho_{t})}^{2}=W_{2}^{2}(\rho_{0},\rho_{1})$,
hence: 
\[
-\int_{0}^{1}\Lambda(\rho_{s},v_{s})G(s,t)ds\leq\lambda 4CW_{2}^{2}(\rho_{0},\rho_{1})\int_{0}^{1}G(s,t)ds\leq2t(1-t)\lambda Cdiam(\mathcal{X})^{2}
\]
where $diam(\mathcal{X})$ is the diameter of $\mathcal{X}$. The rest of the proof follows by directly using \cref{cor:integral_lambda_convexity}
and by setting $K=2\lambda Cdiam(\mathcal{X})$.
\end{proof}
%
\cref{cor:loser_bound}, is a loser bound and does not account for the local
convexity of the MMD. However, it allows to state the following result,
which is inspired from (\cite{Bottou:2017}, Theorem 6.3) but generalizes
it to the case of 'almost convex' functionals.
\begin{proposition}
\label{prop:almost_convex_optimization}
(Almost convex optimization). Let $\mathcal{P}$ be a closed subset
of $\mathcal{P}(\mathcal{X})$ which is displacement convex\aknote{weird for a set to be displacement convex? it was for functionals}. Then
for all $M>\inf_{\rho\in\mathcal{P}}\F(\rho)+K$, the following
holds:
\end{proposition}
\begin{enumerate}
\item The level set $L(\mathcal{P},M)=\{\rho\in\mathcal{P}:\F(\rho)\leq M\}$
is connected
\item For all $\rho_{0}\in\mathcal{P}$ such that $\F(\rho_0)>M$
and all $\epsilon>0$, there exists $\rho\in\mathcal{P}$ such that
$W_{2}(\rho,\rho_{0})=\mathcal{O}(\epsilon)$ and
\[
\F(\rho)\leq \F(\rho_{0})-\epsilon(\F(\rho_{0})-M).
\]
\end{enumerate}
%
\begin{remark}
The result in \Cref{prop:almost_convex_optimization} means that it is possible to optimize the cost function $\rho\mapsto \F(\rho)$
on $\mathcal{P}$ as long as the barrier $\inf_{\rho\in\mathcal{P}}\F(\rho)+K$
is not reached. We provide now a simple proof of this result.
\end{remark}
%
\begin{proof}
The proof is very similar to (\cite{Bottou:2017}, Theorem 6.3 and
Theorem 6.9): \aknote{$\rho_1$ already taken}
\begin{enumerate}
\item First choose $\rho_{1}\in\mathcal{P}$ such that $\F(\rho_{1})<M-K$.
For any $\rho_{0},\rho_{0}'\in L(\mathcal{P},M)$ there exist a displacement
geodesic joining $\rho_{1}$ and $\rho_{0}$ without leaving $\mathcal{P}$,
since $\mathcal{P}$ is by assumption discplacement convex. By \cref{cor:loser_bound}
we have:
\begin{align*}
\F(\rho_{t}) & \leq(1-t)\F(\rho_{0})+t\F(\rho_{1})+t(1-t)K\\
 & \leq(1-t)M+t(M-K)+t(1-t)K\leq M-t^{2}K\leq M
\end{align*}
Hence $\rho_{t}\in L(\mathcal{P},M)$. The same can be done for a
path joining $\rho_{0}'$ and $\rho_{1}$. Hence we can find a path
in $L(\mathcal{P},M)$ joining $\rho_{0}$ and $\rho_{0}'$ , which
means that the level set $L(\mathcal{P},M)$ is connected.
\item Consider now $\rho_{1}\in L(\mathcal{P},M-K)$, note that such an
element exists since $M>\inf_{\rho\in\mathcal{P}}\F(\rho)+K$.
By convexity of $\mathcal{P}$ there exists a constant speed geodesic
$\rho_{t}$ connecting $\rho_{0}$ and $\rho_{1}$. Since it is a
constant speed curve then one has:
\[
W_{2}(\rho_{0},\rho_{t})\leq tW_{2}(\rho_{0},\rho_{1}).
\]
But we also have by \cref{cor:loser_bound}:
\begin{align*}
\F(\rho_{t}) & \leq(1-t)\F(\rho_{0})+t\F(\rho_{1})+t(1-t)K\\
 & \leq \F(\rho_{0})-t(\F(\rho_{0})-M+tK)\\
 & \leq \F(\rho_{0})-t(\F(\rho_{0})-M)
\end{align*}
Here we simply used the fact that $\rho_{1}\in L(\mathcal{P},M-K)$. 
\end{enumerate}
\end{proof}

\begin{remark}
	A possible direction would be to directly leverage the tighter inequality in \cref{eq:integral_lambda_convexity} to get a better description of the loss landscape.
\end{remark}

\subsection{Convergence of the discretized flow}

\cref{prop:almost_convex_optimization} guarantees the existence of a direction of descent that minimizes the functional $\F$ provided that the starting point $\rho_1$ has a potential greater than the barrier $K$, i.e:
\begin{align}\label{eq:barrier_condition}
	\F(\rho_1)> \inf_{\rho\in \mathcal{P}} \F(\rho) + K
\end{align}
One natural question to ask is whether the  discretized gradient flow algorithm provides such way to reach the barrier $K$ and at what speed this happens. The next result answers that question:   

\begin{proposition}\label{prop:evi}
	Consider the sequence of distributions $\nu_n$ obtained from \cref{eq:discretized_flow}. If $\gamma \leq 1/L$, then
	\begin{align}
2\gamma(\F(\nu_{n+1})-\F(\mu))
\leq 
W_2^2(\nu_n,\mu)-W_2^2(\nu_{n+1},\mu)-2\gamma K(\rho^n).
\label{eq:evi}
\end{align}
\end{proposition}

\begin{proof}
Let $\Pi^n$ be the optimal coupling between $\nu_n$ and $\mu$, then the optimal transport between $\nu_n$ and $\mu$ is given by:
\begin{align}
	W_2^2(\mu,\nu_n)=\int \Vert X-Y \Vert^2 d\Pi^n(\nu_n,\mu)
\end{align}
Moreover, consider $Z=X-\gamma \phi_n(X)$ where $(X,Y)$ are samples from $\Pi^n$. It is easy to see that $(Z,Y)$ is a coupling between $\nu_{n+1}$ and $\mu$, therefore, by definition of the optimal transport map between $\nu_{n+1}$ and $\mu$ it follows that:
\begin{align}\label{eq:optimal_upper-bound}
	W_2^2(\nu_{n+1},\mu)\leq \int \Vert X-\gamma \phi_{n}(X)-Y\Vert^2 d\Pi^n(\nu_n,\mu)
\end{align}
By expanding the r.h.s in \cref{eq:optimal_upper-bound}, the following inequality holds:
\begin{align}\label{eq:main_inequality}
	W_2^2(\nu_{n+1},\mu)\leq W_2^2(\nu_{n},\mu) -2\gamma \int \langle \phi_n(X), X-Y \rangle d\Pi^n(\nu_n,\mu)+ \gamma^2D(\nu_n)
\end{align}
where $D(\nu_n) = \int \Vert \phi_n(X)\vert^2 d\nu_n $.
An upper-bound on $-2\gamma \int \langle \phi_n(X), X-Y \rangle d\Pi^n(\nu_n,\mu) $ in terms of the loss functional can be obtained using the $\Lambda$ displacement convexity of $\nu\mapsto \F(\nu)$. Indeed, by \cref{lem:grad_flow_lambda_version} it holds that:
\begin{align}\label{eq:flow_upper-bound}
	-2\gamma \int  \phi(X).(X-Y) d\Pi(\nu,\mu)
	\leq
	-2\gamma\left(\F(\nu)- \F(\mu) +K(\rho^n)\right)
\end{align}
where $(\rho^n_t)_{0\leq t \leq 1}$ is a constant-speed geodesic from $\nu_n$ to $\mu$ and $K(\rho^n):=\int_0^1 \Lambda(\rho^n_s,\dot{\rho^n}_s)(1-s)ds$\aknote{as Adil said, we should try to quantify this}. Note that when $K(\rho^n)\leq 0$ it falls back to the convex setting.
Therefore, the following inequality holds:
\begin{align}
	W_2^2(\nu_{n+1},\mu)\leq W_2^2(\nu_{n},\mu) - 2\gamma\left(\F(\nu_n)- \F(\mu) +K(\rho^n)\right) +\gamma^2 D(\nu_n)
\end{align}
Now we introduce a term involving $\F(\nu_{n+1})$. The above inequality becomes:
\begin{align}
	W_2^2(\nu_{n+1},\mu)\leq & W_2^2(\nu_{n},\mu) - 2\gamma\left(\F(\nu_{n+1})- \F(\mu) +K(\rho^n)\right) \\
		&+\gamma^2 D(\nu_n) -2\gamma (\F(\nu_n)-\F(\nu_{n+1}))
	\label{eq:main_ineq_2}
\end{align}
It is possible to upper-bound the last two terms by a negative quantity when the step-size is small enough. This is mainly a consequence of the smoothness of the functional $\F$ and the fact that $\nu_{n+1}$ is obtained by following the steepest direction of $\F$ starting from $\nu_n$. \cref{prop:decreasing_functional} makes this statement more precise and enables to get the following inequality:
\begin{align}
	\gamma^2 D(\nu_n) -2\gamma (\F(\nu_n)-\F(\nu_{n+1})\leq -\gamma^2 (1-\gamma L)D(\nu_n),
	\label{eq:decreasing_functional}
\end{align}
where $L$ is a constant that depends only on the choice of the kernel $k$ in $\F$. Combining  \cref{eq:main_ineq_2} and \cref{eq:decreasing_functional} it follows that:
\begin{align}
2\gamma(\F(\nu_{n+1})-\F(\mu))+\gamma^2(1-\gamma L)D(\nu_n)
\leq 
W_2^2(\nu_n,\mu)-W_2^2(\nu_{n+1},\mu)-2\gamma K(\rho^n).
\label{eq:main_final}
\end{align}
\end{proof}
\begin{theorem}\label{th:rates_mmd}
	Consider the sequence of distributions $\nu_n$ obtained from \cref{eq:discretized_flow}. If $\gamma \leq 1/L$, then
	\begin{align}
\F(\bar{\nu}_{n})-\F(\mu)\leq  \frac{W_2^2(\nu_0,\mu)}{2 \gamma n} -\bar{K}
\end{align}
where $\bar{\nu}=\frac{1}{N}\sum_{n=1}^N \nu_n$. Moreover, 
\begin{align}
\F(\nu_n)-\F(\mu)\leq  \frac{W_2^2(\nu_0,\mu)}{2 \gamma n} -\bar{K}.
\end{align}
\end{theorem}
\begin{proof}
Iterating in \cref{eq:evi} we get:
\begin{align}
	2\gamma \sum_{j=1}^{n+1} (\F(\nu_{j}) - \F(\mu)) \leq W_2^2(\nu_0,\mu) - 2\gamma \sum_{j=0}^n K(\rho^j)
\end{align}
Let us denote $\bar{K}$ the average value\asnote{$K(\rho^j)$ is bounded in $j$?} of $(K(\rho^j))_{0\leq j \leq n}$ over iterations from $0$ to $n$. Using \cref{lem:mixture_convexity} we have:
\begin{align}
\F(\bar{\nu}_{n+1})-\F(\mu)\leq  \frac{W_2^2(\nu_0,\mu)}{2 \gamma (n+1)} -\bar{K}
\end{align}
Now, consider the Lyapunov function $L_n = n \gamma (\F(\nu_n) - \F(\mu)) + \frac12 W_2^2(\nu_n,\mu)$. Then,
\begin{align*}
    L_{n+1} &= n\gamma(\F(\nu_{n+1}) - \F(\mu)) + \gamma(\F(\nu_{n+1}) - \F(\mu)) + \frac12 W_2^2(\nu_{n+1},\mu)\\
    &\leq n\gamma(\F(\nu_{n+1}) - \F(\mu)) + \frac12 W_2^2(\nu_n,\mu)-\gamma K(\rho^n)\\
    &\leq n\gamma(\F(\nu_{n}) - \F(\mu)) + \frac12 W_2^2(\nu_n,\mu)-\gamma K(\rho^n) -n\gamma^2 (1-\frac{\gamma}{2}L )\int \Vert \phi_n(X)\Vert^2 d\nu_n \\
    &\leq  L_n - \gamma K(\rho^n).
\end{align*}
where we used Proposition~\cref{prop:decreasing_functional} in the penultimate inequality\asnote{Les deux derniers termes pourraient-ils se manger par miracle?}.
Finally, 
\begin{equation}
    n\gamma (\F(\nu_{n}) - \F(\mu)) \leq L_n \leq L_0 -\gamma \sum_{j = 0}^{n-1} K(\rho^j)
\end{equation}
\end{proof}





