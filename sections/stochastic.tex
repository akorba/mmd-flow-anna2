%In this subsection we assume that $MMD^2$ is $\lambda$-geodesically-convex. Conditions under which this holds will be provided in the next section.

%\subsection{Analysis of the theoretical algorithm}

%Equation~\eqref{eq:discretized_process} provides a theoretical algorithm to minimize $MMD^2(\cdot,\pi)$. The algorithm is only theoretical because it requires to compute $\nabla f_k(X_k)$.

%This algorithm is the discretization of the Gradient flow associated to $MMD^2$. Since $MMD^2$ is $\lambda$-convex, using Theorem 11.1.4 of~\cite{ambrosio2008gradient}, the gradient flow $(\rho_t)$ satisfies
%\begin{equation}
%    \label{eq:evi}
%    \frac12 \frac{d}{dt} W^2(\rho_t,\nu) + \frac{\lambda}{2}W^2(\rho_t,\nu) \leq MMD^2(\nu,\mu) - MMD^2(\rho_t,\pi)
%\end{equation}
%Unfortunately, $\lambda \leq 0$ (otherwise it would mean that $MMD^2$ is strongly-geodesically-convex and hence geodesically convex).

%For the theoretical algorithm~\eqref{eq:discretized_process} we can expect a discretized version of~\eqref{eq:evi} to hold : \asnote{This should hold, I haven't proved it yet but I will do it later}
%\begin{equation}
%    \label{eq:evi-discrete}
%    W^2(\rho_{k+1},\pi) \leq  W^2(\rho_{k},\pi) -2\gamma_{k+1}\left( \frac{\lambda}{2}W^2(\rho_{k},\pi) + MMD^2(\rho_{k+1},\pi) - MMD^2(\pi,\pi)\right)
%\end{equation}
%Since $\lambda \leq 0$ we cannot have a rate from this inequality (I think).
%However, if $\sum \gamma_k < \infty$, using Robbins Siegmund lemma, we know that $W^2(\rho_{k},\pi)$ converges to some $\ell \geq 0$. \asnote{From this it might be possible to prove that $W^2(\overline{\rho_{k}},\pi)$ converges to zero, but it would be a lot of work. It looks like Pakes Hasminskii criterion}

%\subsection{Another Lyapunov function}

%In this section we try to use $MMD^2(\cdot,\pi)$ as a Lyapunov function (instead of $W^2(\cdot,\pi)$), like in Theorem 3.3 of Liu 2017. Once this is done, we can use the Gradient Lojasiewicz inequality to get a rate (see Bolte, it's like log sobolev inequality)

%Taylor : 


\subsection{Space discretization - Sample-based setting}

Two settings are usually encountered in the sampling literature:
\begin{itemize}
	\item Density-based: $\mu$ is known up to a constant
	\item Sample-based: we only have access to a set of samples $X \sim \mu$.
\end{itemize}

\aknote{to investigate much further} 
The Unadjusted Langevin Algorithm (ULA) seems much more adapted to the first setting, since it only requires the knowledge of $\nabla \log \mu$ while our algorithm requires the knowledge of $\mu$ (since $\nabla f_t$ involves an integration over $\mu$). However, in the sample-based setting, it may be difficult to adapt ULA by replacing $\nabla \log \mu$ in \eqref{eq:langevin_algorithm} by an estimator based on samples. Indeed, it has been the subject of a lot of work (see \cite{li2017gradient})\aknote{check reference}. In contrast, the gradient of $f_t$ can be 'easily' estimated by:
\begin{equation}
\widehat{\nabla f_t}(z)= \frac{1}{n}\sum_{i=1}^{n}\nabla_{z}k(u_i,z) -\frac{1}{n}\sum_{i=1}^{n}\nabla_{z}k(v_i,z) 
\end{equation}
where $(u_1, \dots, u_n)\sim \mu$ and $(v_1, \dots, v_n)\sim \rho_t$. We denote by $\widehat{ \mu}=\sum_{j=1}^{n}\delta_{u_i}$.

\vspace{0.5cm}
The following is based on the formalism and some results of \cite{jourdain2007nonlinear}. Firstly, we recall that \eqref{eq:stochastic_process} can be written as a Mac-Kean Vlasov model, a particular kind of SDE driven by a Levy process:
\begin{align}\label{eq:theoretical_process}
&X_t=X_{0}+\int_{0}^t \sigma(X_s, \rho_s, \mu)ds \quad \text{for t in [0,T]}\\
&\forall s \in [0,T]\;,\quad \rho_s \text{ denotes the probability distribution of } X_s
\end{align}
with $\sigma(X_s, \rho_s, \mu)=-\nabla f_t(X_s)=\int \nabla_{X_s}k(.,X_s) d\rho_t -  \int \nabla_{X_s}k(.,X_s) d\mu$. Notice that $\sigma$ is Lipschitz continuous in its second and third variable.

In the sample-based setting, i.e. given $\widehat{\mu}$, it is thus natural to consider the following system of $n$ interacting particles:
\begin{align}\label{eq:sample_based_process}
&\widehat{X}_t^{j,n}=X_{0}+\int_{0}^t \sigma(\widehat{X}_s^{j,n}, \widehat{\rho}_s^n, \widehat{\mu})ds \quad \text{for t in [0,T]}\\
&\forall s \in [0,T]\;,\quad \widehat{\rho}_s^n=\sum_{j=1}^{n} \delta_{\widehat{X}_s^{j,n}} \text{ denotes the empirical measure } 
\end{align}



%It is thus natural to consider the stochastic process:
%\begin{equation}\label{eq:sample_based_process}
%dY_{t}=-\widehat{\nabla f_t}(Y_t) 
%\end{equation}


\begin{proposition}\aknote{proof not complete} Let $\rho_t$, $\widehat{\rho}_t$, the distributions of the processes \eqref{eq:theoretical_process} and \eqref{eq:sample_based_process} respectively. Suppose that $k$ is bounded and measurable on $\X$, and that there exists $L_k$ such that $\forall x,y \in \X$, $\| k(x,.)-k(y,.) \|_{\kH}\le L_k \|x-y\|$. Then:
	\begin{equation}
	MMD^2(\rho_t,\widehat{\rho_t})\le L_k^2( \frac{C_1}{n}+ \frac{C_2}{n^{\frac{1}{d}}})
	\end{equation}
\end{proposition}
\begin{proof}  \aknote{true?}
	%We now introduce the process:
	%\begin{equation}\label{eq:intermediary_process}
	%dZ_{t}=\widetilde{\nabla f_t}(Z_t) 
	%\end{equation}
	%where $\widetilde{\nabla f_t}(z)=\int \nabla_{z}k(X,z)\diff \mu- \frac{1}{n}\sum_{i=1}^{n}\nabla_{z}k(v_i,z)$ and $(v_1, \dots, v_n)\sim \rho_t$.  Let $\widetilde{\rho}_t$ be the distribution of \eqref{eq:intermediary_process}. 
	Introduce the system of $n$ interacting particles:
	\begin{align}\label{eq:intermediary_process}
	&X_t^{j,n}=X_{0}+\int_{0}^t \sigma(X_s^{j,n}, \rho_s^n, \mu)ds \quad \text{for t in [0,T]}\\
	&\forall s \in [0,T]\;,\quad \rho_s^n=\sum_{j=1}^{n} \delta_{X_s^{j,n}} \text{ denotes the empirical measure } 
	\end{align}
	By Theorem 3 in \cite{jourdain2007nonlinear}, since $\sigma$ is Lipschitz continuous in its second variable, we have:	
	\begin{equation}\label{eq:upp_bound1}
	\sup_{j \le n}\E[\sup_{t\le T}\| X_t - X_t^{j,n}\|^2]\le \frac{C_1}{n}
	\end{equation}
	
		
	\vspace{2cm}
	Let $\widetilde{\rho_t}$ be the distribution of \eqref{eq:intermediary_process}. We firstly have the following decomposition:
	\begin{align}\label{eq:decompose_process}
	MMD^2(\widehat{\rho_t}, \rho_t) \le MMD^2(\widehat{\rho_t}, \widetilde{\rho_t})+ MMD^2(\widetilde{\rho_t}, \rho_t)
	\end{align}
	 We will firstly bound the first term on the r.h.s. of \eqref{eq:decompose_process}. By \cref{lem:mmd_w2}, we have that:
	 \begin{equation}
	 MMD^2(\widehat{\rho_t}, \widetilde{\rho_t})\le L_k^2 W_2^2(\widehat{\rho_t}, \widetilde{\rho_t})
	 \end{equation}
	 Then:
	 \begin{align}
	 W_2^2(\rho_t, \widetilde{\rho_t}) = \inf_{\pi \in \Pi(\rho_t, \widetilde{\rho_t})} \int \| x-z \|^2 d\pi(\rho_t, \widetilde{\rho_t}) \le \int \|x-z\|^2 d\rho_t(x)d\widetilde{\rho_t}(z)
	 \end{align}
	 Hence by \eqref{eq:upp_bound1}:
	 \begin{equation}
	 MMD^2(\rho_t, \widetilde{\rho_t})\le L_k^2 \frac{C_1}{n}
	 \end{equation}
	We now turn to the second term at the r.h.s. of \eqref{eq:decompose_process}. We can derive a similar proof than Theorem 3 in \cite{jourdain2007nonlinear}:\aknote{same, we need Lipschitz continuity of the process in terms of the third variable}
	 \begin{align}
	 \E[\sup_{t\le T}\|Z_t-Y_t\|] \le   C \int_{O}^T |\sigma() -\sigma()| ds \le \E[W_1^2(\widehat{\mu},\mu)]
	 \end{align}
	where $\widehat{\mu}=\frac{1}{n}\sum_{i=1, \dots,n}\delta_{u_i}$ and $u_i \sim \mu$. It was shown in \cite{dudley1969speed} that when $d > 2$, if $\mu$ has a compact support in $\R^d$ then:
	\begin{equation}
		\E[W_1^2(\widehat{\mu},\mu)]\le \frac{C_2}{n^{\frac{1}{d}}}
	\end{equation}
	\begin{remark}
	Note that more recently, sharper rates of convergence  for $W_p(\widehat{ \mu}, \mu)$, for $p\ge 1$, have been computed in \cite{weed2017sharp} for a larger class of measures. These rates involve an intrinsic dimension of the measure $\mu$ (its Wassertein dimension). 
	\end{remark}
\end{proof}


\begin{remark}
	We point out here that algorithm~\eqref{eq:sample_based_process} is different from the descent proposed by \cite{mroueh2018regularized}. 
\end{remark}

\begin{remark}
	Birth-Death Dynamics to improve convergence (see \cite{rotskoff2019global}).
\end{remark}
