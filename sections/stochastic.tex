In this subsection we assume that $MMD^2$ is $\lambda$-geodesically-convex. Conditions under which this holds will be provided in the next section.

\subsection{Analysis of the theoretical algorithm}

Equation~\eqref{eq:discretization} provides a theoretical algorithm to minimize $MMD^2(\cdot,\pi)$. The algorithm is only theoretical because it requires to compute $\nabla f_k(X_k)$.

This algorithm is the discretization of the Gradient flow associated to $MMD^2$. Since $MMD^2$ is $\lambda$-convex, using Theorem 11.1.4 of~\cite{ambrosio2008gradient}, the gradient flow $(\rho_t)$ satisfies
\begin{equation}
    \label{eq:evi}
    \frac12 \frac{d}{dt} W^2(\rho_t,\nu) + \frac{\lambda}{2}W^2(\rho_t,\nu) \leq MMD^2(\nu,\mu) - MMD^2(\rho_t,\pi)
\end{equation}
Unfortunately, $\lambda \leq 0$ (otherwise it would mean that $MMD^2$ is strongly-geodesically-convex and hence geodesically convex).

For the theoretical algorithm~\eqref{eq:discretization} we can expect a discretized version of~\eqref{eq:evi} to hold : \asnote{This should hold, I haven't proved it yet but I will do it later}
\begin{equation}
    \label{eq:evi-discrete}
    W^2(\rho_{k+1},\pi) \leq  W^2(\rho_{k},\pi) -2\gamma_{k+1}\left( \frac{\lambda}{2}W^2(\rho_{k},\pi) + MMD^2(\rho_{k+1},\pi) - MMD^2(\pi,\pi)\right)
\end{equation}
Since $\lambda \leq 0$ we cannot have a rate from this inequality (I think).
However, if $\sum \gamma_k < \infty$, using Robbins Siegmund lemma, we know that $W^2(\rho_{k},\pi)$ converges to some $\ell \geq 0$. \asnote{From this it might be possible to prove that $W^2(\overline{\rho_{k}},\pi)$ converges to zero, but it would be a lot of work. It looks like Pakes Hasminskii criterion}

\subsection{Another Lyapunov function}

In this section we try to use $MMD^2(\cdot,\pi)$ as a Lyapunov function (instead of $W^2(\cdot,\pi)$), like in Theorem 3.3 of Liu 2017. Once this is done, we can use the Gradient Lojasiewicz inequality to get a rate (see Bolte, it's like log sobolev inequality)

Taylor : 


\subsection{Sample-based setting}

Two settings are usually encountered in the sampling literature:
\begin{itemize}
	\item Density-based: $\mu$ is known up to a constant
	\item Sample-based: we only have access to a set of samples $X \sim \mu$.
\end{itemize}

\aknote{to investigate much further} 
The Unadjusted Langevin Algorithm (ULA) seems much more adapted to the first setting, since it only requires the knowledge of $\nabla \log \mu$ while our algorithm requires the knowledge of $\mu$ (since $\nabla f_t$ involves an integration over $\mu$). However, in the sample-based setting, it may be difficult to adapt ULA by replacing $\nabla \log \mu$ in \eqref{eq:langevin_algorithm} by an estimator based on samples. Indeed, it has been the subject of a lot of work (see \cite{li2017gradient})\aknote{check reference}. In contrast, the gradient of $f_t$ can be 'easily' estimated by:
\begin{equation}
\widehat{\nabla f_k}(X_k)=\frac{1}{n}\sum_{i=1,\dots,n}\nabla_{X_k}k(x_i,X_k) - \frac{1}{n}\sum_{i=1,\dots,n}\nabla_{X_k}k(y_i,X_k)
\end{equation}
where $(x_1, \dots, x_n)\sim \rho_k$ and $(y_1, \dots, y_n)\sim \mu$. 
It is thus natural to consider the stochastic process:
\begin{equation}\label{eq:sample_based_stochastic_process}
X_{k+1}=X_k-\gamma_{k+1}\widehat{\nabla f_k}(X_k) 
\end{equation}
Let $\widetilde{\nu}_k$ be the distribution of~\eqref{eq:sample_based_stochastic_process}. Can we say something about $MMD^2(\widetilde{\nu_k}, \nu_k)$? If yes, we have rates as well.

\begin{remark}
	We point out here that algorithm~\eqref{eq:sample_based_stochastic_process} is different from the descent proposed by \cite{mroueh2018regularized}. 
\end{remark}

\begin{remark}
	Birth-Death Dynamics to improve convergence (see \cite{rotskoff2019global}).
\end{remark}
