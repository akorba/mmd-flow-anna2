In this subsection we assume that $MMD^2$ is $\lambda$-geodesically-convex. Conditions under which this holds will be provided in the next section.

\subsection{Analysis of the theoretical algorithm}

Equation~\eqref{eq:discretization} provides a theoretical algorithm to minimize $MMD^2(\cdot,\pi)$. The algorithm is only theoretical because it requires to compute $\nabla f_k(X_k)$.

This algorithm is the discretization of the Gradient flow associated to $MMD^2$. Since $MMD^2$ is $\lambda$-convex, using Theorem 11.1.4 of~\cite{ambrosio2008gradient}, the gradient flow $(\rho_t)$ satisfies
\begin{equation}
    \label{eq:evi}
    \frac12 \frac{d}{dt} W^2(\rho_t,\nu) + \frac{\lambda}{2}W^2(\rho_t,\nu) \leq MMD^2(\nu,\mu) - MMD^2(\rho_t,\pi)
\end{equation}
Unfortunately, $\lambda \leq 0$ (otherwise it would mean that $MMD^2$ is strongly-geodesically-convex and hence geodesically convex).

For the theoretical algorithm~\eqref{eq:discretization} we can expect a discretized version of~\eqref{eq:evi} to hold : \asnote{This should hold, I haven't proved it yet but I will do it later}
\begin{equation}
    \label{eq:evi-discrete}
    W^2(\rho_{k+1},\pi) \leq  W^2(\rho_{k},\pi) -2\gamma_{k+1}\left( \frac{\lambda}{2}W^2(\rho_{k},\pi) + MMD^2(\rho_{k+1},\pi) - MMD^2(\pi,\pi)\right)
\end{equation}
Since $\lambda \leq 0$ we cannot have a rate from this inequality (I think).
However, if $\sum \gamma_k < \infty$, using Robbins Siegmund lemma, we know that $W^2(\rho_{k},\pi)$ converges to some $\ell \geq 0$. \asnote{From this it might be possible to prove that $W^2(\overline{\rho_{k}},\pi)$ converges to zero, but it would be a lot of work. It looks like Pakes Hasminskii criterion}

\subsection{Another Lyapunov function}

In this section we try to use $MMD^2(\cdot,\pi)$ as a Lyapunov function (instead of $W^2(\cdot,\pi)$), like in Theorem 3.3 of Liu 2017. Once this is done, we can use the Gradient Lojasiewicz inequality to get a rate (see Bolte, it's like log sobolev inequality)

Taylor : 


\subsection{Sample-based setting}

Two settings are usually encountered in the sampling literature:
\begin{itemize}
	\item Density-based: $\mu$ is known up to a constant
	\item Sample-based: we only have access to a set of samples $X \sim \mu$.
\end{itemize}

\aknote{to investigate much further} 
The Unadjusted Langevin Algorithm (ULA) seems much more adapted to the first setting, since it only requires the knowledge of $\nabla \log \mu$ while our algorithm requires the knowledge of $\mu$ (since $\nabla f_t$ involves an integration over $\mu$). However, in the sample-based setting, it may be difficult to adapt ULA by replacing $\nabla \log \mu$ in \eqref{eq:langevin_algorithm} by an estimator based on samples. Indeed, it has been the subject of a lot of work (see \cite{li2017gradient})\aknote{check reference}. In contrast, the gradient of $f_t$ can be 'easily' estimated by:
\begin{equation}
\widehat{\nabla f_k}(X_k)= \frac{1}{n}\sum_{i=1,\dots,n}\nabla_{X_k}k(y_i,X_k) -\frac{1}{n}\sum_{i=1,\dots,n}\nabla_{X_k}k(x_i,X_k) 
\end{equation}
where $(x_1, \dots, x_n)\sim \rho_k$ and $(y_1, \dots, y_n)\sim \mu$. 
It is thus natural to consider the stochastic process:
\begin{equation}\label{eq:sample_based_stochastic_process}
Y_{k+1}=Y_k-\gamma_{k+1}\widehat{\nabla f_k}(Y_k) 
\end{equation}


\begin{proposition} Let $\widehat{\rho}_k$ be the distribution of~\eqref{eq:sample_based_stochastic_process}.\aknote{not sure, proof not complete}
	\begin{equation}
	MMD^2(\rho_k,\widehat{\rho_k})\le \frac{C}{n}
	\end{equation}
\end{proposition}
\begin{proof}
	Let $\mu, \nu$ in $\mathcal{P}(\X)$. Suppose that $k$ is bounded and measurable on $\X$, and that there exists $L_k$ such that $\forall x,y \in \X$, $\| k(x,.)-k(y,.) \|_{\kH}^2\le L_k \|x-y\|^2$. By Proposition 20 in \cite{sriperumbudur2010hilbert} we firstly get the following inequality between $MMD$ and $W_2$:
	\begin{align}
	MMD^2(\mu, \nu)	 &\le \inf_{\pi \in \Pi(\mu, \nu)} \int \| k(x,.)-k(y,.) \|_{\kH}^2 d\pi(\mu, \nu)\\
	&\le  \inf_{\pi \in \Pi(\mu, \nu)} \int L_k \| x-y \|^2 d\pi(\mu, \nu)= L_k W_2^2(\mu, \nu)
	\end{align}
	We now introduce the process:
	\begin{equation}\label{eq:intermediary_process}
	Z_{k+1}=Z_k-\gamma_{k+1}\widetilde{\nabla f_k}(Z_k) 
	\end{equation}
	where $\widetilde{\nabla f_k}(Z_k)=\int \nabla_{Z_k}k(X,Z_k)\diff \mu- \frac{1}{n}\sum_{i=1,\dots,n}\nabla_{Z_k}k(x_i,Z_k)$. Let $\widetilde{\rho}_k$ be the distribution of \eqref{eq:intermediary_process}.
	\begin{align}\label{eq:decompose_process}
	MMD^2(\widehat{\rho_k}, \rho_k) \le MMD^2(\widehat{\rho_k}, \widetilde{\rho_k})+ MMD^2(\widetilde{\rho_k}, \rho_k)
	\end{align}
	 Then, concerning the first term on r.h.s. of \eqref{eq:decompose_process}:
	 \begin{align}
	 W_2^2(\widehat{\rho_k}, \widetilde{\rho_k}) \le \inf_{\pi \in \Pi(\widehat{\rho_k}, \widetilde{\rho_k})} \int \| y-z \|^2 d\pi(\widehat{\rho_k}, \widetilde{\rho_k}) \le \int \|y-z\|^2 d\widehat{\rho_k}(y)d\widetilde{\rho_k}(z)\le \frac{C_1}{n}
	 \end{align}
	 where the last inequality results from Theorem 3 in \cite{jourdain2007nonlinear}. \aknote{we need Lipschitz continuity of the coefficient in the process... check}
	 Then, we try a similar proof than Theorem 3 in \cite{jourdain2007nonlinear} for the first r.h.s. in \eqref{eq:decompose_process}:
	 \begin{align}
	 \sup_{k}\|X_k-Y_k\|^2 \le C W_2^2(\widehat{\mu},\mu)
	 \end{align}
	where $\widehat{\mu}=\frac{1}{n}\sum_{i=1, \dots,n}\delta_{y_i}$ and $y_i \sim \mu$. For the first inequality we also need Lipschitz continuity of the coefficient in the process.
\end{proof}

\begin{remark}
	We point out here that algorithm~\eqref{eq:sample_based_stochastic_process} is different from the descent proposed by \cite{mroueh2018regularized}. 
\end{remark}

\begin{remark}
	Birth-Death Dynamics to improve convergence (see \cite{rotskoff2019global}).
\end{remark}
