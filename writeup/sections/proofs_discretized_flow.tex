
\subsection{Proofs of \cref{sec:discretized_flow}}

\subsubsection{Proof of \cref{prop:decreasing_functional}}

\begin{lemma}	\label{lem:grad_flow_lambda_version}
Let $\nu$ be a distribution in $\mathcal{P}_2(\X)$ and $\mu$ the target distribution such that $\F(\mu)=0$.  Let $\pi$ be an optimal coupling between $\nu$ and $\mu$, and $\rho_t$ the displacement geodesic defined by \cref{eq:displacement_geodesic} with its corresponding velocity vector  $v_t$ as defined in \cref{eq:continuity_equation}. Finally let $\phi(X)=\nabla f_{\nu,\mu}(X)$ the gradient of the witness function between $\mu$ and $\nu$. The following inequality holds: \manote{This should be a standard result, just need to cite it}
\begin{align*}
	\int \phi(x).(y-x) d\pi(x,y)
	\leq
	\F(\mu)- \F(\nu) -\int_0^1 \Lambda(\rho_s,v_s)(1-s)ds
\end{align*}

\end{lemma}
\begin{proof}
Recall that $\rho_t$ is given by $\rho_t = (s_t)_{\#}\pi$. By $\Lambda$-convexity of $\mathcal{F}$ the following inequality holds:
	\begin{align*}
		\mathcal{F}(\rho_{t})\leq (1-t)\mathcal{F}(\nu)+t \mathcal{F}(\mu) - \int_0^1 \Lambda(\rho_s,v_s)G(s,t)ds
	\end{align*}
	Hence by bringing $\mathcal{F}(\nu)$ to the l.h.s and dividing by $t$ and then taking its limit at $0$ it follows that:
	\begin{align*}
	\dot{\F}(\rho_t)\vert_{t=0}\leq \mathcal	{F}(\mu)-\mathcal{F}(\nu)-\int_0^1 \Lambda(\rho_s,v_s)(1-s)ds.	
	\end{align*}
	Moreover, by \cref{lem:derivatives_witness}, the time derivative of the witness function between $\nu$ and $\mu$ is well defined, so that $\dot{\F}(\rho_t)$ can be written as:
	\[
	\dot{\F}(\rho_t) = \langle f_{\mu,\rho_t},\dot{f}_{\mu,\rho_t} \rangle_{\kH}
	\]
	Now by \cref{lem:derivatives_witness},\cref{eq:inner_prod_deriative_witness} it follows that:
\[
\dot{\F}(\rho_t) = \int \nabla f_{\mu,\rho_t}(x).v_t(x)\diff \rho_t(x)
\]
By definition of $\rho_t$,  one can further write:
\[
\dot{\F}(\rho_t) = \int \nabla f_{\mu,\rho_t}(s_t(x,y)).(y-x)\diff \pi(x,y)
\]
where we used the fact that $v_t(s_t(x,y))=(y-x)$\manote{cite something}. Hence at $t=0$ we get:
\[
\dot{\F}(\rho_t)\vert_{t=0} = \int \nabla f_{\mu,\nu}(x).(y-x)\diff \pi(x,y)
\]
which shows the desired result.
\end{proof}





\begin{lemma}\label{lem:derivative_mmd}\manote{Notations still needs to be adjusted in this lemma}
	Let $\phi$ be a vector field on $\X$ and $\nu$ in $\mathcal{P}_2(\X)$. Consider the path $\delta_t$ between $\nu$ and $(I+\phi)_{\#}\nu$ given by:
	\begin{align*}
		\delta_t=  (I+t\phi)_{\#}\nu \qquad \forall t\in [0,1]
	\end{align*}
The time derivative of $\mathcal{F}(\delta_t)$ is given by:
	\begin{align*}
		\dot{\F}(\delta_t)&=\int \nabla f_{\mu,\delta_t}(x+t\phi(x)) \phi(x)d\nu(x)\\
	\end{align*}
where $f_{\mu,\delta_t}$ is the witness function between $\mu$ and $\delta_t$ as defined in \cref{eq:witness_function}.	
	Moreover, under \cref{assump:bounded_trace,assump:bounded_hessian}, the second time derivative satisfies:
	\begin{align*}
		\ddot{\F}(\delta_t) \vert \leq 3L \int \Vert \phi(x) \Vert^2 d\nu(x)
	\end{align*}
	where $L$ is a positive constant defined in \cref{assump:bounded_trace,assump:bounded_hessian}.
	
\end{lemma}
\begin{proof}
For simplicity, we write $f_t$ instead of $f_{\mu,\delta_t}$.
We start by computing the first derivative. Recalling that $\mathcal{F}(\delta_t)$ is given by $\frac{1}{2}\Vert f_t\Vert^2_{\kH} $, it follows that:
\[
\dot{\F}(\delta_t)=\langle f_{t},\frac{df_{t}}{dt}\rangle_{\kH}.
\]
Using the definition
of $\delta_{t}=(I+t\phi)_{\#}\nu$ we can write:\aknote{$\pi_t$? guess this corresponds to the paragraph below}
\[
\frac{df_{t}}{dt}=\int \phi(X).\nabla k(\pi_{t}(X),.)d\nu(X),
\]
hence:
\[
\frac{d\mathcal{F}(\delta_{t})}{dt}=2\int\phi(X).\nabla f_{t}(\pi_{t}(X))d\nu(X)
\]
Now the second derivative is obtained by direct derivation of the above expression:
	\begin{align*}
		\frac{d^2 \mathcal{F}(\delta_t)}{dt^2} =& \int \phi(X)^THf_t(\pi_t(X))\phi(X)d\nu(X)\\ 
		&+\int \phi(X)^T\nabla_x\nabla_y k(\pi_t(X),\pi_t(X')) ) \phi(X')d\nu(X)d\nu(X') 
	\end{align*}
where $Hf_t$ is the hessian of $f_t$ in space and  $\nabla_x\nabla_y k(x,y)$ is the cross diagonal term of the hessian of $k$. By \cref{assump:bounded_hessian}, the first term in the above equation can be easily upper-bounded by:
\begin{align*}
	4L \int \Vert \phi(X)\Vert^2d\nu(X)  
\end{align*}
The last term can also be upper-bounded by $2L$ by \cref{assump:bounded_trace}.
\end{proof}



\begin{lemma}\label{lem:derivative_mmd_augmented}\manote{Notations still needs to be adjusted in this lemma}
	Let $\phi$ be a map from  $\X times\mathcal{U} $ to $\X$  and $\nu$ in $\mathcal{P}_2(\X \times \mathcal{U})$. Denote by  $P_x$ as the projection map from $\X times\mathcal{U}$ to $\X$ and consider the path $\delta_t$ between $(P_x)_{\#}\nu$ and $(P_x+\phi)_{\#}\nu$ given by:
	\begin{align*}
		\delta_t=  (P_x+t\phi)_{\#}\nu \qquad \forall t\in [0,1]
	\end{align*}
The time derivative of $\mathcal{F}(\delta_t)$ is given by:
	\begin{align*}
		\dot{\F}(\delta_t)&=\int \nabla f_{\mu,\delta_t}(x+t\phi(x,u)) \phi(x,u)d\nu(x,u)\\
	\end{align*}
where $f_{\mu,\delta_t}$ is the witness function between $\mu$ and $\delta_t$ as defined in \cref{eq:witness_function}.	
	Moreover, under \cref{assump:bounded_trace,assump:bounded_hessian}, the second time derivative satisfies:
	\begin{align*}
		\ddot{\F}(\delta_t) \vert \leq 3L \int \Vert \phi(x,u) \Vert^2 d\nu(x,u)
	\end{align*}
	where $L$ is a positive constant defined in \cref{assump:bounded_trace,assump:bounded_hessian}.
	
\end{lemma}
\begin{proof}
For simplicity, we write $f_t$ instead of $f_{\mu,\delta_t}$.
We start by computing the first derivative. Recalling that $\mathcal{F}(\delta_t)$ is given by $\frac{1}{2}\Vert f_t\Vert^2_{\kH} $, it follows that:
\[
\dot{\F}(\delta_t)=\langle f_{t},\frac{df_{t}}{dt}\rangle_{\kH}.
\]
Using the definition
of $\delta_{t}=(I+t\phi)_{\#}\nu$ we can write:\aknote{$\pi_t$? guess this corresponds to the paragraph below}
\[
\frac{df_{t}}{dt}=\int \phi(X).\nabla k(\pi_{t}(X),.)d\nu(X),
\]
hence:
\[
\frac{d\mathcal{F}(\delta_{t})}{dt}=2\int\phi(X).\nabla f_{t}(\pi_{t}(X))d\nu(X)
\]
Now the second derivative is obtained by direct derivation of the above expression:
	\begin{align*}
		\frac{d^2 \mathcal{F}(\delta_t)}{dt^2} =& \int \phi(X)^THf_t(\pi_t(X))\phi(X)d\nu(X)\\ 
		&+\int \phi(X)^T\nabla_x\nabla_y k(\pi_t(X),\pi_t(X')) ) \phi(X')d\nu(X)d\nu(X') 
	\end{align*}
where $Hf_t$ is the hessian of $f_t$ in space and  $\nabla_x\nabla_y k(x,y)$ is the cross diagonal term of the hessian of $k$. By \cref{assump:bounded_hessian}, the first term in the above equation can be easily upper-bounded by:
\begin{align*}
	4L \int \Vert \phi(X)\Vert^2d\nu(X)  
\end{align*}
The last term can also be upper-bounded by $2L$ by \cref{assump:bounded_trace}.
\end{proof}


Let $  \nu$ and $\nu'$ be two distributions and $\Pi$ a coupling between $\nu$ and $\nu'$. We consider the path $\rho_t$ defined as $\rho_t=(\pi_t)_{\#}\Pi$ where $\pi_t(X,Y)=(1-t)X+tY$. It is possible to provide an expression for the time derivative of $\mathcal{F}{\rho_t}$. This is given by ?\\\aknote{beware, not finished!}

%\begin{lemma}\label{lem:time_derivative}
%The time derivative of $\mathcal{F}(\rho_t)$ is given by:
%	\begin{align*}
%		\frac{d \mathcal{F}(\rho_t)}{dt}&=\int \nabla f_t(\pi_t(X)).(Y-X)d\Pi(X,Y)\\
%	\end{align*}
%	where $f_t$ is the witness function at time $t$ and is given by:
%	\begin{align}
%	f_t(x)=\rho_t(k(X,x))-\mu(k(X,x)) \qquad \forall t\in [0,1]
%	\end{align}	
%\end{lemma}
%\begin{proof}
%	The proof is very similar to the one in \cref{lem:derivative_mmd}. Indeed we still have
%	\begin{align*}
%		\frac{d \mathcal{F}(\rho_t)}{dt} = \langle f_t , \frac{df_t}{dt} \rangle
%	\end{align*}
%	And the time derivative of $f_t$ at each point $x\in\mathbb{R}^d$ is obtained by direct computation:
%	\begin{align*}
%		 \frac{df_t}{dt}= \int \nabla k(\pi_t(X,Y),.).(Y-X)d\Pi(X,Y)
%	\end{align*}
%	The result follows using the reproducing property in $\kH$.
% \end{proof}




\begin{proof}
	Here we consider a path between $\nu_n$ and $\nu_{n+1}$ of the form:
	\begin{align*}
	\rho_t	=(I-\gamma t\phi_n)_{\#}\nu_n
	\end{align*}
	The function $t\mapsto \mathcal{F}(\rho_t)$ is twice differentiable, hence one can use a Taylor expansion with integral remainder to get:
	\begin{align}\label{eq:taylor_expansion}
	\mathcal{F}(\nu_{n+1})-\mathcal{F}(\nu_{n})=\mathcal{F}(\rho_1)-\mathcal{F}(\rho_0) = \frac{d \mathcal{F}(\rho_t) }{dt}\vert_{t=0}+ \frac{1}{2} \int_0^1 \frac{d^2 \mathcal{F}(\rho_t)}{dt^2}(1-t)^2 dt 
	\end{align} 
	By taking $\phi=-\gamma \phi_n$ in \cref{lem:derivative_mmd} we have that:
	\begin{align*}
	\frac{d \mathcal{F}(\rho_t) }{dt} = -\gamma \int \nabla f_n(X).\phi_n(X)d\nu_n(X)=-\gamma \int \Vert \phi_n(X) \Vert^2 d\nu_n(X)
	\end{align*}
	since $\nabla f_n=\phi_n$.
	Moreover, by \cref{assump:bounded_trace,assump:bounded_hessian} it follows from \cref{lem:derivative_mmd} that:
	\begin{align}\label{eq:upper_bound_1}
	\vert \frac{d^2 \mathcal{F}(\rho_t) }{dt^2}   \vert\leq L\int \Vert \phi_n(X) \Vert^2 d\nu_n(X)
	\end{align}
	Using \cref{eq:taylor_expansion,eq:upper_bound_1} the result follows.
\end{proof}

\subsubsection{Proof of \cref{prop:evi}}

\begin{proof}
	Let $\Pi^n$ be the optimal coupling between $\nu_n$ and $\mu$, then the optimal transport between $\nu_n$ and $\mu$ is given by:
	\begin{align}
	W_2^2(\mu,\nu_n)=\int \Vert X-Y \Vert^2 d\Pi^n(\nu_n,\mu)
	\end{align}
	Moreover, consider $Z=X-\gamma \phi_n(X)$ where $(X,Y)$ are samples from $\Pi^n$. It is easy to see that $(Z,Y)$ is a coupling between $\nu_{n+1}$ and $\mu$, therefore, by definition of the optimal transport map between $\nu_{n+1}$ and $\mu$ it follows that:
	\begin{align}\label{eq:optimal_upper-bound}
	W_2^2(\nu_{n+1},\mu)\leq \int \Vert X-\gamma \phi_{n}(X)-Y\Vert^2 d\Pi^n(\nu_n,\mu)
	\end{align}
	By expanding the r.h.s in \cref{eq:optimal_upper-bound}, the following inequality holds:
	\begin{align}\label{eq:main_inequality}
	W_2^2(\nu_{n+1},\mu)\leq W_2^2(\nu_{n},\mu) -2\gamma \int \langle \phi_n(X), X-Y \rangle d\Pi^n(\nu_n,\mu)+ \gamma^2D(\nu_n)
	\end{align}
	where $D(\nu_n) = \int \Vert \phi_n(X)\vert^2 d\nu_n $.
	An upper-bound on $-2\gamma \int \langle \phi_n(X), X-Y \rangle d\Pi^n(\nu_n,\mu) $ in terms of the loss functional can be obtained using the $\Lambda$ displacement convexity of $\nu\mapsto \F(\nu)$. Indeed, by \cref{lem:grad_flow_lambda_version} it holds that:
	\begin{align}\label{eq:flow_upper-bound}
	-2\gamma \int  \phi(X).(X-Y) d\Pi(\nu,\mu)
	\leq
	-2\gamma\left(\F(\nu)- \F(\mu) +K(\rho^n)\right)
	\end{align}
	where $(\rho^n_t)_{0\leq t \leq 1}$ is a constant-speed geodesic from $\nu_n$ to $\mu$ and $K(\rho^n):=\int_0^1 \Lambda(\rho^n_s,\dot{\rho^n}_s)(1-s)ds$\aknote{as Adil said, we should try to quantify this}. Note that when $K(\rho^n)\leq 0$ it falls back to the convex setting.
	Therefore, the following inequality holds:
	\begin{align}
	W_2^2(\nu_{n+1},\mu)\leq W_2^2(\nu_{n},\mu) - 2\gamma\left(\F(\nu_n)- \F(\mu) +K(\rho^n)\right) +\gamma^2 D(\nu_n)
	\end{align}
	Now we introduce a term involving $\F(\nu_{n+1})$. The above inequality becomes:
	\begin{align}
	W_2^2(\nu_{n+1},\mu)\leq & W_2^2(\nu_{n},\mu) - 2\gamma\left(\F(\nu_{n+1})- \F(\mu) +K(\rho^n)\right) \\
	&+\gamma^2 D(\nu_n) -2\gamma (\F(\nu_n)-\F(\nu_{n+1}))
	\label{eq:main_ineq_2}
	\end{align}
	It is possible to upper-bound the last two terms by a negative quantity when the step-size is small enough. This is mainly a consequence of the smoothness of the functional $\F$ and the fact that $\nu_{n+1}$ is obtained by following the steepest direction of $\F$ starting from $\nu_n$. \cref{prop:decreasing_functional} makes this statement more precise and enables to get the following inequality:
	\begin{align}
	\gamma^2 D(\nu_n) -2\gamma (\F(\nu_n)-\F(\nu_{n+1})\leq -\gamma^2 (1-\gamma L)D(\nu_n),
	\label{eq:decreasing_functional}
	\end{align}
	where $L$ is a constant that depends only on the choice of the kernel $k$ in $\F$. Combining  \cref{eq:main_ineq_2} and \cref{eq:decreasing_functional} it follows that:
	\begin{align}
	2\gamma(\F(\nu_{n+1})-\F(\mu))+\gamma^2(1-\gamma L)D(\nu_n)
	\leq 
	W_2^2(\nu_n,\mu)-W_2^2(\nu_{n+1},\mu)-2\gamma K(\rho^n).
	\label{eq:main_final}
	\end{align}
\end{proof}


Here we provide a proof for \cref{prop:decreasing_loss_iterations}


\subsubsection{Proof of \cref{prop:decreasing_loss_iterations}}\label{eq:proof_decreasing_noisy_loss}
\begin{proof}
To simplify notations, we write $V = \nabla f_{\mu,\nu_n}$ and 
	\[
	\mathcal{D}_{\beta_n}(\nu_n)  = \int \Vert V(x+\beta_n u) \Vert^2 g(u)\diff \nu_n \diff u  
	\]
	Recall that a sample $X_{n+1}$ from $\nu_{n+1}$ is obtained using 
	\begin{align}
		X_{n+1} = X_n - \gamma V(X_n+ \beta_n U_n)
	\end{align}
	where $X_n$ is a sample from $\nu_n$ and $U_n$ is a sample from a standard gaussian distribution that is independent from $X_n$. Moreover, $\beta_n$ is a non-negative scalar satisfying:
	\begin{align}\label{eq:control_noise_level_bis}
		8L^2\beta_n^2 \F(\nu_n) \leq \mathcal{D}_{\beta_n}(\nu_n)  
	\end{align}
	 Consider now the map $(x,u)\mapsto T_t(x)= x - \gamma tV(x+\beta_n u)$ for $0\leq t\leq 1$, then $\nu_{n+1}$ is obtained as a push-forward of $\nu_n\otimes g$ by $T_1$: $\nu_{n+1} = (T_1)_{\#}(\nu_n\otimes g)$. Moreover, the curve $\rho_t = (T_t)_{\#}(\nu_n\otimes g)$ is a path from $\nu_n$ to $\nu_{n+1}$. 
	 We will evaluate now the difference between $\mathcal{F}(\nu_{n+1})$ and $\mathcal{F}(\nu_{n})$ using  Taylor expansion with integral remainder since $t\mapsto \F(\rho_t)$ is continuously twice differentiable:
	\begin{align}\label{eq:taylor_expansion}
	\mathcal{F}(\nu_{n+1})-\mathcal{F}(\nu_{n})= \frac{d \mathcal{F}(\rho_t) }{dt}\vert_{t=0}+ \frac{1}{2} \int_0^1 \frac{d^2 \mathcal{F}(\rho_t)}{dt^2}(1-t)^2 dt 
	\end{align} 
	By taking $\phi(x,u) = - \gamma V(x+\beta_n u)$ and $ \nu = \nu_n\otimes g $  and applying \cref{lem:derivative_mmd_augmented} it follows:
	\begin{align*}
	\frac{d \mathcal{F}(\rho_t) }{dt} =\int V(T_t(x,u)).\phi(x,u) g(u)\diff\nu_n(x)\diff u
	\end{align*}
	Evaluating at $t=0$ one gets:
	\begin{align}
		\dot{\F}(\rho_t)\vert_{t=0} = -\gamma \int  V(x).V(x+\beta_n u) g(u)\diff\nu_n(x)\diff u
	\end{align}
	Moreover, by \cref{assump:bounded_trace,assump:bounded_hessian} it follows again from \cref{lem:derivative_mmd_augmented} that for all $0\leq t\leq 1$:
	\begin{align}\label{eq:upper_bound_1}
	\vert  \ddot{\mathcal{F}}(\rho_t)   \vert\leq \gamma^2 L \mathcal{D}_{\beta_n}(\nu_n) 
	\end{align}
	Where $L$ is a constant that only depends on the kernel $k$. We have shown so far that:
	\[
	\mathcal{F}(\nu_{n+1})-\mathcal{F}(\nu_{n})\leq   -\gamma \int  V(x).V(x+\beta_n u) g(u)\diff\nu_n(x)\diff u +  \frac{\gamma^2L}{2}\mathcal{D}_{\beta_n}(\nu_n) 
	\]
	Adding and substracting  $\gamma \mathcal{D}_{\beta_n}(\nu_n)$ in the equation above, it follows directly that:
\begin{align}\label{eq:penultimate}
\begin{split}
			\mathcal{F}(\nu_{n+1})-\mathcal{F}(\nu_{n} )\leq &   -\gamma (1-\frac{\gamma L}{2} )\mathcal{D}_{\beta_n}(\nu_n)
 \\
 &+ \gamma\int  (V(x+\beta_n u) -V(x)).V(x+\beta_n u) g(u)\diff\nu_n(x)\diff u	
\end{split}
\end{align}
We shall control now the last term in \cref{eq:penultimate}. Recalling that   $ V_i(x) = \partial_i f_{\mu,\nu_n}(x) = \langle f_{\mu,\nu_n} , \partial_i k(x,.)\rangle $,  we have by Cauchy-Schwartz in the RKHS space that
\[
\Vert V(x+\beta_n u) -V(x)\Vert^2\leq \Vert f_{\mu,\nu_n} \Vert_{\mathcal{H}}^2  \Vert k(x+\beta_n u,.) -k(x,.)\Vert_{\mathcal{H}}^2\qquad \forall x,u \in \X
\]
Now using \cref{assump:bounded_hessian} it follows that $\Vert k(x+\beta_n u,.) -k(x,.)\Vert_{\mathcal{H}}\leq L \beta_n \Vert u \Vert  $, hence:
\[
\Vert V(x+\beta_n u) -V(x)\Vert^2 \leq L^2\beta^2_n \Vert f_{\mu,\nu_n} \Vert_{\mathcal{H}}^2 \Vert u \Vert^2
\]
Now integrating both sides w.r.t. $\nu_n$ and $g$ and recalling that $g$ is a standard gaussian, we have:
\begin{align}
	 \int  \Vert V(x+\beta_n u) -V(x)\Vert^2 g(u)\diff\nu_n(x)\diff u
\leq 
	L^2\beta^2_n\Vert f_{\mu,\nu_n} \Vert_{\mathcal{H}}^2
\end{align}
Getting back to \cref{eq:penultimate} and applying Cauchy-Schwarz in $L_2(\nu_n\otimes g)$ it follows:
\begin{align}
	\mathcal{F}(\nu_{n+1})-\mathcal{F}(\nu_{n} )\leq &   -\gamma (1-\frac{\gamma L}{2} )\mathcal{D}_{\beta_n}(\nu_n) +\gamma L\beta_n\Vert f_{\mu,\nu_n} \Vert_{\mathcal{H}}\mathcal{D}^{\frac{1}{2}}_{\beta_n}(\nu_n)
\end{align}
It remains to notice that $\Vert f_{\mu,\nu_n} \Vert_{\mathcal{H}}^2 = 2\F(\nu_n)$ and that $\beta_n$ satisfies \cref{eq:control_noise_level_bis}, so that:
\begin{align}
	\mathcal{F}(\nu_{n+1})-\mathcal{F}(\nu_{n} )\leq &   -\frac{\gamma}{2} (1-\gamma L )\mathcal{D}_{\beta_n}(\nu_n)
\end{align}
\end{proof}


\subsubsection{Proof of \cref{th:rates_mmd}}

\begin{proof}
	Iterating in \cref{eq:evi} we get:
	\begin{align}
	2\gamma \sum_{j=1}^{n+1} (\F(\nu_{j}) - \F(\mu)) \leq W_2^2(\nu_0,\mu) - 2\gamma \sum_{j=0}^n K(\rho^j)
	\end{align}
	Let us denote $\bar{K}$ the average value\asnote{$K(\rho^j)$ is bounded in $j$?} of $(K(\rho^j))_{0\leq j \leq n}$ over iterations from $0$ to $n$. Using \cref{lem:mixture_convexity} we have:
	\begin{align}
	\F(\bar{\nu}_{n+1})-\F(\mu)\leq  \frac{W_2^2(\nu_0,\mu)}{2 \gamma (n+1)} -\bar{K}
	\end{align}
	Now, consider the Lyapunov function $L_n = n \gamma (\F(\nu_n) - \F(\mu)) + \frac12 W_2^2(\nu_n,\mu)$. Then,
	\begin{align*}
	L_{n+1} &= n\gamma(\F(\nu_{n+1}) - \F(\mu)) + \gamma(\F(\nu_{n+1}) - \F(\mu)) + \frac12 W_2^2(\nu_{n+1},\mu)\\
	&\leq n\gamma(\F(\nu_{n+1}) - \F(\mu)) + \frac12 W_2^2(\nu_n,\mu)-\gamma K(\rho^n)\\
	&\leq n\gamma(\F(\nu_{n}) - \F(\mu)) + \frac12 W_2^2(\nu_n,\mu)-\gamma K(\rho^n) -n\gamma^2 (1-\frac{\gamma}{2}L )\int \Vert \phi_n(X)\Vert^2 d\nu_n \\
	&\leq  L_n - \gamma K(\rho^n).
	\end{align*}
	where we used ~\cref{prop:decreasing_functional} in the penultimate inequality\asnote{Les deux derniers termes pourraient-ils se manger par miracle?}.
	Finally, 
	\begin{equation}
	n\gamma (\F(\nu_{n}) - \F(\mu)) \leq L_n \leq L_0 -\gamma \sum_{j = 0}^{n-1} K(\rho^j)
	\end{equation}
\end{proof}

\subsubsection{Proof of \cref{prop:sample_based_rates}}

\begin{proof} 
	Introduce the theoretical process associated to \eqref{eq:sample_based_process}, i.e. associated to the target distribution $\widehat{\mu}$:
	\begin{align}\label{eq:intermediary_process}
	\widetilde{X}_t=X_{0}+\int_{0}^t \nabla f_{\widehat{\mu}, \widetilde{\nu}_s}(\widetilde{X}_s)ds \quad \text{for t in [0,T]}
	%	&\forall s \in [0,T]\;,\quad \widetilde{\rho}_s \text{ denotes the probability distribution of } X_s
	\end{align}
	where $\widetilde{\nu}_t \text{ denotes the probability distribution of } X_t \text{for all} t>0$. The convergence of the empirical measure of the particle system \eqref{eq:sample_based_process} to the solution of \eqref{eq:intermediary_process} has been stated under the name propagation of chaos (see \cite{kac1956foundations}, \cite{sznitman1991topics}), and is given by:
	\aknote{at some point, I was hoping for a uniform in time propagation of chaos (i.e. the constant does not increase with time $t$), check for instance \cite{durmus2018elementary}. However, I am not sure it is possible in our case. Apparently, when there are several solutions/invariant measures for \eqref{eq:continuity_mmd}}
	\begin{equation}
	W_1(\widetilde{\nu}_t,\widehat{\nu}_t^N)\le \frac{C(t)}{\sqrt{n}}
	\end{equation}
	Then, by \cref{lem:mmd_w2} we have that $MMD^2(\widetilde{\rho}_t,\widehat{\rho}_t^N)\le C_1(t)/n$.%We now turn to bounding the distance between $\nu_t$ and $\widetilde{\nu_t}$.
	
	
	We now turn to bounding the distance between $\mu$ and $\widetilde{\rho_t}$. By the triangle inequality:
	\begin{equation}\label{eq:decomposition}
	MMD^2(\mu, \widetilde{\nu}_t)\le MMD^2(\mu, \widehat{\mu})+MMD^2(\widehat{\mu}, \widetilde{\nu_t})\le L_k^2 W_1^2(\mu, \widehat{\mu})+MMD^2(\widehat{\mu}, \widetilde{\nu_t})
	\end{equation}
	where the last inequality results from \cref{lem:mmd_w2}. Firstly, the first r.h.s. term in \eqref{eq:decomposition} can be upper bounded since it was shown in \cite{dudley1969speed} that when $d > 2$, if $\mu$ has a compact support in $\R^d$ then:\aknote{there's maybe a better rate in MMD, in $\sqrt{n}$, since it is twice diff in $\rho$, see Lemma 5.10 in \url{https://arxiv.org/pdf/1804.08542.pdf}}
	\begin{equation}
	\E[W_1^2(\widehat{\mu},\mu)]\le \frac{C}{n^{\frac{1}{d}}}
	\end{equation}
	\begin{remark}
		Note that more recently, sharper rates of convergence  for $W_p(\widehat{ \mu}, \mu)$, for $p\ge 1$, have been computed in \cite{weed2017sharp} for a larger class of measures. These rates involve an intrinsic dimension of the measure $\mu$ (its Wassertein dimension). 
	\end{remark}
	Let $C_2=C L_k^2$. Then, for the second term in \eqref{eq:decomposition}, we can apply the rates of convergence for the time continuous flow, applied to the process $\widetilde{X_t}$. Hence, if $\Vert \widetilde{\rho}_t  - \widehat{\mu} \Vert_{\dot{H}^{-1}(\widetilde{\rho}_t)} \leq C \; \forall t\geq 0$, by \cref{prop:lojasiewicz} we have $MMD^2(\widehat{ \mu},\widehat{\rho}_t)\le C_3/t$. 
\end{proof}