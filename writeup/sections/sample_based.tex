\subsection{Convergence of the space discretized flow - Sample-based setting}

Two settings are usually encountered in the sampling literature: one is called \textit{density-based}, i.e. $\mu$ is known up to a constant, and the other is called \textit{sample-based}, i.e. we only have access to a set of samples $X \sim \mu$.
The Unadjusted Langevin Algorithm (ULA), which involves a time-discretized version of \eqref{eq:langevin_diffusion}, seems much more adapted to the first setting, since it only requires the knowledge of $\nabla \log \mu$, whereas our algorithm requires the knowledge of $\mu$ (since $\nabla f_t$ involves an integration over $\mu$). However, in the sample-based setting, it may be difficult to adapt the ULA algorithm, since it would require firstly to estimate $\nabla \log(\mu)$ based on a set of samples of $\mu$, before plugging this estimate in the update of the algorithm. This problem, sometimes referred to as \textit{score estimation} in the literature, has been the subject of a lot of work but remains hard especially in high dimensions (see \cite{sutherland2017efficient,li2018gradient,shi2018spectral}). In contrast, the gradient of $f_t$ can be 'easily' estimated by:
\begin{equation}
\widehat{\nabla f_t}(z)= \frac{1}{n}\sum_{i=1}^{n}\nabla_{z}k(u_i,z) -\frac{1}{n}\sum_{i=1}^{n}\nabla_{z}k(v_i,z) 
\end{equation}
where $(u_1, \dots, u_n)\sim \mu$ and $(v_1, \dots, v_n)\sim \rho_t$. We denote by $\widehat{ \mu}=\sum_{j=1}^{n}\delta_{u_i}$. In the sample-based setting, i.e. given $\widehat{\mu}$, since we do not have access to $(v_1, \dots, v_n)\sim \rho_t$, it is natural to consider the following system of $n$ interacting particles (sometimes referred to as \textit{mean-field interaction} in mathematical physics and stochastic analysis):
\begin{align}\label{eq:sample_based_process}
&\widehat{X}_t^{j,n}=X_{0}+\int_{0}^t \sigma_{\widehat{\mu}}(\widehat{X}_s^{j,n}, \widehat{\rho}_s^n)ds \quad \text{for t in [0,T]}\\
&\forall s \in [0,T]\;,\quad \widehat{\rho}_s^n=\sum_{j=1}^{n} \delta_{\widehat{X}_s^{j,n}} \text{ denotes the empirical measure } 
\end{align}
where the coefficient $\sigma_{\mu}$ in \eqref{eq:theoretical_process} has been replaced by $\sigma_{\widehat{\mu}}$. The convergence of the empirical measure of the particle system \eqref{eq:sample_based_process} to the solution of \eqref{eq:theoretical_process} has been stated under the name propagation of chaos (see \cite{kac1956foundations}, \cite{sznitman1991topics}).



\begin{proposition}\aknote{proof not complete} Let $\widehat{\rho}_t^N$, be the distributions of the process \eqref{eq:sample_based_process}.
	\begin{equation}
	MMD^2(\rho_t,\widehat{\rho}_t^N)\le ?+ \frac{C_2}{n^{\frac{1}{d}}} + \frac{C_3}{t}
	\end{equation}
\end{proposition}
\begin{proof} 
	Introduce the process:
	\begin{align}\label{eq:intermediary_process}
	&\widetilde{X}_t=X_{0}+\int_{0}^t \sigma_{\widehat{\mu}}(\widetilde{X}_s, \widetilde{\rho}_s)ds \quad \text{for t in [0,T]}\\
	&\forall s \in [0,T]\;,\quad \widetilde{\rho}_s \text{ denotes the probability distribution of } X_s
	\end{align}
	We firstly use new types of bound in the literature, characterizing a \textit{uniform in time} propagation of chaos.
	\begin{lemma}
		By \cite{durmus2018elementary}, we have a uniform in time propagation of chaos:\aknote{cannot use in our case. looking for a bound now}
		\begin{equation}
		W_1(\widetilde{\rho}_t,\widehat{\rho}_t^N)\le \frac{A}{\sqrt{N}}
		\end{equation}
	\end{lemma}
\begin{proof}
	%Indeed, we verify the three assumptions. 
\end{proof}
	We now turn to bounding the distance between $\mu$ and $\widetilde{\rho_t}$. By the triangle inequality:
	\begin{equation}\label{eq:decomposition}
	MMD^2(\mu, \widetilde{\rho}_t)\le MMD^2(\mu, \widehat{\mu})+MMD^2(\widehat{\mu}, \widetilde{\rho_t})\le L_k^2 W_1^2(\mu, \widehat{\mu})+MMD^2(\widehat{\mu}, \widetilde{\rho_t})
	\end{equation}
	where the last inequality results from \cref{lem:mmd_w2}. Firstly, the first r.h.s. term in \eqref{eq:decomposition} can be upper bounded since it was shown in \cite{dudley1969speed} that when $d > 2$, if $\mu$ has a compact support in $\R^d$ then:\aknote{there's maybe a better rate in MMD, in $\sqrt{n}$, since it is twice diff in $\rho$, see Lemma 5.10 in \url{https://arxiv.org/pdf/1804.08542.pdf}}
	\begin{equation}
	\E[W_1^2(\widehat{\mu},\mu)]\le \frac{C}{n^{\frac{1}{d}}}
	\end{equation}
	\begin{remark}
		Note that more recently, sharper rates of convergence  for $W_p(\widehat{ \mu}, \mu)$, for $p\ge 1$, have been computed in \cite{weed2017sharp} for a larger class of measures. These rates involve an intrinsic dimension of the measure $\mu$ (its Wassertein dimension). 
	\end{remark}
	Let $C_2=C L_k^2$. Then, for the second term in \eqref{eq:decomposition}, we can apply the rates of convergence for the time continuous flow, applied to the process $\widetilde{X_t}$. Hence, if $\Vert \widetilde{\rho}_t  - \widehat{\mu} \Vert_{\dot{H}^{-1}(\widetilde{\rho}_t)} \leq C \; \forall t\geq 0$, by \cref{prop:lojasiewicz} we have $MMD^2(\widehat{ \mu},\widehat{\rho}_t)\le C_3/t$. 
\end{proof}