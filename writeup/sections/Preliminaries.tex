
\section{Gradient flow of the MMD in $W_2$}\label{sec:gradient_flow}
In this section we introduce the gradient flow of the Maximum Mean Discrepancy (MMD) and provide some of its properties. We start by briefly reviewing the MMD introduced in \cite{gretton2012kernel}, more details are provided in \cref{sec:rkhs}. In all the paper, $\X\subset\R^d$ is the closure of a convex open set.
\paragraph{Maximum Mean Discrepancy.}\label{subsec:MMD}
Given a characteristic kernel $k$ defined on $\X$, we denote by $\kH$ its corresponding RKHS (see \cite{smola1998learning}). The space $\kH$ is a Hilbert space with inner product $\langle .,. \rangle_{\kH}$ and corresponding norm $\Vert . \Vert_{\kH}$.
When the kernel has at most a quadratic growth, it is possible to define a distance on $\mathcal{P}_2(\X)$ called the Maximum Mean Discrepancy. It is given as the RKHS norm of the \textit{witness function} $f_{\mu,\nu}$ between $\mu$ and $\nu$ which is by definition the difference between the mean embeddings of $\nu$ and $\mu$: 
\begin{align}\label{eq:witness_function}
MMD(\mu,\nu) = \Vert f_{\mu,\nu} \Vert_{\kH}, \qquad f_{\nu,\mu}(z) = \int k(x,z)\diff \nu(x) - \int k(x,z)\diff \mu(x)  \qquad z\in \X
\end{align}
Throughout all the paper, $\mu$ will be fixed and $\nu$ could vary, hence, we will only consider the dependence in $\nu$ and denote by $\F(\nu)= \frac{1}{2}MMD^2(\mu,\nu)$. One noticeable property of $\F(\nu)$ is that its differential evaluated at $\nu$ is exactly given by $f_{\mu,\nu}$:
\begin{proposition}\label{prop:differential_mmd}
	For any finite signed measure $\chi$ the following holds:
	\begin{align}
		\lim_{\epsilon\rightarrow 0} \epsilon^{-1}(\F(\nu +\epsilon\chi) - \F(\nu)) = \int f_{\mu,\nu}(x)d\chi(x) 
	\end{align}
\end{proposition}
This property will be key to construct a gradient flow of the MMD in $W_2$ as we will see in \cref{paragraph:flow_MMD}.
 Interestingly, $\F(\nu)$ can be written by means of an external potential $V$, an interaction potential $W$ and a constant off-set $C$:
\begin{align}\label{eq:potentials}
V(x)=-\int  k(x,x')\mu(x'), \qquad
W(x,x')=k(x,x'), \qquad
C = \frac{1}{2} \int k(x,x')\diff\mu(x) \diff\mu(x') 
\end{align}
This leads to a "free-energy" expression for $\mathcal{F}(\nu)$:
\begin{align}\label{eq:mmd_as_free_energy}
	\F(\nu) = \int V(x) \diff \nu(x) +\frac{1}{2} \int W(x,y)\diff \nu(x)\diff \nu(y) + C.
\end{align}
If $X$ and $Y$ are two samples or particles following $\nu$, then $V$ reflects the external potential generated by $\mu$ and acting on each particle $X$ and $Y$, while $W$ would reflect the potential arising from the interactions between the two particles $X$ and $Y$. This formulation will be useful to define a gradient flow of $\F(\nu)$ in \cref{paragraph:flow_MMD}. 
%When samples from both $\mu$ and $\nu$ are available \cref{eq:closed_form_MMD} can be estimated using those samples.
%by finding a function $g$ in the unit ball $\mathcal{B} = \{ g\in \kH : \quad \Vert g\Vert_{\kH}\leq 1 \}$ that maximizes the mean difference between two given distributions $\mu$ and $\nu$. Such a distance is called the Maximum Mean Discrepancy  (MMD) \cite{Gretton:2012}:
%\begin{align}\label{eq:MMD}
%MMD(\mu,\nu) = \sup_{g\in \mathcal{B}} \int g\diff\mu - \int g \diff\nu
%\end{align}
%The optimal function  $g^*$ in $\mathcal{B}$ is then proportional to the  witness function between $\nu$
% and $\mu$ (see  \cite{gretton2012kernel}):
%\begin{align}\label{eq:witness_function}
%f_{\nu,\mu}(z) = \int k(.,z)\diff \mu - \int k(.,z)\diff \nu  \qquad z\in \X
%\end{align}
%This allows to provide a closed form expression for the $MMD$ in terms of expectations of the kernel under $\mu$ and $\nu$, or as the RKHS norm of \cref{eq:witness_function}:
%\begin{align}\label{eq:loss_functional}
%\F(\nu) = \frac{1}{2} MMD^2(\mu,\nu) \qquad \forall \nu \in \mathcal{P}_2(\X).
%\end{align}
\paragraph{Gradient flow of the MMD.}\label{paragraph:flow_MMD}
We consider now the problem of transporting mass from an initial distribution $\nu_0$ to a target distribution $\mu$ by finding a continuous path $\nu_t$ starting from $\nu_0$ that would ideally converge to $\mu$ while decreasing $\F(\nu_t)$. Such path should be physically plausible in that  teleportation phenomena are not allowed. For instance, the path $\nu_t = (1-e^{-t})\mu + e^{-t}\nu_0$ would constantly teleport mass between $\mu$ and $\nu_0$ although it decreases  $\F$ since $\F(\nu_t)=e^{-2t}\F(\nu_0)$. The physicality of the path is better understood in terms of classical statistical physics: if $\nu_0$ is an initial configuration of $N$ particles, these can be moved towards a different configuration $\mu$ through successive small transformations without jumping form a location to another. There are practical situations in machine learning where this constraint will arise naturally as we will see in \cref{subsec:training_neural_networks}. 

The optimal transport theory provides a way to construct such a continuous path by means of the \textit{continuity equation}. Given a vector field $V_t$ on $\X$ and an initial condition $\nu_0$, the continuity equation is a partial differential equation which defines a path $\nu_t$ evolving under the action of the vector field $V_t$, and reads $\partial_t \nu_t = -div(\nu_t V_t)$ for all $t\geq0$.
This equation expresses two facts, the first one is that $-div(\nu_t V_t)$ reflects the infinitesimal changes in $\nu_t$ as dictated by $V_t$, the second fact is that the total mass of $\nu_t$ doesn't vary in time as a consequence of the divergence theorem. The reader can find more detailed discussions in \cite{Santambrogio:2015}.  
The choice of $V_t$ determines the path $\nu_t$, therefore there must be a direct link between $V_t$ and $\F(\nu_t)$ if one wants to construct a gradient flow for of $\F$. Following  \cite{ambrosio2008gradient}, such link is obtained by choosing $V_t$ to be the negative gradient of the differential of $\F(\nu_t)$ at $\nu_t$. By \cref{prop:differential_mmd}, we know that the differential of $\F(\nu_t)$  at $\nu_t$ is given by $f_{\mu,\nu_t}$, hence $V_t(x) = -\nabla f_{\mu,\nu_t}(x)$. The
gradient flow of $\F$ is then defined by the solution $(\nu_t)_{t\geq 0}$ of:
\begin{align}\label{eq:continuity_mmd}
	\partial_t \nu_t = div(\nu_t \nabla f_{\mu,\nu_t}).
\end{align}
\cref{eq:continuity_mmd} is peculiar in that the vector field depends itself on $\nu_t$. This type of equations is associated in probability theory literature to the so called McKean-Vlasov process \cite{kac1956foundations,mckean1966class}:
\begin{align}\label{eq:mcKean_Vlasov_process}
	\dot{X}_t = -\nabla f_{\mu,\nu_t}(X_t) \qquad X_0\sim \nu_0.
\end{align}
In fact,  \cref{eq:mcKean_Vlasov_process} defines a process $(X_t)_{t\geq 0}$ whose distribution $(\nu_t)_{t\geq 0}$ satisfies \cref{eq:continuity_mmd} as shown in \cref{thm:existence_uniqueness}. 
$(X_t)_{t\geq 0}$ can be interpreted as  the trajectory of a single particle starting from an initial random position $X_0$ drawn from $\nu_0$. Its trajectory is then driven by a velocity field $-\nabla f_{\mu,\nu_t}$. However, such particle interacts with other particles driven by the same velocity field, which affects its trajectory. This interaction is captured by the velocity field through the dependence on the current configuration of all particles $\nu_t$.
Existence and uniqueness of a solution to \cref{eq:continuity_mmd,eq:mcKean_Vlasov_process} is guaranteed under mild conditions on the kernel $k$ and are provided in \cref{thm:existence_uniqueness}:
\begin{theorem}\label{thm:existence_uniqueness}(Existence and uniqueness)
	Under \manote{some assumptions}, and given $\nu_0\in \mathcal{P}_2(\X)$ there exists a unique process $(\X_t)_{t\geq 0}$ with $X_0\sim \nu_0$ and satisfying the McKean-Vlasov equation in \cref{eq:mcKean_Vlasov_process}. Moreover, the distribution $\nu_t$ of $X_t$ is the unique solution of \cref{eq:continuity_mmd} and defines a gradient flow of $\F$. 
\end{theorem}
A proof of \cref{thm:existence_uniqueness} is provided in \manote{proof} and relies on standard existence and uniqueness results of McKean-Vlasov processes under regularity of the map $(x,\nu)\mapsto \nabla f_{\mu,\nu}(x)$ \cite{Jourdain:2007}. Such regularity is ensured if the kernel $k$ has Lipschitz gradients for instance. Besides, the existence and uniqueness of the process itself, one would like the functional $\F$ to decrease along the path $\nu_t$ and ideally to converge towards $0$. While the latter is hard to obtain and will be discussed in \cref{sec:Lojasiewicz_inequality}, the first property is rather easy to get and is the object of \cref{prop:decay_mmd}:
\begin{proposition}\label{prop:decay_mmd}
	Under the assumptions of \cref{thm:existence_uniqueness}, $\F(\nu_t)$ is decreasing in time and satisfies:
	\begin{align}\label{eq:time_evolution_mmd}
		\dot{\F}(\nu_t)= - \int \Vert \nabla f_{\mu,\nu_t}(x) \Vert^2 \diff \nu_t(x)  
	\end{align}
\end{proposition}
From \cref{eq:time_evolution_mmd}, $\F$ can be seen as a Lyapounov functional for the dynamics defined by \cref{eq:continuity_mmd}. This property will be crucial in the analysis of \cref{sec:Lojasiewicz_inequality} and ensures that $\F(\nu_t)$  converges to some non-negative limit $l$ since it is a lower-bounded and non-increasing function of time. We conclude this section with a remark regarding to connexion to other gradient flows in Wasserstein space.


\begin{remark}
A different choice for $\F$  leads to a different continuity equation. A celebrated example is the case where $\F$ is equal to the KL divergence between some reference distribution $\mu$ and a current distribution $\nu$: $KL(\mu\Vert \nu_t ) =  \int \log (\mu(x))\diff \nu_t(x) -\int \log(\nu_t(x))\diff \nu_t(x)$. In \cite{jordan1998variational}, it was shown, under mild conditions on $\mu$ and $\nu_0$, that the gradient flow associated to the $KL$ leads to the so-called Fokker-Planck equation: $ \partial_t \nu_t = - div(\nu_t \nabla \log(\mu(x))) + \Delta \nu_t $ with Langevin diffusion as its corresponding process: $\diff X_t = \nabla \log(\mu(x))\diff t + \sqrt{2} \diff W_t $ with $X_0\sim \nu_0$.
 
 While the entropy term in the $KL$ prevents the particle from "crashing" onto the mode of $\mu$, this role could be played by the interaction energy $W$ for $MMD^2$ defined in \cref{eq:potentials}. Indeed, when $W$ is convex, this gives raise to a general aggregation behavior of the particles, while when it is not, the particles would push each other apart.  \aknote{to check, ref malrieu?}

The solution to the Fokker-Planck equation describing the gradient flow of the $KL$ can be shown to towards $\mu$ under mild assumptions. This follows from the displacement convexity of the $KL$ along the Wasserstein geodesics \manote{ref}. Unfortunately the $MMD$ is not displacement convex in general as we will see in \manote{ref}. This makes the task of proving the convergence of the gradient flow of the $MMD$ to the global optimum $\mu$ much harder. In fact, when the external potential $V$ in \cref{eq:potentials} is not convex, which is the case for instance when $k$ is a gaussian kernel, it was shown that the diffusion may admit several local minima (see \cite{herrmann2010non,tugaut2014phase}) \manote{make sure that these conditions are relevant for us}. Moreover, we show in \cref{sec:Lojasiewicz_inequality} that local minima which are not global exist and that it is rather easy to reach them.
\end{remark}

\subsection{Training neural networks as flow of the MMD}\label{subsec:training_neural_networks}
In this sub-section we establish a formal connection between the MMD gradient flow defined in \cref{eq:continuity_mmd} and neural networks optimization in the limit of infinitely many neurons.
The MMD was successfully used to train generative networks \cite{Arbel:2018,Binkowski:2018}\manote{cite others}, where it is used as a loss functional between the data distribution and a parametric model distribution.
Here we consider a different problem which arises naturally from optimizing neural networks using gradient descent. We show that this problem can be lifted to a non-parametric problem with the MMD as a cost function in a similar way as in \cite{Rotskoff:2019}. To remain consistent with the rest of the paper, the parameters of a network will be denoted by $x\in \X$ while the input and outputs will be denoted as $z$ and $y$.
 Given a neural network or any parametric function $(z,x)\mapsto \psi(z,x)$ with parameter $x \in \X $ and input data $z$ we consider the supervised learning problem:
\begin{align}\label{eq:regression_network}
	\min_{(x_1,...,x_m )\in \X} \frac{1}{2}\mathbb{E}_{(y,z)\sim \mathbb{P}  } \left[ \Vert y - \frac{1}{m}\sum_{i=1}^m\psi(z,x_i) \Vert^2 \right ]
\end{align}
where $(y,z)$ are samples from the data distribution and the regression function is an average network of $m$ different networks. The formulation in \cref{eq:regression_network} includes any type of networks. Indeed, the averaged function can itself be seen as one network with augmented parameters $(x_1,...,x_m)$ and any network can be written as an average of sub-networks with potentially shared weights. In the limit $m\rightarrow \infty$, the average can be seen as an expectation over the parameters under some probability distribution $\nu$. This leads to an expected network $\Psi(z,\nu) =  \int \psi(z,x) \diff \nu(x) $ and the optimization problem in \cref{eq:regression_network} can be lifted to an optimization problem in $\mathcal{P}_2(\X)$ the space of probability distributions:
\begin{align}\label{eq:lifted_regression}
	\min_{\nu \in \mathcal{P}_2(\X)}  \mathcal{L}(\nu) :=  \mathbb{E}_{(y,u)\sim \mathbb{P} } \left [ \Vert y - \int \psi(z,x) \diff \nu(x) \Vert^2 \right ]
\end{align} 
In the case where the model is well specified, there exists $\mu \in \mathcal{P}_2(\X) $ such that $\mathbb{E}_{y\sim \mathbb{P}(.|z)}[y] =  \int \psi(z,x) \diff \mu(x)$ and the cost function $\mathcal{L}$ defined in \cref{eq:lifted_regression} matches exactly the functional $\F$ defined in \cref{eq:mmd_as_free_energy} 
\begin{align}\label{eq:mmd_minimization}
	\min_{\nu \in \mathcal{P}_2(\X)} \int k(x,x') (\diff\mu(x) -\diff\nu(x))(\diff\mu(x') -\diff\nu(x'))  := MMD_k^2(\mu,\nu)
\end{align} 
where the kernel $k$ is constructed from the data as an expected product of networks:
\begin{align}
	k(x,x') = \mathbb{E}_{z\sim \mathbb{P}} [\psi(z,x)^T\psi(z,x')]
\end{align}
Moreover, $\bar{\mathcal{L}} = \F$ iif $\bar{\mathcal{L}}(\mu)=0$, which means that the model is well-specified. 
\end{proposition}
\cref{prop:inequality_mmd_loss} is shown in \manote{proof} and is a consequence of the optimality of $\mu$ after rearranging the terms obtained by expanding $\bar{\mathcal{L}}(\nu)$.
 \cref{eq:inequality_mmd_nn} implies that optimizing $\mathcal{F}$ can decrease  $\bar{\mathcal{L}}$ and vice-versa. However, the two functionals do not generally share the same local minima although they share the same global optima in general. One interesting class of problems where \cref{eq:lifted_regression} corresponds exactly to minimizing the $MMD$ is the student-teacher problem or the problem of distilling a pre-trained network into another network with the same architecture \manote{some references and say why this is useful}.  
%\begin{align}\label{eq:mmd_minimization}
%	\min_{\nu \in \mathcal{P}_2(\X)} \int k(x,x') (\diff\mu(x) -\diff\nu(x))(\diff\mu(x') -\diff\nu(x'))  := MMD_k^2(\mu,\nu)
%\end{align} 
% $\mu$\aknote{to precise}. Solving \cref{eq:regression_network} can be done using gradient descent which corresponds to a continuous-time dynamics for the particles $x_i$:
%\begin{align}\label{eq:particle_dynamics}
%	\dot{x}_i(t)= -\nabla \mathcal{L}(x_1(t),...,x_m(t)) \text{ for } i=1, \dots, m
%\end{align}  
%\aknote{The following paragraph may anticipate too much at this point} As pointed out in \cite{chizat2018global,Rotskoff:2019}, the dynamics in \cref{eq:particle_dynamics} can be analyzed in the "mean-field" limit when $m\rightarrow \infty$. For \cref{eq:mmd_minimization}, this leads to a continuity equation:
%\begin{align}\label{eq:mmd_flow}
%	\partial_t \nu_t = div(\nu_t \nabla f_{\mu,\nu_t})
%\end{align}
%for an initial choice of distribution $\nu_0$. In \cref{eq:mmd_flow}, $f_{\mu,\nu_t}$ is the witness function between $\mu$ and $\nu_t$:% defined and is given by:
%\[
%f_{\mu,\nu_t}(\theta) =  \int k(\theta,\theta')(\diff \nu_t(\theta')-\diff\mu(\theta') ) 
%\]
%\cref{eq:mmd_flow} can be shown to have a unique solution and has a particle version which is described by:
%\begin{align}\label{eq:particle_equation}
%	\dot{X}_t = -\nabla f_{\mu,\nu_t}(X_t) \qquad X_0\sim \nu_0
%\end{align}
%In \manote{section} we describe the properties of this flow and explain why in general it might not reach the global optimum.


%\input{sections/background_OT}

%
%In this section we consider the setting of learning a neural network to solve a regression problem in a similar context as in \cite{Rotskoff:2019} and show how this problem is related minimizing an MMD between an unknown target distribution and the current one. The neural network can be written as 
%
%
%
%From now on we consider $\mathcal{P}_2(\X)$ the set of probability distributions with finite second moments defined over a domain $\X\subset \R^d$.
%The Maximum Mean Discrepancy (MMD) between two distributions $\mu$ and $\nu$ in $\mathcal{P}_2(\X)$ is defined by 
% \begin{align}\label{eq:closed_form_MMD}
%MMD^2(\mu,\nu) = \int k(x,x')(\diff\mu(x) -\diff\nu(x))(\diff\mu(x') -\diff\nu(x'))
%\end{align}
%where $k$ is a positive semi-definite kernel \cite{gretton2012kernel}.  We are interested in minimizing the MMD between an unknown target distribution $\mu$ and a current proposal distribution $\nu$:
%\begin{align}
%	\nu^{*} \in \arg\min_{\nu\in \mathcal{P}_2(\X)} MMD(\mu,\nu)
%\end{align}
%When $k$ is characteristic, the MMD defines a distance over $\mathcal{P}_2(\X)$ hence the only optimal solution would be $\nu^*=\mu$.
%
%
%Given a fixed target distribution $\mu$ known only indirectly through observations and an initial distribution  $\nu_0$, we consider the problem of constructing flow of distributions $(\nu_t)_{t \geq 0}$ that would decrease $MMD(\mu,\nu_t)$ in time and eventually converge towards $\mu$. There are many ways one could construct such sequences and a trivial way would be to consider mixtures between samples from $\mu$ and samples $\nu_0$ with proportions gradually in favor of $\mu$:
%\[
%\nu_t = (1-e^{-t})\mu +e^{-t}\nu_0
%\]
%However, such choice would be effectively infeasible in many cases, especially when samples from $\mu$ are  indirectly observed as in the case of regression with neural networks \cite{Rotskoff:2019}.  
%
%
%
%\begin{align}
%	\nu^* = 
%\end{align} 
%
%
%To introduce the problem we consider the consider a setting similar to \cite{Rotskoff:2019}
%
%
%
%
%For a given kernel $k$ defined on a domain $\X\subset \R^d$, we denote by $\kH$ its corresponding Reproducing Kernel Hilbert Space \manote{some reference here needed}. The Maximum Mean Discrepancy between two distributions $\mu$
%Under mild conditions \manote{write conditions} on the kernel $k$, it is possible to define a distance on $\mathcal{P}_2(\X)$ by finding a function $f$ in $\mathcal{B}$ that maximizes the mean difference between two given distributions $\mu$ and $\nu$.  
%
%
% $\kH$ is a Hilbert space with inner product $\langle .,. \rangle_{\kH}$ and corresponding norm $\Vert . \Vert_{\kH}$. The unit ball in $\kH$ which will be denoted as $\mathcal{B}$ is simply the set of functions $f$ in $\kH$ such that $\Vert f\Vert_{\kH}\leq 1 $:
%\begin{align}\label{eq:unit_ball_RKHS}
%\mathcal{B} = \{ f\in \kH : \quad \Vert f\Vert_{\kH}\leq 1 \}
%\end{align}
% Such distance is called the Maximum Mean Discrepancy  (MMD) \cite{Gretton:2012}:
%\begin{align}\label{eq:MMD}
%MMD(\mu,\nu) = \sup_{g\in \mathcal{B}} \int g\diff\mu - \int g \diff\nu
%\end{align}
%The maximization problem in \cref{eq:MMD} is achieved for an optimal $g^*$ in $\mathcal{B}$ that is proportional to the  witness function between $\nu$ and $\mu$:
%\begin{align}\label{eq:witness_function}
%f_{\nu,\mu}(z) = \int k(.,z)\diff \mu - \int k(.,z)\diff \nu  \qquad z\in \X
%\end{align}
%This allows to express the $MMD$ as the norm of \cref{eq:witness_function} $f_{\nu,\mu}$:
%\begin{align}\label{eq:mmd_norm_witness}
%MMD(\mu,\nu) = \Vert f_{\nu,\mu} \Vert_{\mathcal{H}} 
%\end{align}
%Furthermore, a closed form expression in terms of expectations of the kernel under $\mu$ and $\nu$ can be obtained \cite{gretton2012kernel}:
%\begin{align}\label{eq:closed_form_MMD}
%MMD^2(\mu,\nu) = \int k\diff\mu \diff\mu + \int k\diff\nu \diff \nu - 2\int k\diff\mu \diff \nu
%\end{align}
%When samples from both $\mu$ and $\nu$ are available \cref{eq:closed_form_MMD} can be estimated using those samples. For a fixed target distributions $\mu$ we will consider the loss functional defined as:
%\begin{align}\label{eq:loss_functional}
%\F(\nu) = \frac{1}{2} MMD^2(\mu,\nu) \qquad \forall \nu \in \mathcal{P}_2(\X).
%\end{align}
%The next section describes the dynamics of the gradient flow of \cref{eq:loss_functional} under the $2$-Wasserstein metric as defined in \cref{subsec:wasserstein_flow}.
%%The MMD was successfully used for training generative models (\cite{mmd-gan,Binkowski:2018,Arbel:2018}) where it is used in a loss functional to learn the parameters of the generator network. This motivate the  
%
%
%
%
%
%In this section we recall how to endow the space of probability measures $\mathcal{P}(\X)$ on $\X$ a convex  subset of $\R^d$ with a distance (e.g, optimal transport distances), and then deal with gradient flows of suitable functionals on such a metric space. The reader may refer to the recent review of \cite{santambrogio2017euclidean} for further details. For a given distributions $\nu\in\mathcal{P}(\X)$ and an integrable function $f$ under $\nu$, the expectation of $f$ under $\nu$ will be written either as $\nu(f)$ or $\int f \diff\nu$ depending on the context. 
%