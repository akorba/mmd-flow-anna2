
\section{Gradient flow of the MMD in $W_2$}\label{sec:gradient_flow}
\subsection{Construction of the gradient flow}
In this section we introduce the gradient flow of the Maximum Mean Discrepancy (MMD) and highlight some of its properties. We start by briefly reviewing the MMD introduced in \cite{gretton2012kernel}, more details are provided in \cref{sec:rkhs}. In all the paper, $\X\subset\R^d$ is the closure of a convex open set.
\paragraph{Maximum Mean Discrepancy.}\label{subsec:MMD}
Given a characteristic kernel $k$ defined on $\X$, we denote by $\kH$ its corresponding RKHS (see \cite{smola1998learning}). The space $\kH$ is a Hilbert space with inner product $\langle .,. \rangle_{\kH}$ and corresponding norm $\Vert . \Vert_{\kH}$.
When the kernel has at most a quadratic growth, it is possible to define a distance on $\mathcal{P}_2(\X)$ called the Maximum Mean Discrepancy. It is given as the RKHS norm of the \textit{witness function} $f_{\mu,\nu}$ between $\mu$ and $\nu$ which is by definition the difference between the mean embeddings of $\nu$ and $\mu$: 
\begin{align}\label{eq:witness_function}
MMD(\mu,\nu) = \Vert f_{\mu,\nu} \Vert_{\kH}, \qquad f_{\nu,\mu}(z) = \int k(x,z)\diff \nu(x) - \int k(x,z)\diff \mu(x)  \qquad z\in \X
\end{align}
Throughout all the paper, $\mu$ will be fixed and $\nu$ could vary, hence, we will only consider the dependence in $\nu$ and denote by $\F(\nu)= \frac{1}{2}MMD^2(\mu,\nu)$. 
A direct computation shows that for any finite signed measure $\chi$ the following holds:
\begin{align}\label{prop:differential_mmd}
		\lim_{\epsilon\rightarrow 0} \epsilon^{-1}(\F(\nu +\epsilon\chi) - \F(\nu)) = \int f_{\mu,\nu}(x)d\chi(x).
	\end{align}
which implies that the differential of $\F(\nu)$ is given by $f_{\mu,\nu}$. This property will be key to construct a gradient flow of the MMD in $W_2$ as we will see in \cref{paragraph:flow_MMD}.
 Interestingly, $\F(\nu)$ can be written by means of a confinement potential $V$, an interaction potential $W$ and a constant off-set $C$:
\begin{align}\label{eq:potentials}
V(x)=-\int  k(x,x')\mu(x'), \qquad
W(x,x')=k(x,x'), \qquad
C = \frac{1}{2} \int k(x,x')\diff\mu(x) \diff\mu(x') 
\end{align}
This leads to a \textit{free-energy} expression for $\mathcal{F}(\nu)$:
\begin{align}\label{eq:mmd_as_free_energy}
	\F(\nu) = \int V(x) \diff \nu(x) +\frac{1}{2} \int W(x,y)\diff \nu(x)\diff \nu(y) + C.
\end{align}
If $X$ and $Y$ are two samples or particles following $\nu$, then $V$ reflects the potential generated by $\mu$ and acting on each particle $X$ and $Y$, while $W$ would reflect the potential arising from the interactions between the two particles $X$ and $Y$. This formulation will be useful to define a gradient flow of $\F(\nu)$. % in \cref{paragraph:flow_MMD}. 
%When samples from both $\mu$ and $\nu$ are available \cref{eq:closed_form_MMD} can be estimated using those samples.
%by finding a function $g$ in the unit ball $\mathcal{B} = \{ g\in \kH : \quad \Vert g\Vert_{\kH}\leq 1 \}$ that maximizes the mean difference between two given distributions $\mu$ and $\nu$. Such a distance is called the Maximum Mean Discrepancy  (MMD) \cite{Gretton:2012}:
%\begin{align}\label{eq:MMD}
%MMD(\mu,\nu) = \sup_{g\in \mathcal{B}} \int g\diff\mu - \int g \diff\nu
%\end{align}
%The optimal function  $g^*$ in $\mathcal{B}$ is then proportional to the  witness function between $\nu$
% and $\mu$ (see  \cite{gretton2012kernel}):
%\begin{align}\label{eq:witness_function}
%f_{\nu,\mu}(z) = \int k(.,z)\diff \mu - \int k(.,z)\diff \nu  \qquad z\in \X
%\end{align}
%This allows to provide a closed form expression for the $MMD$ in terms of expectations of the kernel under $\mu$ and $\nu$, or as the RKHS norm of \cref{eq:witness_function}:
%\begin{align}\label{eq:loss_functional}
%\F(\nu) = \frac{1}{2} MMD^2(\mu,\nu) \qquad \forall \nu \in \mathcal{P}_2(\X).
%\end{align}
\paragraph{Gradient flow of the MMD.}\label{paragraph:flow_MMD}
We consider now the problem of transporting mass from an initial distribution $\nu_0$ to a target distribution $\mu$ by finding a continuous path $\nu_t$ starting from $\nu_0$ that would ideally converge to $\mu$ while decreasing $\F(\nu_t)$. Such path should be physically plausible in that  teleportation phenomena are not allowed. For instance, the path $\nu_t = (1-e^{-t})\mu + e^{-t}\nu_0$ would constantly teleport mass between $\mu$ and $\nu_0$ although it decreases  $\F$ since $\F(\nu_t)=e^{-2t}\F(\nu_0)$ (see Corollary 1 in \cite{mroueh2018regularized}). The physicality of the path is better understood in terms of classical statistical physics: if $\nu_0$ is an initial configuration of $N$ particles, these can be moved towards a different configuration $\mu$ through successive small transformations without jumping from a location to another. There are practical situations in machine learning where this constraint will arise naturally as we will see in \cref{subsec:training_neural_networks}. 

The optimal transport theory provides a way to construct such a continuous path by means of the \textit{continuity equation}. Given a vector field $V_t$ on $\X$ and an initial condition $\nu_0$, the continuity equation is a partial differential equation which defines a path $\nu_t$ evolving under the action of the vector field $V_t$, and reads $\partial_t \nu_t = -div(\nu_t V_t)$ for all $t\geq0$.
This equation expresses two facts, the first one is that $-div(\nu_t V_t)$ reflects the infinitesimal changes in $\nu_t$ as dictated by the vector field (also referred to as velocity field) $V_t$, the second fact is that the total mass of $\nu_t$ does not vary in time as a consequence of the divergence theorem. The reader can find more detailed discussions in \cite{Santambrogio:2015}. Following  \cite{ambrosio2008gradient}, a natural choice is to choose $V_t$ as the negative gradient of the differential of $\F(\nu_t)$ at $\nu_t$, since it corresponds to a gradient flow of $\F$ associated with the 2-Wasserstein metric (see the definition \cref{subsec:gradient_flows_functionals}). %The choice of $V_t$ determines the path $\nu_t$, therefore there must be a direct link between $V_t$ and $\F(\nu_t)$ if one wants to construct a gradient flow for of $\F$. 
%Following  \cite{ambrosio2008gradient}, such link is obtained by choosing $V_t$ to the negative gradient of the differential of $\F(\nu_t)$ at $\nu_t$.
By \cref{prop:differential_mmd}, we know that the differential of $\F(\nu_t)$  at $\nu_t$ is given by $f_{\mu,\nu_t}$, hence $V_t(x) = -\nabla f_{\mu,\nu_t}(x)$. The
gradient flow of $\F$ is then defined by the solution $(\nu_t)_{t\geq 0}$ of:
\begin{align}\label{eq:continuity_mmd}
	\partial_t \nu_t = div(\nu_t \nabla f_{\mu,\nu_t}).
\end{align}
\cref{eq:continuity_mmd} is peculiar in that the vector field depends itself on $\nu_t$. \aknote{state existence of a solution and talk about uniqueness}  This type of equation is associated in the probability theory literature to the so-called McKean-Vlasov process \cite{kac1956foundations,mckean1966class}:
\begin{align}\label{eq:mcKean_Vlasov_process}
	d X_t = -\nabla f_{\mu,\nu_t}(X_t)dt \qquad X_0\sim \nu_0.
\end{align}

In fact,  \cref{eq:mcKean_Vlasov_process} defines a process $(X_t)_{t\geq 0}$ whose distribution $(\nu_t)_{t\geq 0}$ satisfies \cref{eq:continuity_mmd} as shown in \cref{prop:existence_uniqueness}. 
$(X_t)_{t\geq 0}$ can be interpreted as  the trajectory of a single particle starting from an initial random position $X_0$ drawn from $\nu_0$. Its trajectory is then driven by a velocity field $-\nabla f_{\mu,\nu_t}$. However, such particle interacts with other particles driven by the same velocity field, which affects its trajectory. This interaction is captured by the velocity field through the dependence on the current configuration of all particles $\nu_t$.
%Existence and uniqueness of a solution to \cref{eq:continuity_mmd,eq:mcKean_Vlasov_process} is guaranteed under mild conditions on the kernel $k$ and are provided in \cref{thm:existence_uniqueness}:
%\begin{theorem}\label{thm:existence_uniqueness}(Existence and uniqueness)
%	Under \manote{some assumptions}, and given $\nu_0\in \mathcal{P}_2(\X)$ there exists a unique process $(\X_t)_{t\geq 0}$ with $X_0\sim \nu_0$ and satisfying the McKean-Vlasov equation in \cref{eq:mcKean_Vlasov_process}. Moreover, the distribution $\nu_t$ of $X_t$ is the unique solution of \cref{eq:continuity_mmd} in a weak sense and hence defines a gradient flow of $\F$. 
%\end{theorem}
%A proof of \cref{thm:existence_uniqueness} is provided in \manote{proof} and relies on standard existence and uniqueness results of McKean-Vlasov processes under regularity of the map $(x,\nu)\mapsto \nabla f_{\mu,\nu}(x)$ \cite{Jourdain:2007}. Such regularity is ensured if the kernel $k$ has Lipschitz gradients for instance. Besides, the existence and uniqueness of the process itself, one would like the functional $\F$ to decrease along the path $\nu_t$ and ideally to converge towards $0$. While the latter is hard to obtain and will be discussed in \cref{sec:Lojasiewicz_inequality}, the first property is rather easy to get and is the object of \cref{prop:decay_mmd}:
Existence and uniqueness of a solution to \cref{eq:continuity_mmd,eq:mcKean_Vlasov_process} is guaranteed under mild conditions on the kernel $k$ using \cite[Proposition 2.5]{chizat2018global} (see \cref{proof:prop:existence_uniqueness} for a full proof):
\begin{proposition}\label{prop:existence_uniqueness}(Existence and uniqueness)
	Under Lipschitzness of $\nabla k$, and given $\nu_0\in \mathcal{P}_2(\X)$ there exists a unique process $(\X_t)_{t\geq 0}$ with $X_0\sim \nu_0$ and satisfying the McKean-Vlasov equation in \cref{eq:mcKean_Vlasov_process}. Moreover, the distribution $\nu_t$ of $X_t$ is the unique solution of \cref{eq:continuity_mmd} and defines a gradient flow of $\F$. 
\end{proposition}
Besides existence and uniqueness of the gradient flow of $\F$, one expects $\F$ to decrease along the path $\nu_t$ and ideally to converge towards $0$. The first property is rather easy to get and is the object of \cref{prop:decay_mmd} with a proof differed to \cref{proof:prop:decay_mmd}: 
\begin{proposition}\label{prop:decay_mmd}
	Under the assumptions of \cref{prop:existence_uniqueness}, $\F(\nu_t)$ is decreasing in time and satisfies:
	\begin{align}\label{eq:time_evolution_mmd}
		\frac{d\F(\nu_t)}{dt}= - \int \Vert \nabla f_{\mu,\nu_t}(x) \Vert^2 \diff \nu_t(x)  
	\end{align}
\end{proposition}
Convergence of the flow is harder to obtain and will be discussed in \cref{sec:Lojasiewicz_inequality}.
From \cref{eq:time_evolution_mmd}, $\F$ can be seen as a Lyapounov functional for the dynamics defined by \cref{eq:continuity_mmd}. This property will be crucial in the analysis of \cref{sec:Lojasiewicz_inequality} and ensures that $\F(\nu_t)$  converges to some non-negative limit $l$ since it is a lower-bounded and non-increasing function of time. The reader may refer to \cref{subsec:kl_flow} for a connection to other well-known gradient flows in Wassertein space. We know motivate the study of this flow by recent considerations and applications in Machine Learning.

%We conclude this section with a remark referring to other well-known gradient flows in Wasserstein space.
%\begin{remark}\label{remark:gradient_flow}
%A different choice for $\F$  leads to a different continuity equation. A celebrated example is the case where $\F$ is the KL divergence between the target distribution $\mu$ and a current distribution $\nu_t$: $KL(\nu_t\Vert \mu ) =  \int \log (\nu_t(x))\diff \nu_t(x) -\int \log(\mu(x))\diff \nu_t(x)$. In \cite{jordan1998variational}, it was shown, under mild conditions on $\mu$ and $\nu_0$, that the gradient flow associated to the $KL$ leads to the so-called Fokker-Planck equation: $ \partial_t \nu_t = - div(\nu_t \nabla \log(\mu(x))) + \Delta \nu_t $ with Langevin diffusion as its corresponding process: $d X_t = \nabla \log(\mu(x))\diff t + \sqrt{2} \diff W_t $ with $X_0\sim \nu_0$ and $W_t$ is a Brownian motion.
 
 %While the entropy term in the $KL$ functional prevents the particle from "crashing" onto the mode of $\mu$, this role could be played by the interaction energy $W$ for $MMD^2$ defined in \cref{eq:potentials}. Indeed, consider for instance the gaussian kernel $k(x,x')=e^{-\|x-x'\|^2}$. It is convex thus attractive at long distances ($\|x-x'\|>1$) but not at small distances (so repulsive).\aknote{not very precise yet but i think the interpretation could be interesting}

%The solution to the Fokker-Planck equation describing the gradient flow of the $KL$ can be shown to converge towards $\mu$ under mild assumptions. This follows from the displacement convexity of the $KL$ along the Wasserstein geodesics \manote{ref}. Unfortunately the $MMD^2$ is not displacement convex in general as we will see in \manote{ref}. This makes the task of proving the convergence of the gradient flow of the $MMD^2$ to the global optimum $\mu$ much harder. In fact, when the external potential $V$ in \cref{eq:potentials} is not convex, which is the case for instance when $k$ is a gaussian kernel, it was shown that the diffusion may admit several local minima (see \cite{herrmann2010non,tugaut2014phase}) \manote{make sure that these conditions are relevant for us}. Moreover, we show in \cref{sec:Lojasiewicz_inequality} that local minima which are not global exist and that it is rather easy to reach them.\aknote{hm wrong section?}
%\end{remark}
 






\subsection{Training neural networks as flow of the MMD}\label{subsec:training_neural_networks}
In this sub-section we establish a formal connection between the MMD gradient flow defined in \cref{eq:continuity_mmd} and neural networks optimization in the limit of infinitely many neurons.
The MMD was successfully used to train generative networks \cite{Arbel:2018,Binkowski:2018}\manote{cite others}, where it is used as a loss functional between the data distribution and a parametric model distribution.
Here we consider a different problem which arises naturally from optimizing neural networks using gradient descent. We show that this problem can be lifted to a non-parametric problem with the MMD as a cost function in a similar way as in \cite{Rotskoff:2019}. To remain consistent with the rest of the paper, the parameters of a network will be denoted by $x\in \X$ while the input and outputs will be denoted as $z$ and $y$.
 Given a neural network or any parametric function $(z,x)\mapsto \psi(z,x)$ with parameter $x \in \X $ and input data $z$ we consider the supervised learning problem:
\begin{align}\label{eq:regression_network}
	\min_{(x_1,...,x_m )\in \X} \frac{1}{2}\mathbb{E}_{(y,z)\sim \mathbb{P}  } \left[ \Big\Vert y - \frac{1}{m}\sum_{i=1}^m\psi(z,x_i) \Big\Vert^2 \right ]
\end{align}
where $(y,z)$ are samples from the data distribution and the regression function is an average network of $m$ different networks. The formulation in \cref{eq:regression_network} includes any type of networks. Indeed, the averaged function can itself be seen as one network with augmented parameters $(x_1,...,x_m)$ and any network can be written as an average of sub-networks with potentially shared weights. In the limit $m\rightarrow \infty$, the average can be seen as an expectation over the parameters under some probability distribution $\nu$. This leads to an expected network $\Psi(z,\nu) =  \int \psi(z,x) \diff \nu(x) $ and the optimization problem in \cref{eq:regression_network} can be lifted to an optimization problem in $\mathcal{P}_2(\X)$ the space of probability distributions:
\begin{align}\label{eq:lifted_regression}
	\min_{\nu \in \mathcal{P}_2(\X)}  \mathcal{L}(\nu) :=  \mathbb{E}_{(y,u)\sim \mathbb{P} } \left [ \big\Vert y - \int \psi(z,x) \diff \nu(x) \big\Vert^2 \right ]
\end{align} 
For convenience, we consider $\bar{\mathcal{L}}(\nu)$ the function obtained by subtracting the variance of $y$ from $\mathcal{L}(\nu)$, i.e.: $\bar{\mathcal{L}}(\nu) = \mathcal{L}(\nu) - var(y) $. When the model is well specified, there exists $\mu \in \mathcal{P}_2(\X) $ such that $\mathbb{E}_{y\sim \mathbb{P}(.|z)}[y] =  \int \psi(z,x) \diff \mu(x)$. In that case, the cost function $\bar{\mathcal{L}}$ matches  the functional $\F$ defined in \cref{eq:mmd_as_free_energy}  for a particular choice of the kernel $k$. More generally, as soon as a global minimizer for  \cref{eq:lifted_regression} exists,  \cref{prop:inequality_mmd_loss} relates the two losses $\bar{\mathcal{L}}$ and $\mathcal{F}$:
\begin{proposition}\label{prop:inequality_mmd_loss}
	Assuming a global minimizer of \cref{eq:lifted_regression} is achieved by some $\mu\in \mathcal{P}_2(\X)$, the following inequality holds for any $\nu \in \mathcal{P}_2(\X)$:
	\begin{align}\label{eq:inequality_mmd_nn}
		\left(\bar{\mathcal{L}}(\mu)^{\frac{1}{2}} + \F^{\frac{1}{2}}(\nu)\right)^2
		\geq 
		\bar{\mathcal{L}}(\nu)
		\geq
		\mathcal{F}(\nu) + \bar{\mathcal{L}}(\mu)
	\end{align}
	where $\F(\nu)$ is defined by \cref{eq:mmd_as_free_energy} with  a kernel $k$  constructed from the data as an expected product of networks:
\begin{align}
	k(x,x') = \mathbb{E}_{z\sim \mathbb{P}} [\psi(z,x)^T\psi(z,x')]
\end{align}
Moreover, $\bar{\mathcal{L}} = \F$ iif $\bar{\mathcal{L}}(\mu)=0$, which means that the model is well-specified. 
\end{proposition}
\cref{prop:inequality_mmd_loss} is shown in \cref{proof:prop:inequality_mmd_loss} and is a consequence of the optimality of $\mu$ after rearranging the terms obtained by expanding $\bar{\mathcal{L}}(\nu)$.
The framing \cref{eq:inequality_mmd_nn} implies that optimizing $\mathcal{F}$ can decrease  $\bar{\mathcal{L}}$ and vice-versa. However, the two functionals do not generally share the same local minima although they share the same global optima in general.\aknote{not clear to me..} One interesting class of problems where \cref{eq:lifted_regression} corresponds exactly to minimizing the $MMD$ is the student-teacher problem or the problem of distilling a pre-trained network into another network with the same architecture (see \cite{rotskoff2019global})\manote{some references}. In this case the gradient flow of the MMD defined in \cref{eq:continuity_mmd} corresponds to the population limit of the usual gradient flow of \cref{eq:regression_network} when the final layer becomes infinitely wide\aknote{same, globally we should rewrite this paragraph}. Indeed, solving \cref{eq:regression_network} is usually done using gradient descent. When the step-size approaches $0$, the parameters $(x_1,...,x_m)$ satisfy the continuous-time system of equations:
\begin{align}\label{eq:particle_dynamics}
	\dot{x}_i(t)= -\nabla \mathcal{L}(x_1(t),...,x_m(t)) \text{ for } i=1, \dots, m
\end{align}  
As pointed out in \cite{chizat2018global,Rotskoff:2019}, the dynamics in \cref{eq:particle_dynamics} can be analyzed in the "mean-field" limit when $m\rightarrow \infty$. For \cref{eq:particle_dynamics}, this leads to the continuity equation \cref{eq:continuity_mmd}. A formal statement that relates the finite population dynamics to the mean-field limit is provided in \manote{statement here}. 
%\begin{align}\label{eq:mmd_flow}
%	\partial_t \nu_t = div(\nu_t \nabla f_{\mu,\nu_t})
%\end{align}
%for an initial choice of distribution $\nu_0$. In \cref{eq:mmd_flow}, $f_{\mu,\nu_t}$ is the witness function between $\mu$ and $\nu_t$:% defined and is given by:
%\[
%f_{\mu,\nu_t}(\theta) =  \int k(\theta,\theta')(\diff \nu_t(\theta')-\diff\mu(\theta') ) 
%\]
%\cref{eq:mmd_flow} can be shown to have a unique solution and has a particle version which is described by:
%\begin{align}\label{eq:particle_equation}
%	\dot{X}_t = -\nabla f_{\mu,\nu_t}(X_t) \qquad X_0\sim \nu_0
%\end{align}
%In \manote{section} we describe the properties of this flow and explain why in general it might not reach the global optimum.


%\input{sections/background_OT}

%
%In this section we consider the setting of learning a neural network to solve a regression problem in a similar context as in \cite{Rotskoff:2019} and show how this problem is related minimizing an MMD between an unknown target distribution and the current one. The neural network can be written as 
%
%
%
%From now on we consider $\mathcal{P}_2(\X)$ the set of probability distributions with finite second moments defined over a domain $\X\subset \R^d$.
%The Maximum Mean Discrepancy (MMD) between two distributions $\mu$ and $\nu$ in $\mathcal{P}_2(\X)$ is defined by 
% \begin{align}\label{eq:closed_form_MMD}
%MMD^2(\mu,\nu) = \int k(x,x')(\diff\mu(x) -\diff\nu(x))(\diff\mu(x') -\diff\nu(x'))
%\end{align}
%where $k$ is a positive semi-definite kernel \cite{gretton2012kernel}.  We are interested in minimizing the MMD between an unknown target distribution $\mu$ and a current proposal distribution $\nu$:
%\begin{align}
%	\nu^{*} \in \arg\min_{\nu\in \mathcal{P}_2(\X)} MMD(\mu,\nu)
%\end{align}
%When $k$ is characteristic, the MMD defines a distance over $\mathcal{P}_2(\X)$ hence the only optimal solution would be $\nu^*=\mu$.
%
%
%Given a fixed target distribution $\mu$ known only indirectly through observations and an initial distribution  $\nu_0$, we consider the problem of constructing flow of distributions $(\nu_t)_{t \geq 0}$ that would decrease $MMD(\mu,\nu_t)$ in time and eventually converge towards $\mu$. There are many ways one could construct such sequences and a trivial way would be to consider mixtures between samples from $\mu$ and samples $\nu_0$ with proportions gradually in favor of $\mu$:
%\[
%\nu_t = (1-e^{-t})\mu +e^{-t}\nu_0
%\]
%However, such choice would be effectively infeasible in many cases, especially when samples from $\mu$ are  indirectly observed as in the case of regression with neural networks \cite{Rotskoff:2019}.  
%
%
%
%\begin{align}
%	\nu^* = 
%\end{align} 
%
%
%To introduce the problem we consider the consider a setting similar to \cite{Rotskoff:2019}
%
%
%
%
%For a given kernel $k$ defined on a domain $\X\subset \R^d$, we denote by $\kH$ its corresponding Reproducing Kernel Hilbert Space \manote{some reference here needed}. The Maximum Mean Discrepancy between two distributions $\mu$
%Under mild conditions \manote{write conditions} on the kernel $k$, it is possible to define a distance on $\mathcal{P}_2(\X)$ by finding a function $f$ in $\mathcal{B}$ that maximizes the mean difference between two given distributions $\mu$ and $\nu$.  
%
%
% $\kH$ is a Hilbert space with inner product $\langle .,. \rangle_{\kH}$ and corresponding norm $\Vert . \Vert_{\kH}$. The unit ball in $\kH$ which will be denoted as $\mathcal{B}$ is simply the set of functions $f$ in $\kH$ such that $\Vert f\Vert_{\kH}\leq 1 $:
%\begin{align}\label{eq:unit_ball_RKHS}
%\mathcal{B} = \{ f\in \kH : \quad \Vert f\Vert_{\kH}\leq 1 \}
%\end{align}
% Such distance is called the Maximum Mean Discrepancy  (MMD) \cite{Gretton:2012}:
%\begin{align}\label{eq:MMD}
%MMD(\mu,\nu) = \sup_{g\in \mathcal{B}} \int g\diff\mu - \int g \diff\nu
%\end{align}
%The maximization problem in \cref{eq:MMD} is achieved for an optimal $g^*$ in $\mathcal{B}$ that is proportional to the  witness function between $\nu$ and $\mu$:
%\begin{align}\label{eq:witness_function}
%f_{\nu,\mu}(z) = \int k(.,z)\diff \mu - \int k(.,z)\diff \nu  \qquad z\in \X
%\end{align}
%This allows to express the $MMD$ as the norm of \cref{eq:witness_function} $f_{\nu,\mu}$:
%\begin{align}\label{eq:mmd_norm_witness}
%MMD(\mu,\nu) = \Vert f_{\nu,\mu} \Vert_{\mathcal{H}} 
%\end{align}
%Furthermore, a closed form expression in terms of expectations of the kernel under $\mu$ and $\nu$ can be obtained \cite{gretton2012kernel}:
%\begin{align}\label{eq:closed_form_MMD}
%MMD^2(\mu,\nu) = \int k\diff\mu \diff\mu + \int k\diff\nu \diff \nu - 2\int k\diff\mu \diff \nu
%\end{align}
%When samples from both $\mu$ and $\nu$ are available \cref{eq:closed_form_MMD} can be estimated using those samples. For a fixed target distributions $\mu$ we will consider the loss functional defined as:
%\begin{align}\label{eq:loss_functional}
%\F(\nu) = \frac{1}{2} MMD^2(\mu,\nu) \qquad \forall \nu \in \mathcal{P}_2(\X).
%\end{align}
%The next section describes the dynamics of the gradient flow of \cref{eq:loss_functional} under the $2$-Wasserstein metric as defined in \cref{subsec:gradient_flows_functionals}.
%%The MMD was successfully used for training generative models (\cite{mmd-gan,Binkowski:2018,Arbel:2018}) where it is used in a loss functional to learn the parameters of the generator network. This motivate the  
%
%
%
%
%
%In this section we recall how to endow the space of probability measures $\mathcal{P}(\X)$ on $\X$ a convex  subset of $\R^d$ with a distance (e.g, optimal transport distances), and then deal with gradient flows of suitable functionals on such a metric space. The reader may refer to the recent review of \cite{santambrogio2017euclidean} for further details. For a given distributions $\nu\in\mathcal{P}(\X)$ and an integrable function $f$ under $\nu$, the expectation of $f$ under $\nu$ will be written either as $\nu(f)$ or $\int f \diff\nu$ depending on the context. 
%




%\section{Discretized Gradient flow of the MMD}
\subsection{Euler scheme}
The continuous-time gradient flow introduced in \cref{eq:continuity_mmd} allows to formally consider the notion of gradient descent on $\mathcal{P}_2(\X)$ with $\F$ as a cost function. This is done by considering a forward-Euler scheme of \cref{eq:continuity_mmd}. For any $T: \X \rightarrow \X$ a measurable map, and $\nu \in \mathcal{P}(\X)$, we denote the pushforward measure by $T_{\#}\nu$ (see the formal definition in \cref{subsec:wasserstein_flow}). Starting from an initial $\nu_0\in \mathcal{P}_2(\X)$ and using a step-size $\gamma>0$, it is possible to construct a sequence $\nu_n\in \mathcal{P}_2(\X)$ by iteratively applying:
\begin{align}\label{eq:euler_scheme}
	\nu_{n+1} = (I - \gamma \nabla f_{\mu,\nu_n})_{\#}\nu_n
\end{align} 
Equation \cref{eq:euler_scheme} is the distribution of the stochastic process defined by:
\begin{align}\label{eq:euler_scheme_particles}
	X_{n+1} = X_n - \gamma \nabla f_{\mu,\nu_n}(X_n) \qquad X_0\sim \nu_0.
\end{align}
The asymptotic behavior of \cref{eq:euler_scheme} as $n\rightarrow \infty$ can be analyzed much more easily than the continuous-time flow. \aknote{thought this was false? (since we have PL+the barrier in continuous time) but that the rates in discretized times are stronger} This will be the object of \cref{sec:convergence_mmd_flow}. For now, we provide a guarantee that the sequence $(\nu_n)_{n\geq 0}$ approaches $(\nu_t)_{t\geq 0}$ as $\gamma\rightarrow 0$:
\begin{proposition}\label{prop:convergence_euler_scheme}
	Consider the interpolation path $\rho_t^{\gamma}$ defined as:
	\begin{align}
		\rho_t^{\gamma} = (I-(t- n\gamma) \nabla f_{\mu,\nu_n})_{\#}\nu_n \qquad \forall t\in [n\gamma,(n+1)\gamma), \forall n\geq 0
	\end{align}
where $\nu_n$ is defined in \cref{eq:euler_scheme}. Then under \manote{asumptions: Lipschitz gradient + bounded hessian} and for all $T>0$:
	\begin{align}
		W_2(\rho_t^{\gamma},\nu_t)\leq \gamma C(T) \quad \forall t\in [0,T]
	\end{align}
\end{proposition} 
 A proof of \cref{prop:convergence_euler_scheme} is provided in \cref{proof:prop:convergence_euler_scheme} and relies on standard techniques to control the discretization error of a forward-Euler scheme.
\cref{prop:convergence_euler_scheme} means that $\nu_n$ can be interpolated giving rise to a path $\rho_t^{\gamma}$ which gets arbitrarily close to $\nu_t$ on finite intervals. Note that as $T \rightarrow \infty$ it is expected that the bound $C(T)$ would blow-up. However, this is enough to show that \cref{eq:euler_scheme} is indeed a discrete-time flow of $\F$. In fact, provided that $\gamma$ is small enough, $\F(\nu_k)$ is a decreasing sequence:
\begin{proposition}\label{prop:decreasing_functional}
	Under \cref{assump:bounded_trace,assump:bounded_hessian}, the following inequality holds:
	\begin{align*}
	\F(\nu_{n+1})-\F(\nu_n)\leq -\gamma (1-\frac{\gamma}{2}L )\int \Vert \phi_n(X)\Vert^2 d\nu_n
	\end{align*}
\end{proposition}
\cref{prop:decreasing_functional} is a discrete analog of \cref{prop:decay_mmd} and is proven in \cref{proof:prop:decreasing_functional}. In fact, \cref{eq:euler_scheme} is intractable in general as it requires to evaluate $f_{\mu,\nu_n}$ exactly at each iterations. Nevertheless, we present in \cref{sec:sample_based} a practical algorithm using a finite number of samples which is provably convergent towards \cref{eq:euler_scheme} as the sample-size increases. We thus begin by studying the convergence properties of the discretized in time MMD flow \eqref{eq:euler_scheme} in the next section. 