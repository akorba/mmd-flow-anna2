
\section{Preliminaries}\label{sec:preliminaries}


\subsection{Maximum Mean Discrepancy}\label{subsec:MMD}
We firsty introduce the functional of interest in this paper. The reader may refer to \cref{sec:rkhs} for additional details on Reproducing Kernels Hilbert Spaces (RKHS) and the Maximum Mean Discrepancy (MMD). For a given characteristic kernel $k$ defined on $\X$, we denote by $\kH$ its corresponding RKHS (see \cite{smola1998learning}). The space $\kH$ is a Hilbert space with inner product $\langle .,. \rangle_{\kH}$ and corresponding norm $\Vert . \Vert_{\kH}$. %The unit ball in $\kH$, which will be denoted as $\mathcal{B}$, is simply the set of functions $f$ in $\kH$ such that $\Vert f\Vert_{\kH}\leq 1 $:
%\begin{align}\label{eq:unit_ball_RKHS}
%\mathcal{B} = \{ f\in \kH : \quad \Vert f\Vert_{\kH}\leq 1 \}
%\end{align}
Under mild assumptions on the kernel, it is possible to define a distance on $\mathcal{P}_2(\X)$ by finding a function $f$ in the unit ball $\mathcal{B} = \{ f\in \kH : \quad \Vert f\Vert_{\kH}\leq 1 \}$ that maximizes the mean difference between two given distributions $\mu$ and $\nu$. Such a distance is called the Maximum Mean Discrepancy  (MMD) \cite{Gretton:2012}:
\begin{align}\label{eq:MMD}
MMD(\mu,\nu) = \sup_{g\in \mathcal{B}} \int g\diff\mu - \int g \diff\nu
\end{align}
The maximization problem in \cref{eq:MMD} is achieved for an optimal $g^*$ in $\mathcal{B}$ that is proportional to the  witness function between $\nu$ and $\mu$ (see  \cite{gretton2012kernel}):
\begin{align}\label{eq:witness_function}
f_{\nu,\mu}(z) = \int k(.,z)\diff \mu - \int k(.,z)\diff \nu  \qquad z\in \X
\end{align}
%\begin{align}\label{eq:mmd_norm_witness}
%MMD(\mu,\nu) = \Vert f_{\nu,\mu} \Vert_{\mathcal{H}} 
%\end{align}
This allows to express the $MMD$ in a closed form expression, in terms of expectations of the kernel under $\mu$ and $\nu$, or as the RKHS norm of \cref{eq:witness_function}:
\begin{align}\label{eq:closed_form_MMD}
MMD^2(\mu,\nu) = \int k\diff\mu \diff\mu + \int k\diff\nu \diff \nu - 2\int k\diff\mu \diff \nu=\Vert f_{\nu,\mu} \Vert^2_{\mathcal{H}} 
\end{align}
When samples from both $\mu$ and $\nu$ are available \cref{eq:closed_form_MMD} can be estimated using those samples. 
%\begin{align}\label{eq:loss_functional}
%\F(\nu) = \frac{1}{2} MMD^2(\mu,\nu) \qquad \forall \nu \in \mathcal{P}_2(\X).
%\end{align}

\subsection{Training neural networks as flow of the MMD}
The MMD was successfully used to train generative networks \cite{Arbel:2018,Binkowski:2018}\manote{cite others}, where it is used as a loss functional between the data distribution and a parametric model distribution.
Here we consider a different problem which arises naturally from optimizing neural networks using gradient descent. We show that this problem can be lifted to a non-parametric problem with the MMD as a cost function in a similar way as in \cite{Rotskoff:2019}. Given a neural network or any parametric function $(z,\theta)\mapsto \psi(z,\theta)$ with parameter $\theta \in \Theta \subset \R^d$, we consider the supervised learning problem:
\begin{align}\label{eq:regression_network}
	\min_{(\theta_1,...,\theta_m )\in \Theta} \mathcal{L}(\theta_1,...,\theta_m):= \mathbb{E}_{(y,z)\sim \mathbb{P}  }  \Vert y - \frac{1}{m}\sum_{i=1}^m\psi(z,\theta_i) \Vert^2
\end{align}
where $(y,z)$ are samples from the data distribution and the regression function is an average network of $m$ different networks. The formulation in \cref{eq:regression_network} includes any type of networks. Indeed, the averaged function can itself be seen as one network with augmented parameters $(\theta_1,...,\theta_m)$ and any network can be written as an average of sub-networks with potentially shared weights. In the limit $m\rightarrow \infty$, the average can be seen as an expectation over the parameters under some probability distribution $\nu$. This gives an expected network $\Psi(z,\nu) =  \int \psi(z,\theta) \diff \nu(\theta) $ and the optimization problem in \cref{eq:regression_network} can be lifted to an optimization problem in $\mathcal{P}(\Theta)$ the space of probability distributions:
\begin{align}\label{eq:lifted_regression}
	\min_{\nu \in \mathcal{P}(\Theta)} \mathbb{E}_{(y,u)\sim \mathbb{P} }  \Vert y - \int \psi(z,\theta) \diff \nu(\theta) \Vert^2
\end{align} 
In the case where the model is well specified, there exists $\mu \in \mathcal{P}(\Theta) $ such that 
\begin{equation}
\mathbb{E}_{y\sim \mathbb{P}(.|z)}[y] =  \int \psi(z,\theta) \diff \mu(\theta)
\end{equation}
and the cost function \cref{eq:lifted_regression} can be expressed as an MMD between $\mu$ and $\nu$:
\begin{align}\label{eq:mmd_minimization}
	\min_{\nu \in \mathcal{P}_2(\Theta)} \int k(\theta,\theta') (\diff\mu(\theta) -\diff\nu(\theta))(\diff\mu(\theta') -\diff\nu(\theta'))  := MMD_k^2(\mu,\nu)
\end{align} 
where the kernel $k$ is constructed from the data as an expected product of networks:
\begin{align}
	k(\theta,\theta') = \mathbb{E}_{z\sim \mathbb{P}} [\psi(z,\theta)^T\psi(z,\theta')]
\end{align}
When the problem is miss-specified, the same formulation can be obtained provided that \cref{eq:lifted_regression} achieves a minimum at $\mu$\aknote{to precise}. Solving \cref{eq:regression_network} can be done using gradient descent which corresponds to a continuous-time dynamics for the particles $\theta_i$:
\begin{align}\label{eq:particle_dynamics}
	\dot{\theta}_i(t)= -\nabla \mathcal{L}(\theta_1(t),...,\theta_m(t)) \text{ for } i=1, \dots, m
\end{align}  
\aknote{The following paragraph may anticipate too much at this point}As pointed out in \cite{chizat2018global,Rotskoff:2019}, the dynamics in \cref{eq:particle_dynamics} can be analyzed in the "mean-field" limit when $m\rightarrow \infty$. For \cref{eq:mmd_minimization}, this leads to a continuity equation:
\begin{align}\label{eq:mmd_flow}
	\partial_t \nu_t = div(\nu_t \nabla f_{\mu,\nu_t})
\end{align}
for an initial choice of distribution $\nu_0$. In \cref{eq:mmd_flow}, $f_{\mu,\nu_t}$ is the witness function between $\mu$ and $\nu_t$:% defined and is given by:
\[
f_{\mu,\nu_t}(\theta) =  \int k(\theta,\theta')(\diff \nu_t(\theta')-\diff\mu(\theta') ) 
\]
\cref{eq:mmd_flow} can be shown to have a unique solution and has a particle version which is described by:
\begin{align}\label{eq:particle_equation}
	\dot{X}_t = -\nabla f_{\mu,\nu_t}(X_t) \qquad X_0\sim \nu_0
\end{align}
In \manote{section} we describe the properties of this flow and explain why in general it might not reach the global optimum.


%\input{sections/background_OT}

%
%In this section we consider the setting of learning a neural network to solve a regression problem in a similar context as in \cite{Rotskoff:2019} and show how this problem is related minimizing an MMD between an unknown target distribution and the current one. The neural network can be written as 
%
%
%
%From now on we consider $\mathcal{P}_2(\X)$ the set of probability distributions with finite second moments defined over a domain $\X\subset \R^d$.
%The Maximum Mean Discrepancy (MMD) between two distributions $\mu$ and $\nu$ in $\mathcal{P}_2(\X)$ is defined by 
% \begin{align}\label{eq:closed_form_MMD}
%MMD^2(\mu,\nu) = \int k(x,x')(\diff\mu(x) -\diff\nu(x))(\diff\mu(x') -\diff\nu(x'))
%\end{align}
%where $k$ is a positive semi-definite kernel \cite{gretton2012kernel}.  We are interested in minimizing the MMD between an unknown target distribution $\mu$ and a current proposal distribution $\nu$:
%\begin{align}
%	\nu^{*} \in \arg\min_{\nu\in \mathcal{P}_2(\X)} MMD(\mu,\nu)
%\end{align}
%When $k$ is characteristic, the MMD defines a distance over $\mathcal{P}_2(\X)$ hence the only optimal solution would be $\nu^*=\mu$.
%
%
%Given a fixed target distribution $\mu$ known only indirectly through observations and an initial distribution  $\nu_0$, we consider the problem of constructing flow of distributions $(\nu_t)_{t \geq 0}$ that would decrease $MMD(\mu,\nu_t)$ in time and eventually converge towards $\mu$. There are many ways one could construct such sequences and a trivial way would be to consider mixtures between samples from $\mu$ and samples $\nu_0$ with proportions gradually in favor of $\mu$:
%\[
%\nu_t = (1-e^{-t})\mu +e^{-t}\nu_0
%\]
%However, such choice would be effectively infeasible in many cases, especially when samples from $\mu$ are  indirectly observed as in the case of regression with neural networks \cite{Rotskoff:2019}.  
%
%
%
%\begin{align}
%	\nu^* = 
%\end{align} 
%
%
%To introduce the problem we consider the consider a setting similar to \cite{Rotskoff:2019}
%
%
%
%
%For a given kernel $k$ defined on a domain $\X\subset \R^d$, we denote by $\kH$ its corresponding Reproducing Kernel Hilbert Space \manote{some reference here needed}. The Maximum Mean Discrepancy between two distributions $\mu$
%Under mild conditions \manote{write conditions} on the kernel $k$, it is possible to define a distance on $\mathcal{P}_2(\X)$ by finding a function $f$ in $\mathcal{B}$ that maximizes the mean difference between two given distributions $\mu$ and $\nu$.  
%
%
% $\kH$ is a Hilbert space with inner product $\langle .,. \rangle_{\kH}$ and corresponding norm $\Vert . \Vert_{\kH}$. The unit ball in $\kH$ which will be denoted as $\mathcal{B}$ is simply the set of functions $f$ in $\kH$ such that $\Vert f\Vert_{\kH}\leq 1 $:
%\begin{align}\label{eq:unit_ball_RKHS}
%\mathcal{B} = \{ f\in \kH : \quad \Vert f\Vert_{\kH}\leq 1 \}
%\end{align}
% Such distance is called the Maximum Mean Discrepancy  (MMD) \cite{Gretton:2012}:
%\begin{align}\label{eq:MMD}
%MMD(\mu,\nu) = \sup_{g\in \mathcal{B}} \int g\diff\mu - \int g \diff\nu
%\end{align}
%The maximization problem in \cref{eq:MMD} is achieved for an optimal $g^*$ in $\mathcal{B}$ that is proportional to the  witness function between $\nu$ and $\mu$:
%\begin{align}\label{eq:witness_function}
%f_{\nu,\mu}(z) = \int k(.,z)\diff \mu - \int k(.,z)\diff \nu  \qquad z\in \X
%\end{align}
%This allows to express the $MMD$ as the norm of \cref{eq:witness_function} $f_{\nu,\mu}$:
%\begin{align}\label{eq:mmd_norm_witness}
%MMD(\mu,\nu) = \Vert f_{\nu,\mu} \Vert_{\mathcal{H}} 
%\end{align}
%Furthermore, a closed form expression in terms of expectations of the kernel under $\mu$ and $\nu$ can be obtained \cite{gretton2012kernel}:
%\begin{align}\label{eq:closed_form_MMD}
%MMD^2(\mu,\nu) = \int k\diff\mu \diff\mu + \int k\diff\nu \diff \nu - 2\int k\diff\mu \diff \nu
%\end{align}
%When samples from both $\mu$ and $\nu$ are available \cref{eq:closed_form_MMD} can be estimated using those samples. For a fixed target distributions $\mu$ we will consider the loss functional defined as:
%\begin{align}\label{eq:loss_functional}
%\F(\nu) = \frac{1}{2} MMD^2(\mu,\nu) \qquad \forall \nu \in \mathcal{P}_2(\X).
%\end{align}
%The next section describes the dynamics of the gradient flow of \cref{eq:loss_functional} under the $2$-Wasserstein metric as defined in \cref{subsec:wasserstein_flow}.
%%The MMD was successfully used for training generative models (\cite{mmd-gan,Binkowski:2018,Arbel:2018}) where it is used in a loss functional to learn the parameters of the generator network. This motivate the  
%
%
%
%
%
%In this section we recall how to endow the space of probability measures $\mathcal{P}(\X)$ on $\X$ a convex  subset of $\R^d$ with a distance (e.g, optimal transport distances), and then deal with gradient flows of suitable functionals on such a metric space. The reader may refer to the recent review of \cite{santambrogio2017euclidean} for further details. For a given distributions $\nu\in\mathcal{P}(\X)$ and an integrable function $f$ under $\nu$, the expectation of $f$ under $\nu$ will be written either as $\nu(f)$ or $\int f \diff\nu$ depending on the context. 
%