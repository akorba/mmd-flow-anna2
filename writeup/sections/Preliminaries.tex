
\section{Preliminaries}\label{sec:preliminaries}


\subsection{Training neural networks as flow of the MMD}
The MMD was successfully used to train generative networks \cite{Arbel:2018,Binkowski:2018}\manote{cite others}, where it is used as a loss functional between the data distribution and a parametric model distribution.
Here we consider a different problem which arises naturally from optimizing neural networks using gradient descent. We show that this problem can be lifted to a non-parametric problem with the MMD as a cost function in a similar way as in \cite{Rotskoff:2019}. Given a neural network or any parametric function $(z,\theta)\mapsto \psi(z,\theta)$ with parameter $\theta \in \Theta \subset \R^d$, we consider the supervised learning problem:
\begin{align}\label{eq:regression_network}
	\min_{(\theta_1,...,\theta_m )\in \Theta} \mathcal{L}(\theta_1,...,\theta_m):= \mathbb{E}_{(y,z)\sim \mathbb{P}  }  \Vert y - \frac{1}{m}\sum_{i=1}^m\psi(z,\theta_i) \Vert^2
\end{align}
where $(y,z)$ are samples from the data distribution and the regression function is an average network of $m$ different networks. The formulation in \cref{eq:regression_network} includes any type of networks. Indeed, the averaged function can itself be seen as one network with augmented parameters $(\theta_1,...,\theta_m)$ and any network can be written as an average of sub-networks with potentially shared weights. In the limit $m\rightarrow \infty$, the average can be seen as an expectation over the parameters under some probability distribution $\nu$. This gives an expected network $\Psi(z,\nu) =  \int \psi(z,\theta) \diff \nu(\theta) $ and the optimization problem in \cref{eq:regression_network} can be lifted to an optimization problem in $\mathcal{P}(\Theta)$ the space of probability distributions:
\begin{align}\label{eq:lifted_regression}
	\min_{\nu \in \mathcal{P}(\Theta)} \mathbb{E}_{(y,u)\sim \mathbb{P} }  \Vert y - \int \psi(z,\theta) \diff \nu(\theta) \Vert^2
\end{align} 
In the case where the model is well specified, there exists $\mu \in \mathcal{P}(\Theta) $ such that 
\[\mathbb{E}_{y\sim \mathbb{P}(.|z)}[y] =  \int \psi(z,\theta) \diff \mu(\theta)
\] 
and the cost function \cref{eq:lifted_regression} can be expressed as an MMD between $\mu$ and $\nu$:
\begin{align}\label{eq:mmd_minimization}
	\min_{\nu \in \mathcal{P}_2(\Theta)} \int k(\theta,\theta') (\diff\mu(\theta) -\diff\nu(\theta))(\diff\mu(\theta') -\diff\nu(\theta'))  := MMD_k^2(\mu,\nu)
\end{align} 
where the kernel $k$ is constructed from the data as an expected product of networks:
\begin{align}
	k(\theta,\theta') = \mathbb{E}_{z\sim \mathbb{P}} [\psi(z,\theta)^T\psi(z,\theta')]
\end{align}
When the problem is miss-specified, the same formulation can be obtained provided that \cref{eq:lifted_regression} achieves a minimum at $\mu$. Solving \cref{eq:regression_network} can be done using gradient descent which leads to a continuous-time dynamics for the particles $\theta_i$:
\begin{align}\label{eq:particle_dynamics}
	\dot{\theta}_i(t)= -\nabla \mathcal{L}(\theta_1(t),...,\theta_m(t))
\end{align}  
As pointed out in \cite{chizat2018global,Rotskoff:2019}, the dynamics in \cref{eq:particle_dynamics} can be analyzed in the "mean-field" limit when $m\rightarrow \infty$. For \cref{eq:mmd_minimization}, this leads to a continuity equation:
\begin{align}\label{eq:mmd_flow}
	\partial_t \nu_t = div(\nu_t \nabla f_{\mu,\nu_t})
\end{align}
for an initial choice of distribution $\nu_0$. In \cref{eq:mmd_flow}, $f_{\mu,\nu_t}$ is the witness function between $\mu$ and $\nu_t$ defined and is given by:
\[
f_{\mu,\nu_t}(\theta) =  \int k(\theta,\theta')(\diff \nu(\theta')-\diff\mu(\theta') ) 
\]
\cref{eq:mmd_flow} can be shown to have a unique solution and has a particle version which is described by:
\begin{align}\label{eq:particle_equation}
	\dot{X}_t = -\nabla f_{\mu,\nu_t}(X_t) \qquad X_0\sim \nu_0
\end{align}
In \manote{section} we describe the properties of this flow and explain why in general it might not reach the global optimum.




%
%In this section we consider the setting of learning a neural network to solve a regression problem in a similar context as in \cite{Rotskoff:2019} and show how this problem is related minimizing an MMD between an unknown target distribution and the current one. The neural network can be written as 
%
%
%
%From now on we consider $\mathcal{P}_2(\X)$ the set of probability distributions with finite second moments defined over a domain $\X\subset \R^d$.
%The Maximum Mean Discrepancy (MMD) between two distributions $\mu$ and $\nu$ in $\mathcal{P}_2(\X)$ is defined by 
% \begin{align}\label{eq:closed_form_MMD}
%MMD^2(\mu,\nu) = \int k(x,x')(\diff\mu(x) -\diff\nu(x))(\diff\mu(x') -\diff\nu(x'))
%\end{align}
%where $k$ is a positive semi-definite kernel \cite{gretton2012kernel}.  We are interested in minimizing the MMD between an unknown target distribution $\mu$ and a current proposal distribution $\nu$:
%\begin{align}
%	\nu^{*} \in \arg\min_{\nu\in \mathcal{P}_2(\X)} MMD(\mu,\nu)
%\end{align}
%When $k$ is characteristic, the MMD defines a distance over $\mathcal{P}_2(\X)$ hence the only optimal solution would be $\nu^*=\mu$.
%
%
%Given a fixed target distribution $\mu$ known only indirectly through observations and an initial distribution  $\nu_0$, we consider the problem of constructing flow of distributions $(\nu_t)_{t \geq 0}$ that would decrease $MMD(\mu,\nu_t)$ in time and eventually converge towards $\mu$. There are many ways one could construct such sequences and a trivial way would be to consider mixtures between samples from $\mu$ and samples $\nu_0$ with proportions gradually in favor of $\mu$:
%\[
%\nu_t = (1-e^{-t})\mu +e^{-t}\nu_0
%\]
%However, such choice would be effectively infeasible in many cases, especially when samples from $\mu$ are  indirectly observed as in the case of regression with neural networks \cite{Rotskoff:2019}.  
%
%
%
%\begin{align}
%	\nu^* = 
%\end{align} 
%
%
%To introduce the problem we consider the consider a setting similar to \cite{Rotskoff:2019}
%
%
%
%
%For a given kernel $k$ defined on a domain $\X\subset \R^d$, we denote by $\kH$ its corresponding Reproducing Kernel Hilbert Space \manote{some reference here needed}. The Maximum Mean Discrepancy between two distributions $\mu$
%Under mild conditions \manote{write conditions} on the kernel $k$, it is possible to define a distance on $\mathcal{P}_2(\X)$ by finding a function $f$ in $\mathcal{B}$ that maximizes the mean difference between two given distributions $\mu$ and $\nu$.  
%
%
% $\kH$ is a Hilbert space with inner product $\langle .,. \rangle_{\kH}$ and corresponding norm $\Vert . \Vert_{\kH}$. The unit ball in $\kH$ which will be denoted as $\mathcal{B}$ is simply the set of functions $f$ in $\kH$ such that $\Vert f\Vert_{\kH}\leq 1 $:
%\begin{align}\label{eq:unit_ball_RKHS}
%\mathcal{B} = \{ f\in \kH : \quad \Vert f\Vert_{\kH}\leq 1 \}
%\end{align}
% Such distance is called the Maximum Mean Discrepancy  (MMD) \cite{Gretton:2012}:
%\begin{align}\label{eq:MMD}
%MMD(\mu,\nu) = \sup_{g\in \mathcal{B}} \int g\diff\mu - \int g \diff\nu
%\end{align}
%The maximization problem in \cref{eq:MMD} is achieved for an optimal $g^*$ in $\mathcal{B}$ that is proportional to the  witness function between $\nu$ and $\mu$:
%\begin{align}\label{eq:witness_function}
%f_{\nu,\mu}(z) = \int k(.,z)\diff \mu - \int k(.,z)\diff \nu  \qquad z\in \X
%\end{align}
%This allows to express the $MMD$ as the norm of \cref{eq:witness_function} $f_{\nu,\mu}$:
%\begin{align}\label{eq:mmd_norm_witness}
%MMD(\mu,\nu) = \Vert f_{\nu,\mu} \Vert_{\mathcal{H}} 
%\end{align}
%Furthermore, a closed form expression in terms of expectations of the kernel under $\mu$ and $\nu$ can be obtained \cite{gretton2012kernel}:
%\begin{align}\label{eq:closed_form_MMD}
%MMD^2(\mu,\nu) = \int k\diff\mu \diff\mu + \int k\diff\nu \diff \nu - 2\int k\diff\mu \diff \nu
%\end{align}
%When samples from both $\mu$ and $\nu$ are available \cref{eq:closed_form_MMD} can be estimated using those samples. For a fixed target distributions $\mu$ we will consider the loss functional defined as:
%\begin{align}\label{eq:loss_functional}
%\F(\nu) = \frac{1}{2} MMD^2(\mu,\nu) \qquad \forall \nu \in \mathcal{P}_2(\X).
%\end{align}
%The next section describes the dynamics of the gradient flow of \cref{eq:loss_functional} under the $2$-Wasserstein metric as defined in \cref{subsec:wasserstein_flow}.
%%The MMD was successfully used for training generative models (\cite{mmd-gan,Binkowski:2018,Arbel:2018}) where it is used in a loss functional to learn the parameters of the generator network. This motivate the  
%
%
%
%
%
%In this section we recall how to endow the space of probability measures $\mathcal{P}(\X)$ on $\X$ a convex  subset of $\R^d$ with a distance (e.g, optimal transport distances), and then deal with gradient flows of suitable functionals on such a metric space. The reader may refer to the recent review of \cite{santambrogio2017euclidean} for further details. For a given distributions $\nu\in\mathcal{P}(\X)$ and an integrable function $f$ under $\nu$, the expectation of $f$ under $\nu$ will be written either as $\nu(f)$ or $\int f \diff\nu$ depending on the context. 
%
\subsection{Background on optimal transport}

In this section we recall how to endow the space of probability measures $\mathcal{P}(\X)$ on $\X$ a compact, convex subset of $\R^d$ with a distance (e.g, optimal transport distances), and then deal with gradient flows of suitable functionals on such a metric space. The reader may refer to the recent review of \cite{santambrogio2017euclidean} for further details. For a given distributions $\nu\in\mathcal{P}(\X)$ and an integrable function $f$ under $\nu$, the expectation of $f$ under $\nu$ will be written either as $\nu(f)$ or $\int f \diff\nu$ depending on the context. 

\subsubsection{$2$-Wasserstein geometry}\label{subsec:wasserstein_flow}

Let $\mathcal{P}_2(\X)$ the set of probability distributions on $\X$ with finite second moment.
Let $T: \X \rightarrow \X$ be a measurable map, and $\rho \in \mathcal{P}(\X)$. The push-forward measure $T_{\#}\rho$
is characterized by:
\begin{align*}
	&\quad T_{\#}\rho(A) = \rho(T^{-1}(A)) \text{ for every measurable set A,}\\
\text{or}& \int_{y \in \X} \phi(y) d(T_{\#}\rho)(y) =\int_{x \in \X}\phi(T(x)) d\rho(x) \text{ for every measurable function $\phi$.}
\end{align*}
For two given probability distributions $\nu$ and $\mu$ in $\mathcal{P}_2(\X)$ we denote by $\Pi(\nu,\mu)$ the set of possible couplings between $\nu$ and $\mu$. In other words $\Pi(\nu,\mu)$ contains all possible distributions $\pi$ on $\X\times \X$ such that if $(X,Y) \sim \pi $ then $X \sim \nu $ and $Y\sim \mu$. The $2$-Wasserstein distance on $\mathcal{P}_2(\X)$ is defined by means of optimal coupling between $\nu$ and $\mu$ in the following way:
\begin{align}\label{eq:wasserstein_2}
W_2^2(\nu,\mu) := \inf_{\pi\in\Pi(\nu,\mu)} \int \Vert x - y\Vert^2 d\pi(x,y) \qquad \forall \nu, \mu\in \mathcal{P}_2(\X)
\end{align}
It is a well established fact that such optimal coupling $\pi^*$ exists. Moreover, it can be used to define a path $(\rho_t)_{t\in [0,1]}$ between $\nu$ and $\mu$ in $\mathcal{P}_2(\X)$. For a given time $t$ in $[0,1]$ and given a sample $(x,y)$ from $\pi^{*}$, it possible to construct a sample $z_t$ from $\rho_t$ by taking the convex combination of $x$ and $y$: $z_t = s_t(x,y)$ where $s_t$ is given by \cref{eq:convex_combination}
\begin{equation}\label{eq:convex_combination}
s_t(x,y) = (1-t)x+ty \qquad \forall x,y\in \X, \; \forall t\in [0,1].
\end{equation}
The function $s_t$ is well defined since $\X$ is a convex set. More formally, $\rho_t$ can be written as the projection or push-forward of the optimal coupling $\pi^{*}$ by $s_t$:    \aknote{weird. st not operator}
\begin{equation}\label{eq:displacement_geodesic}
\rho_t = (s_t)_{\#}\pi^{*}
\end{equation}
It is easy to see that \cref{eq:displacement_geodesic} satisfies the following boundary conditions:
\begin{align}\label{eq:boundary_conditions}
\rho_0 = \nu \qquad \rho_1 = \mu.
\end{align}
Paths of the form of \cref{eq:displacement_geodesic} are called \textit{displacement geodesics}. They can be seen as the shortest paths from $\nu$ to $\mu$ in terms of mass transport (\cite{Santambrogio:2015} Theorem 5.27). It can be shown that there exists a vector field $(t,x)\mapsto v_t(x)$ with values in $\R^d$ such that $\rho$ satisfies the continuity equation:
\begin{equation}\label{eq:continuity_equation}
\partial_t \rho_t + div(\rho_t v_t ) = 0 \qquad \forall t\in[0,1].
\end{equation}
Equation \cref{eq:continuity_equation} is well defined in distribution sense even when $\rho_t$ doesn't have a density. $v_t$ can be interpreted as a tangent vector to the curve $(\rho_t)_{t\in[0,1]}$ at time $t$ so that the length $l(\rho)$ of the curve $\rho$ would be given by:
\begin{equation}
l(\rho)^2 = \int_0^1 \Vert v_t \Vert^2_{L_2(\rho_t)} \diff t \quad \text{ where } \quad 
\Vert v_t \Vert^2_{L_2(\rho_t)} =  \int \Vert v_t(x) \Vert^2 \diff \rho_t(x)
\end{equation}
\aknote{add constant speed geodesics}
This perspective allows to provide a dynamical interpretation of the $W_2$ as the length  of the shortest path from $\nu$ to $\mu$ and is summarized by the celebrated Benamou-Brenier formula (\cite{Santambrogio:2015} 5.28 ):
\begin{align}\label{eq:benamou-brenier-formula}
W_2(\nu,\mu) = \inf_{(\rho,v)} l(\rho)
\end{align}
where the infimum is taken  over all couples  $\rho$ and $v$ satisfying  \cref{eq:continuity_equation}  with boundary conditions given by \cref{eq:boundary_conditions}.

\begin{remark}
	Such paths should not be confused with another kind of paths called \textit{mixture geodesics}. The mixture geodesic $(m_t)_{t\in[0,1]}$ from $\nu$ to $\mu$ is obtained by first choosing either $\nu$ or $\mu$ according to a Bernoulli distribution of parameter $t$ and then sampling from the chosen distribution:
	\begin{align}\label{eq:mixture_geodesic}
	m_t = (1-t)\nu + t\mu \qquad \forall t \in [0,1].
	\end{align}
	Paths of the form \cref{eq:mixture_geodesic} can be thought as the shortest paths between two distributions when distances on $\mathcal{P}_2(\X)$ are measured using the $MMD$ (\cite{Bottou:2017} Theorem 5.3). We refer to \cite{Bottou:2017} for an overview of the notion of shortest paths in probability spaces and for the differences between mixture geodesics and displacement geodesics.
	Although, we will be interested in the $MMD$ as a loss function, we will not consider the geodesics that are naturally associated to it and we will rather consider the displacement geodesics defined in \cref{eq:displacement_geodesic} for reason that will become clear in \cref{subsec:wasserstein_flow}.
\end{remark}


\subsubsection{Gradient flows on the space of probability measures}\label{sec:gradient_flows_functionals}


Let $\F : \mathcal{P}(\X) \rightarrow \R \cup \infty$, $\rho \mapsto \F(\rho)$ a functional. We call $\frac{\partial{\F}}{\partial{\rho}}$ if it exists, the unique (up to additive constants) function such that $\frac{d}{d\epsilon}\F(\rho+\epsilon  f)_{\epsilon=0}=\int\frac{\partial{\F}}{\partial{\rho}}(\rho) df$ for every perturbation $f$ such that, at least for $\epsilon \in [0, \epsilon_0]$, the measure $\rho +\epsilon f$ belongs to $\mathcal{P}(\X)$. The function $\frac{\partial{\F}}{\partial{\rho}}$ is called first variation of the functional $\F$ at $\rho$. Consider a 
\textit{Lyapunov functional} 
(or "free energy") $\F$ 
(see \cite{Villani:2004}), 
i.e. a functional of the form:
\begin{equation}\label{eq:lyapunov}
\F(\rho)=\int U(\rho(x)) \rho(x)dx + \int V(x)\rho(x)dx + \int W(x,y)\rho(x)\rho(y)dxdy
\end{equation}
where  $U$ is the internal energy, $V$ the potential (or confinement) energy and $W$ the
interaction energy. The formal gradient flow equation associated to this functional can be written:
\begin{equation}\label{eq:continuity_equation1}
\frac{\partial \rho}{\partial t}= div( \rho \nabla \frac{\partial \F}{\partial \rho})=div( \rho \nabla (U'(\rho) + V + W * \rho))
\end{equation}
where $\nabla \frac{\partial \F}{\partial \rho}$ is the strong subdifferential of $\F(\rho_t)$ associated with the 2-Wasserstein
metric (see \cite{ambrosio2008gradient}, Lemma 10.4.1). Indeed, for some generalized notion of gradient $\nabla_{W_2}$, and for sufficiently regular $\rho$ and $\F$, the r.h.s. of \eqref{eq:continuity_equation1} corresponds to $-\nabla_{W_2}\F(\rho)$.
The dissipation of entropy is defined as: %see  http://wwwf.imperial.ac.uk/~jcarrill/RICAM/CharlaRICAM2014-1.pdf
\begin{align}
       \frac{d \F(\rho)}{dt} =-D(\rho) \quad \text{ with } D(\rho)= \int |\xi|^2 \rho(x)dx
%&\text{ and } \xi= \nabla \frac{\partial \F}{\partial \rho} = \nabla (U'(\rho) + V + W * \rho)
\end{align}
Standard considerations from fluid mechanics tell us that the continuity equation \eqref{eq:continuity_equation1} may be interpreted as the equation ruling the evolution of the density $\rho_t$ of a family of particles initially distributed according to some $\rho_0$ and each of which follows the velocity/vector field $v_t=\nabla \frac{\partial{\F}}{\partial{\rho_t}}$.

\begin{remark} \label{rem:KL_Lyapunov}
	A famous example of a functional \eqref{eq:lyapunov} is the Kullback-Leibler divergence, defined for $\rho, \mu \in \mathcal{M}^+$ by
	$KL(\rho,\mu)=\int log(\frac{\rho(x)}{\mu(x)})\rho(x)dx$. Indeed, $KL(\rho, \mu)=\int U(\rho(x))dx + \int V(x) \rho(x)dx$ with $U(s)=s\log(s)$ the entropy function and $V(x)=-log(\mu(x))$. In this case, $\nabla \frac{\partial \F}{\partial \rho}= \nabla \log(\rho) + \nabla V=  \nabla \log(\frac{\rho}{\mu})$ and equation \eqref{eq:continuity_equation1} leads to the classical Fokker-Planck equation:
	\begin{equation}\label{eq:Fokker-Planck}
	\frac{\partial{\rho}}{\partial t}= div(\rho \nabla V )+ \Delta \rho
	\end{equation}
It is well-known (see for instance \cite{jordan1998variational}) that the distribution of the Langevin diffusion:
	\begin{equation}\label{eq:langevin_diffusion}
	dX_t= -\nabla \log \mu (X_t)dt+\sqrt{2}dB_t
	\end{equation}
	where $(B_t)_{t\ge0}$ is a $d$-dimensional Brownian motion, satisfies \eqref{eq:Fokker-Planck}.
\end{remark}



\subsection{Maximum Mean Discrepancy}\label{subsec:MMD}
We now turn to the functional of interest in this paper. The reader may refer to \cref{sec:rkhs} for additional details on Reproducing Kernels Hilbert Spaces (RKHS) and the Maximum Mean Discrepancy (MMD). For a given characteristic kernel $k$ defined on $\X$, we denote by $\kH$ its corresponding RKHS (see \cite{smola1998learning}). The space $\kH$ is a Hilbert space with inner product $\langle .,. \rangle_{\kH}$ and corresponding norm $\Vert . \Vert_{\kH}$. The unit ball in $\kH$ which will be denoted as $\mathcal{B}$ is simply the set of functions $f$ in $\kH$ such that $\Vert f\Vert_{\kH}\leq 1 $:
\begin{align}\label{eq:unit_ball_RKHS}
\mathcal{B} = \{ f\in \kH : \quad \Vert f\Vert_{\kH}\leq 1 \}
\end{align}
Under mild assumptions on the kernel, it is possible to define a distance on $\mathcal{P}_2(\X)$ by finding a function $f$ in $\mathcal{B}$ that maximizes the mean difference between two given distributions $\mu$ and $\nu$. Such a distance is called the Maximum Mean Discrepancy  (MMD) \cite{Gretton:2012}:
\begin{align}\label{eq:MMD}
MMD(\mu,\nu) = \sup_{g\in \mathcal{B}} \int g\diff\mu - \int g \diff\nu
\end{align}
The maximization problem in \cref{eq:MMD} is achieved for an optimal $g^*$ in $\mathcal{B}$ that is proportional to the  witness function between $\nu$ and $\mu$:
\begin{align}\label{eq:witness_function}
f_{\nu,\mu}(z) = \int k(.,z)\diff \mu - \int k(.,z)\diff \nu  \qquad z\in \X
\end{align}
This allows to express the $MMD$ as the norm of \cref{eq:witness_function} $f_{\nu,\mu}$:
\begin{align}\label{eq:mmd_norm_witness}
MMD(\mu,\nu) = \Vert f_{\nu,\mu} \Vert_{\mathcal{H}} 
\end{align}
Furthermore, a closed form expression in terms of expectations of the kernel under $\mu$ and $\nu$ can be obtained \cite{gretton2012kernel}:
\begin{align}\label{eq:closed_form_MMD}
MMD^2(\mu,\nu) = \int k\diff\mu \diff\mu + \int k\diff\nu \diff \nu - 2\int k\diff\mu \diff \nu
\end{align}
When samples from both $\mu$ and $\nu$ are available \cref{eq:closed_form_MMD} can be estimated using those samples. For a fixed target distributions $\mu$ we will consider the loss functional defined as:
\begin{align}\label{eq:loss_functional}
\F(\nu) = \frac{1}{2} MMD^2(\mu,\nu) \qquad \forall \nu \in \mathcal{P}_2(\X).
\end{align}
The next section describes the dynamics of the gradient flow of \cref{eq:loss_functional} under the $2$-Wasserstein metric as defined in \cref{subsec:wasserstein_flow}.
%The MMD was successfully used for training generative models (\cite{mmd-gan,Binkowski:2018,Arbel:2018}) where it is used in a loss functional to learn the parameters of the generator network. This motivate the  