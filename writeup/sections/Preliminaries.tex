
\section{Gradient flow of the MMD in $W_2$}\label{sec:gradient_flow}
\subsection{Construction of the gradient flow}
In this section we introduce the gradient flow of the Maximum Mean Discrepancy (MMD) and highlight some of its properties. We start by briefly reviewing the MMD introduced in \cite{gretton2012kernel}. %, more details are provided in \cref{sec:rkhs}. 
%The reader may also refer to \cref{subsec:kl_flow} for a connection to other well-known gradient flows in Wassertein space. 
We define $\X\subset\R^d$ as the closure of a convex open set, and $\mathcal{P}_2(\X)$ as the set of probability distributions on $\X$ with finite second moment, equipped with the 2-Wassertein metric denoted $W_2$. For any $\nu \in \mathcal{P}_2(\X)$, $L_2(\nu)$ is the set of square integrable functions w.r.t. $\nu$. The reader may find a relevant mathematical background \cref{sec:appendix_math_background}. 

%%AG: I write below ``unnormalised witness function'' since the witness function for the MMD has norm 1 (it's in the unit ball).

\paragraph{Maximum Mean Discrepancy.}\label{subsec:MMD} Given a characteristic kernel $k : \X \times \X \to \bR$, we denote by $\kH$ its corresponding RKHS (see \cite{smola1998learning}). The space $\kH$ is a Hilbert space with inner product $\langle .,. \rangle_{\kH}$ and norm $\Vert . \Vert_{\kH}$. We will rely on specific assumptions on the kernel which are given in \cref{sec:assumptions_kernel}. In particular, \cref{assump:lipschitz_gradient_k} states that the gradient of the kernel, $\nabla k$, is Lipschitz with constant $L$.
%\aknote{and fourth order derivatives bounded by $\lambda$?}
For such kernels, it is possible to define the Maximum Mean Discrepancy as a distance on $\mathcal{P}_2(\X)$. The MMD can be written as the RKHS norm of the unnormalised \textit{witness function} $f_{\mu,\nu}$ between $\mu$ and $\nu$, which is the difference between the mean embeddings of $\nu$ and $\mu$,
\begin{align}\label{eq:witness_function}
MMD(\mu,\nu) = \Vert f_{\mu,\nu} \Vert_{\kH}, \qquad f_{\nu,\mu}(z) = \int k(x,z)\diff \nu(x) - \int k(x,z)\diff \mu(x)  \quad \forall z\in \X
\end{align}
%%AG: I added the citation below to Mroueh Appendix B
Throughout the paper, $\mu$ will be fixed and $\nu$ can vary, hence we will only consider the dependence in $\nu$ and denote by $\F(\nu)= \frac{1}{2}MMD^2(\mu,\nu)$. 
A direct computation \cite[Appendix B]{Mroueh:2019} shows that for any %finite signed measure 
finite measure $\chi$ such that $\nu + \epsilon \chi \in \cP_2(\X)$, we have
\begin{align}\label{prop:differential_mmd}
		\lim_{\epsilon\rightarrow 0} \epsilon^{-1}(\F(\nu +\epsilon\chi) - \F(\nu)) = \int f_{\mu,\nu}(x)d\chi(x).
	\end{align}
This means that $f_{\mu,\nu}$ is the differential of $\F(\nu)$ .  %This property will be key to construct a gradient flow of the MMD  as we will see in \cref{paragraph:flow_MMD}.
 Interestingly, $\F(\nu)$ admits a \textit{free-energy} expression:
\begin{align}\label{eq:mmd_as_free_energy}
	\F(\nu) = \int V(x) \diff \nu(x) +\frac{1}{2} \int W(x,y)\diff \nu(x)\diff \nu(y) + C.
\end{align}
where $V$ is a confinement potential, $W$ an interaction potential and $C$ a constant defined by:
\begin{align}\label{eq:potentials}
V(x)=-\int  k(x,x')\diff\mu(x'), \quad
W(x,x')=k(x,x'), \quad
C = \frac{1}{2} \int k(x,x')\diff\mu(x) \diff\mu(x') 
\end{align}
Formulation \cref{eq:mmd_as_free_energy} and the simple expression of the differential in \cref{prop:differential_mmd} will be key to construct a gradient flow of $\F(\nu)$, to transport particles. In \cref{eq:potentials}, $V$ reflects the potential generated by $\mu$ and acting on each particle, while $W$ reflects the potential arising from the interactions between those particles.  % in \cref{paragraph:flow_MMD}. 
%When samples from both $\mu$ and $\nu$ are available \cref{eq:closed_form_MMD} can be estimated using those samples.
%by finding a function $g$ in the unit ball $\mathcal{B} = \{ g\in \kH : \quad \Vert g\Vert_{\kH}\leq 1 \}$ that maximizes the mean difference between two given distributions $\mu$ and $\nu$. Such a distance is called the Maximum Mean Discrepancy  (MMD) \cite{Gretton:2012}:
%\begin{align}\label{eq:MMD}
%MMD(\mu,\nu) = \sup_{g\in \mathcal{B}} \int g\diff\mu - \int g \diff\nu
%\end{align}
%The optimal function  $g^*$ in $\mathcal{B}$ is then proportional to the  witness function between $\nu$
% and $\mu$ (see  \cite{gretton2012kernel}):
%\begin{align}\label{eq:witness_function}
%f_{\nu,\mu}(z) = \int k(.,z)\diff \mu - \int k(.,z)\diff \nu  \qquad z\in \X
%\end{align}
%This allows to provide a closed form expression for the $MMD$ in terms of expectations of the kernel under $\mu$ and $\nu$, or as the RKHS norm of \cref{eq:witness_function}:
%\begin{align}\label{eq:loss_functional}
%\F(\nu) = \frac{1}{2} MMD^2(\mu,\nu) \qquad \forall \nu \in \mathcal{P}_2(\X).
%\end{align}
\paragraph{Gradient flow of the MMD.}\label{paragraph:flow_MMD}
We consider now the problem of transporting mass from an initial distribution $\nu_0$ to a target distribution $\mu$, by finding a continuous path $\nu_t$ starting from $\nu_0$ that  converges to $\mu$ while decreasing $\F(\nu_t)$. Such a path should be physically plausible, in that  teleportation phenomena are not allowed. For instance, the path $\nu_t = (1-e^{-t})\mu + e^{-t}\nu_0$ would constantly teleport mass between $\mu$ and $\nu_0$ although it decreases  $\F$ since $\F(\nu_t)=e^{-2t}\F(\nu_0)$
\cite[Section 3.1, Case 1]{Mroueh:2019}.
%AG: better to just consistently cite the AISTATS, not the earlier tech reports
%\cite[Corollary 1]{mroueh2018regularized}).
The physicality of the path is understood in terms of classical statistical physics: given an initial configuration $\nu_0$ of $N$ particles, these can move towards a new configuration $\mu$ through successive small transformations, without jumping from one location to another. %There are practical situations in machine learning where this constraint will arise naturally as we will see in \cref{subsec:training_neural_networks}. 

Optimal transport theory provides a way to construct such a continuous path by means of the \textit{continuity equation}. Given a vector field $V_t$ on $\X$ and an initial condition $\nu_0$, the continuity equation is a partial differential equation which defines a path $\nu_t$ evolving under the action of the vector field $V_t$, and reads $\partial_t \nu_t = -div(\nu_t V_t)$ for all $t \geq 0$.
%This equation expresses two facts, the first one is that $-div(\nu_t V_t)$ reflects the infinitesimal changes in $\nu_t$ as dictated by the vector field (also referred to as velocity field) $V_t$, the second one is that the total mass of $\nu_t$ does not vary in time as a consequence of the divergence theorem. 
The reader can find more detailed discussions in \cref{subsec:wasserstein_flow} or \cite{Santambrogio:2015}. Following  \cite{ambrosio2008gradient}, a natural choice is to choose $V_t$ as the negative gradient of the differential of $\F(\nu_t)$ at $\nu_t$, since it corresponds to a gradient flow of $\F$ associated with the $W_2$ metric (see \cref{subsec:gradient_flows_functionals}). %The choice of $V_t$ determines the path $\nu_t$, therefore there must be a direct link between $V_t$ and $\F(\nu_t)$ if one wants to construct a gradient flow for of $\F$. 
%Following  \cite{ambrosio2008gradient}, such link is obtained by choosing $V_t$ to the negative gradient of the differential of $\F(\nu_t)$ at $\nu_t$.
By \cref{prop:differential_mmd}, we know that the differential of $\F(\nu_t)$  at $\nu_t$ is given by $f_{\mu,\nu_t}$, hence $V_t(x) = -\nabla f_{\mu,\nu_t}(x)$.\footnote{Also, $V_t=\nabla V+\nabla W \otimes \nu_t$ (see \cref{sec:gradient_flow}) where $\otimes$ denotes the classical convolution.} The
gradient flow of $\F$ is then defined by the solution $(\nu_t)_{t\geq 0}$ of
\begin{align}\label{eq:continuity_mmd}
	\partial_t \nu_t = div(\nu_t \nabla f_{\mu,\nu_t}).
\end{align}
Equation \cref{eq:continuity_mmd} is non-linear in that the vector field depends itself on $\nu_t$. This type of equation is associated in the probability theory literature to the so-called McKean-Vlasov process \cite{kac1956foundations,mckean1966class},
\begin{align}\label{eq:mcKean_Vlasov_process}
	d X_t = -\nabla f_{\mu,\nu_t}(X_t)dt \qquad X_0\sim \nu_0.
\end{align}

In fact,  \cref{eq:mcKean_Vlasov_process} defines a process $(X_t)_{t\geq 0}$ whose distribution $(\nu_t)_{t\geq 0}$ satisfies \cref{eq:continuity_mmd}, as shown in \cref{prop:existence_uniqueness}. 
$(X_t)_{t\geq 0}$ can be interpreted as the trajectory of a single particle, starting from an initial random position $X_0$ drawn from $\nu_0$. The trajectory is driven by the velocity field $-\nabla f_{\mu,\nu_t}$, and is affected by other particles. These interactions are captured by the velocity field through the dependence on the current distribution $\nu_t$ of all particles.
%Existence and uniqueness of a solution to \cref{eq:continuity_mmd,eq:mcKean_Vlasov_process} is guaranteed under mild conditions on the kernel $k$ and are provided in \cref{thm:existence_uniqueness}:
%\begin{theorem}\label{thm:existence_uniqueness}(Existence and uniqueness)
%	Under \manote{some assumptions}, and given $\nu_0\in \mathcal{P}_2(\X)$ there exists a unique process $(\X_t)_{t\geq 0}$ with $X_0\sim \nu_0$ and satisfying the McKean-Vlasov equation in \cref{eq:mcKean_Vlasov_process}. Moreover, the distribution $\nu_t$ of $X_t$ is the unique solution of \cref{eq:continuity_mmd} in a weak sense and hence defines a gradient flow of $\F$. 
%\end{theorem}
%A proof of \cref{thm:existence_uniqueness} is provided in \manote{proof} and relies on standard existence and uniqueness results of McKean-Vlasov processes under regularity of the map $(x,\nu)\mapsto \nabla f_{\mu,\nu}(x)$ \cite{Jourdain:2007}. Such regularity is ensured if the kernel $k$ has Lipschitz gradients for instance. Besides, the existence and uniqueness of the process itself, one would like the functional $\F$ to decrease along the path $\nu_t$ and ideally to converge towards $0$. While the latter is hard to obtain and will be discussed in \cref{sec:Lojasiewicz_inequality}, the first property is rather easy to get and is the object of \cref{prop:decay_mmd}:
Existence and uniqueness of a solution to \cref{eq:continuity_mmd,eq:mcKean_Vlasov_process} are guaranteed in the next proposition, whose proof is given \cref{proof:prop:existence_uniqueness}.
% is guaranteed using \cite{jourdain2007nonlinear}, \cite{ambrosio2008gradient} respectively (see \cref{proof:prop:existence_uniqueness} for a full proof).
\begin{proposition}\label{prop:existence_uniqueness}
	Let $\nu_0\in \mathcal{P}_2(\X)$. %Assume that $\nabla k$ is Lipschitz continuous. 
	Then, under \cref{assump:lipschitz_gradient_k}, there exists a unique process $(X_t)_{t\geq 0}$  satisfying the McKean-Vlasov equation in \cref{eq:mcKean_Vlasov_process} such that $X_0 \sim \nu_0$. Moreover, the distribution $\nu_t$ of $X_t$ is the unique solution of \cref{eq:continuity_mmd} starting from $\nu_0$, and defines a gradient flow of $\F$. 
\end{proposition}
Besides existence and uniqueness of the gradient flow of $\F$, one expects $\F$ to decrease along the path $\nu_t$ and ideally to converge towards $0$. The first property, stated in the next proposition, is rather easy to get and is the object of \cref{prop:decay_mmd}, similar to the result for KSD flow in \cite[Section 3.1]{Mroueh:2019}.
\begin{proposition}\label{prop:decay_mmd}
	%Under the assumptions of \cref{prop:existence_uniqueness}, 
	Under \cref{assump:lipschitz_gradient_k}, $\F(\nu_t)$ is decreasing in time and satisfies:
	\begin{align}\label{eq:time_evolution_mmd}
		\frac{d\F(\nu_t)}{dt}= - \int \Vert \nabla f_{\mu,\nu_t}(x) \Vert^2 \diff \nu_t(x).  
	\end{align}
\end{proposition}
This property results from \cref{eq:continuity_mmd} and the energy identity in \cite[Theorem 11.3.2]{ambrosio2008gradient} and is proved in \cref{proof:prop:decay_mmd}. %Convergence of the flow is harder to obtain and will be discussed in \cref{sec:Lojasiewicz_inequality}.
From \cref{eq:time_evolution_mmd}, $\F$ can be seen as a Lyapunov functional for the dynamics defined by \cref{eq:continuity_mmd}, since it is decreasing in time. Hence, the continuous-time gradient flow introduced in \cref{eq:continuity_mmd} allows to formally consider the notion of gradient descent on $\mathcal{P}_2(\X)$ with $\F$ as a cost function.
A time-discretized version of the flow naturally follows, and is provided in the next section.
%This property will be crucial to analyze the convergence towards the global minimum in \cref{sec:Lojasiewicz_inequality}.
%Although $\F(\nu_t)$ decreases in time, it can very well
%remain stuck in local minima. One way to see it, is by looking at the equilibrium condition for \cref{eq:time_evolution_mmd}. Indeed, as a non-negative and non-increasing function, $t \mapsto \F(\nu_t)$  is guaranteed to converge towards a finite non-negative limit $l$, which implies in turn that the r.h.s. of \cref{eq:time_evolution_mmd} converges to $0$. If $\nu_t$ happens to converge towards some distribution $\nu^{*}$\footnote{There are cases when $\nu_t$ does not converge to any $\nu^*$. This would happen if the sequence $(\nu_t)_{t\geq 0}$ is not tight.} 
%it is possible to show that the equilibrium condition condition \cref{eq:equilibrium_condition} must hold (\cite[Proposition 2]{mei2018mean}
%):
%\begin{align}\label{eq:equilibrium_condition}
%\int \Vert \nabla f_{\mu,\nu^{*}}(x)\Vert^2 \diff \nu^{*}(x) =0  
%\end{align}
%Condition \cref{eq:equilibrium_condition} does not necessarily implies that $\nu^{*}$ is a global optimum (\cite[Theorem 6]{mei2018mean} and \cite{rotskoff2019global}). 
%We remark on a claim that  KSD flow converges globally, (recall that KSD is related to MMD) \cite[Proposition 3, Appendix B.1]{Mroueh:2019}. This  requires an assumption \cite[Assumption A]{Mroueh:2019} that amounts to assuming that equilibria satisfying \cref{eq:equilibrium_condition} do not exist \manote{ref to appendix}.
%Actually, convergence of the flow is harder to obtain and will be discussed at length in \cref{sec:convergence_mmd_flow}.
%Meanwhile, we provide a time discretization of the MMD flow.

%This can happen for instance when the negative Sobolev norm in \cref{eq:inequality_neg_sobolev} diverges. 
%\asnote{I think that the same problem happens with the dynamics of SVGD. Because KSD = 0 doesn't imply p = q unless absolute continuity + other requirements} 
%Indeed $t \mapsto \F(\nu_t)$ is a non-negative decreasing function, it must therefore converge to some limit, which implies in turn that its time derivative would also converge to $0$. Assuming that $\nu_t$ also converged to some limit distribution $\nu^{*}$
%In \cite[Proposition 3]{Mroueh:2019} such equilibria are excluded by assumption \cite[Assumption A]{Mroueh:2019} to provide convergence towards a global minimizer for the KSD. 
%In \cref{sec:Lojasiewicz_inequality} we show global convergence, under the boundedness at all times $t$ of a specific distance between $\nu_t$ and $\mu$.
%If $\nu^*$ turns out to have a positive density, then $f_{\mu,\nu^{*}}(x)$ would be constant everywhere. This in turn would mean that $f_{\mu,\nu^{*}}=0$ when the RKHS does not contain constant functions, as for a gaussian kernel. Hence, $\nu^*$ would be a global optimum since $\F(\nu^{*})=0$. 
%In \cite{mroueh2018regularized}, it is assumed that the only distribution $\nu^*$ satisfying the equilibrium condition is equal to $\mu$\aknote{true?}.  However, the limit distribution $\nu^*$  might be very singular, it could even be a dirac distribution. This suggests that the gradient flow could converge to a suboptimal solution $\nu^*$ for which

%\cref{eq:equilibrium_condition} is true. Actually, convergence of the flow is harder to obtain and will be discussed in \cref{sec:Lojasiewicz_inequality}.
 %We know motivate the study of this flow by recent considerations and applications in Machine Learning.

%We conclude this section with a remark referring to other well-known gradient flows in Wasserstein space.
%\begin{remark}\label{remark:gradient_flow}
%A different choice for $\F$  leads to a different continuity equation. A celebrated example is the case where $\F$ is the KL divergence between the target distribution $\mu$ and a current distribution $\nu_t$: $KL(\nu_t\Vert \mu ) =  \int \log (\nu_t(x))\diff \nu_t(x) -\int \log(\mu(x))\diff \nu_t(x)$. In \cite{jordan1998variational}, it was shown, under mild conditions on $\mu$ and $\nu_0$, that the gradient flow associated to the $KL$ leads to the so-called Fokker-Planck equation: $ \partial_t \nu_t = - div(\nu_t \nabla \log(\mu(x))) + \Delta \nu_t $ with Langevin diffusion as its corresponding process: $d X_t = \nabla \log(\mu(x))\diff t + \sqrt{2} \diff W_t $ with $X_0\sim \nu_0$ and $W_t$ is a Brownian motion.
 
 %While the entropy term in the $KL$ functional prevents the particle from "crashing" onto the mode of $\mu$, this role could be played by the interaction energy $W$ for $MMD^2$ defined in \cref{eq:potentials}. Indeed, consider for instance the gaussian kernel $k(x,x')=e^{-\|x-x'\|^2}$. It is convex thus attractive at long distances ($\|x-x'\|>1$) but not at small distances (so repulsive).\aknote{not very precise yet but i think the interpretation could be interesting}

%The solution to the Fokker-Planck equation describing the gradient flow of the $KL$ can be shown to converge towards $\mu$ under mild assumptions. This follows from the displacement convexity of the $KL$ along the Wasserstein geodesics \manote{ref}. Unfortunately the $MMD^2$ is not displacement convex in general as we will see in \manote{ref}. This makes the task of proving the convergence of the gradient flow of the $MMD^2$ to the global optimum $\mu$ much harder. In fact, when the external potential $V$ in \cref{eq:potentials} is not convex, which is the case for instance when $k$ is a gaussian kernel, it was shown that the diffusion may admit several local minima (see \cite{herrmann2010non,tugaut2014phase}) \manote{make sure that these conditions are relevant for us}. Moreover, we show in \cref{sec:Lojasiewicz_inequality} that local minima which are not global exist and that it is rather easy to reach them.\aknote{hm wrong section?}
%\end{remark}
 

%\begin{align}\label{eq:mmd_flow}
%	\partial_t \nu_t = div(\nu_t \nabla f_{\mu,\nu_t})
%\end{align}
%for an initial choice of distribution $\nu_0$. In \cref{eq:mmd_flow}, $f_{\mu,\nu_t}$ is the witness function between $\mu$ and $\nu_t$:% defined and is given by:
%\[
%f_{\mu,\nu_t}(\theta) =  \int k(\theta,\theta')(\diff \nu_t(\theta')-\diff\mu(\theta') ) 
%\]
%\cref{eq:mmd_flow} can be shown to have a unique solution and has a particle version which is described by:
%\begin{align}\label{eq:particle_equation}
%	\dot{X}_t = -\nabla f_{\mu,\nu_t}(X_t) \qquad X_0\sim \nu_0
%\end{align}
%In \manote{section} we describe the properties of this flow and explain why in general it might not reach the global optimum.


%\input{sections/background_OT}

%
%In this section we consider the setting of learning a neural network to solve a regression problem in a similar context as in \cite{Rotskoff:2019} and show how this problem is related minimizing an MMD between an unknown target distribution and the current one. The neural network can be written as 
%
%
%
%From now on we consider $\mathcal{P}_2(\X)$ the set of probability distributions with finite second moments defined over a domain $\X\subset \R^d$.
%The Maximum Mean Discrepancy (MMD) between two distributions $\mu$ and $\nu$ in $\mathcal{P}_2(\X)$ is defined by 
% \begin{align}\label{eq:closed_form_MMD}
%MMD^2(\mu,\nu) = \int k(x,x')(\diff\mu(x) -\diff\nu(x))(\diff\mu(x') -\diff\nu(x'))
%\end{align}
%where $k$ is a positive semi-definite kernel \cite{gretton2012kernel}.  We are interested in minimizing the MMD between an unknown target distribution $\mu$ and a current proposal distribution $\nu$:
%\begin{align}
%	\nu^{*} \in \arg\min_{\nu\in \mathcal{P}_2(\X)} MMD(\mu,\nu)
%\end{align}
%When $k$ is characteristic, the MMD defines a distance over $\mathcal{P}_2(\X)$ hence the only optimal solution would be $\nu^*=\mu$.
%
%
%Given a fixed target distribution $\mu$ known only indirectly through observations and an initial distribution  $\nu_0$, we consider the problem of constructing flow of distributions $(\nu_t)_{t \geq 0}$ that would decrease $MMD(\mu,\nu_t)$ in time and eventually converge towards $\mu$. There are many ways one could construct such sequences and a trivial way would be to consider mixtures between samples from $\mu$ and samples $\nu_0$ with proportions gradually in favor of $\mu$:
%\[
%\nu_t = (1-e^{-t})\mu +e^{-t}\nu_0
%\]
%However, such choice would be effectively infeasible in many cases, especially when samples from $\mu$ are  indirectly observed as in the case of regression with neural networks \cite{Rotskoff:2019}.  
%
%
%
%\begin{align}
%	\nu^* = 
%\end{align} 
%
%
%To introduce the problem we consider the consider a setting similar to \cite{Rotskoff:2019}
%
%
%
%
%For a given kernel $k$ defined on a domain $\X\subset \R^d$, we denote by $\kH$ its corresponding Reproducing Kernel Hilbert Space \manote{some reference here needed}. The Maximum Mean Discrepancy between two distributions $\mu$
%Under mild conditions \manote{write conditions} on the kernel $k$, it is possible to define a distance on $\mathcal{P}_2(\X)$ by finding a function $f$ in $\mathcal{B}$ that maximizes the mean difference between two given distributions $\mu$ and $\nu$.  
%
%
% $\kH$ is a Hilbert space with inner product $\langle .,. \rangle_{\kH}$ and corresponding norm $\Vert . \Vert_{\kH}$. The unit ball in $\kH$ which will be denoted as $\mathcal{B}$ is simply the set of functions $f$ in $\kH$ such that $\Vert f\Vert_{\kH}\leq 1 $:
%\begin{align}\label{eq:unit_ball_RKHS}
%\mathcal{B} = \{ f\in \kH : \quad \Vert f\Vert_{\kH}\leq 1 \}
%\end{align}
% Such distance is called the Maximum Mean Discrepancy  (MMD) \cite{Gretton:2012}:
%\begin{align}\label{eq:MMD}
%MMD(\mu,\nu) = \sup_{g\in \mathcal{B}} \int g\diff\mu - \int g \diff\nu
%\end{align}
%The maximization problem in \cref{eq:MMD} is achieved for an optimal $g^*$ in $\mathcal{B}$ that is proportional to the  witness function between $\nu$ and $\mu$:
%\begin{align}\label{eq:witness_function}
%f_{\nu,\mu}(z) = \int k(.,z)\diff \mu - \int k(.,z)\diff \nu  \qquad z\in \X
%\end{align}
%This allows to express the $MMD$ as the norm of \cref{eq:witness_function} $f_{\nu,\mu}$:
%\begin{align}\label{eq:mmd_norm_witness}
%MMD(\mu,\nu) = \Vert f_{\nu,\mu} \Vert_{\mathcal{H}} 
%\end{align}
%Furthermore, a closed form expression in terms of expectations of the kernel under $\mu$ and $\nu$ can be obtained \cite{gretton2012kernel}:
%\begin{align}\label{eq:closed_form_MMD}
%MMD^2(\mu,\nu) = \int k\diff\mu \diff\mu + \int k\diff\nu \diff \nu - 2\int k\diff\mu \diff \nu
%\end{align}
%When samples from both $\mu$ and $\nu$ are available \cref{eq:closed_form_MMD} can be estimated using those samples. For a fixed target distributions $\mu$ we will consider the loss functional defined as:
%\begin{align}\label{eq:loss_functional}
%\F(\nu) = \frac{1}{2} MMD^2(\mu,\nu) \qquad \forall \nu \in \mathcal{P}_2(\X).
%\end{align}
%The next section describes the dynamics of the gradient flow of \cref{eq:loss_functional} under the $2$-Wasserstein metric as defined in \cref{subsec:gradient_flows_functionals}.
%%The MMD was successfully used for training generative models (\cite{mmd-gan,Binkowski:2018,Arbel:2018}) where it is used in a loss functional to learn the parameters of the generator network. This motivate the  
%
%
%
%
%
%In this section we recall how to endow the space of probability measures $\mathcal{P}(\X)$ on $\X$ a convex  subset of $\R^d$ with a distance (e.g, optimal transport distances), and then deal with gradient flows of suitable functionals on such a metric space. The reader may refer to the recent review of \cite{santambrogio2017euclidean} for further details. For a given distributions $\nu\in\mathcal{P}(\X)$ and an integrable function $f$ under $\nu$, the expectation of $f$ under $\nu$ will be written either as $\nu(f)$ or $\int f \diff\nu$ depending on the context. 
%




%\section{Discretized Gradient flow of the MMD}
\subsection{Euler scheme}
%The continuous-time gradient flow introduced in \cref{eq:continuity_mmd} allows to formally consider the notion of gradient descent on $\mathcal{P}_2(\X)$ with $\F$ as a cost function. 
We consider here a forward-Euler scheme of \cref{eq:continuity_mmd}. For any $T: \X \rightarrow \X$ a measurable map, and $\nu \in \mathcal{P}_2(\X)$, we denote the pushforward measure by $T_{\#}\nu$ (see \cref{subsec:wasserstein_flow}). Starting from $\nu_0\in \mathcal{P}_2(\X)$ and using a step-size $\gamma>0$, a sequence $\nu_n\in \mathcal{P}_2(\X)$ is given by iteratively applying
\begin{align}\label{eq:euler_scheme}
	\nu_{n+1} = (I - \gamma \nabla f_{\mu,\nu_n})_{\#}\nu_n.
\end{align} 
For all $n\ge 0$, equation \cref{eq:euler_scheme} is the distribution of the process defined by
\begin{align}\label{eq:euler_scheme_particles}
	X_{n+1} = X_n - \gamma \nabla f_{\mu,\nu_n}(X_n) \qquad X_0\sim \nu_0.
\end{align}
The asymptotic behavior of \cref{eq:euler_scheme} as $n\rightarrow \infty$ %is easier to analyze than the continuous-time flow. \aknote{thought this was false? (since we have PL+the barrier in continuous time) but that the rates in discretized times are stronger} 
%This 
will be the object of \cref{sec:convergence_mmd_flow}. For now, we provide a guarantee that the sequence $(\nu_n)_{n\in \mathbb{N}}$ approaches $(\nu_t)_{t\geq 0}$ as the step-size $\gamma\rightarrow 0$.
\begin{proposition}\label{prop:convergence_euler_scheme}
	Let $n\ge0$. Consider $\nu_n$ defined in \cref{eq:euler_scheme}, and the interpolation path $\rho_t^{\gamma}$ defined as: $\rho_t^{\gamma} = (I-(t- n\gamma) \nabla f_{\mu,\nu_n})_{\#}\nu_n$, $\forall t\in [n\gamma,(n+1)\gamma)$. Then, under \cref{assump:lipschitz_gradient_k},  $\forall \;T>0$,%\manote{asumptions: Lipschitz gradient + bounded hessian} 
	\begin{align}
		W_2(\rho_t^{\gamma},\nu_t)\leq \gamma C(T) \quad \forall t\in [0,T]
	\end{align}
	where $C(T)$ is a constant that depends only on $T$.
\end{proposition} 
 A proof of \cref{prop:convergence_euler_scheme} is provided in \cref{proof:prop:convergence_euler_scheme} and relies on standard techniques to control the discretization error of a forward-Euler scheme.
\cref{prop:convergence_euler_scheme} means that $\nu_n$ can be linearly interpolated giving rise to a path $\rho_t^{\gamma}$ which gets arbitrarily close to $\nu_t$ on bounded intervals. Note that as $T \rightarrow \infty$ the bound $C(T)$ it is expected to blow up. However, this result is enough to show that \cref{eq:euler_scheme} is indeed a discrete-time flow of $\F$. In fact, provided that $\gamma$ is small enough, $\F(\nu_n)$ is a decreasing sequence, as shown in \cref{prop:decreasing_functional}.
\begin{proposition}\label{prop:decreasing_functional}
 	Under \cref{assump:lipschitz_gradient_k}, and for $\gamma \leq 2/3L$, the sequence $\F(\nu_n)$ is decreasing, and
	\begin{align*}
	\F(\nu_{n+1})-\F(\nu_n)\leq -\gamma (1-\frac{3\gamma}{2}L )\int \Vert \nabla f_{\mu, \nu_n}(X)\Vert^2 \diff \nu_n, \quad\forall n\geq 0.
	\end{align*}
\end{proposition}
\cref{prop:decreasing_functional}, whose proof is given in \cref{proof:prop:decreasing_functional}, is a discrete analog of \cref{prop:decay_mmd}. %The proofs and detailed assumptions are given in \cref{proof:prop:decreasing_functional}. 
In fact, \cref{eq:euler_scheme} is intractable in general as it requires the knowledge of $\nabla f_{\mu,\nu_n}$ (and thus of $\nu_n$) exactly at each iteration $n$. Nevertheless, we present in \cref{sec:sample_based} a practical algorithm using a finite number of samples which is provably convergent towards \cref{eq:euler_scheme} as the sample-size increases. We thus begin by studying the convergence properties of the time discretized MMD flow \cref{eq:euler_scheme} in the next section.% \asnote{Related works here?}
