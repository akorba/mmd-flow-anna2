\section{Introduction}


%%%%% definitions and relation to fokker planck and Sobolev

%% AG: I used \nu, not \nu_0 as the starting distribution, to try to keep things simple in the intro, but we could revert to \nu_0

We address the problem of defining a gradient flow  on the space of probability distributions endowed with the Wasserstein metric, which transports probability mass from a starting distribtion $\nu$ to a target distribution $\mu$.   Our flow is defined on the maximum mean discrepancy (MMD) \cite{gretton2012kernel}, an integral probability metric \cite{Mueller97} which uses the unit ball in a characteristic RKHS \cite{sriperumbudur2010hilbert} as its witness function class.
Specifically, we choose the function in the witness class that has the largest difference in expectation under $\nu$ and $\mu$: this difference constitutes the MMD.
The idea of descending a gradient flow over the space of distributions can be traced to the seminal work of \cite{jordan1998variational}, who revealed that the Fokker-Planck equation is a gradient flow of the Kullback-Leibler divergence. Its time-discretization leads to the celebrated Langevin Monte Carlo algorithm, which comes with strong convergence guarantees  (see \cite{durmus2018analysis}), but requires the knowledge of an analytical form of the target $\mu$.
A recent gradient flow approach, Stein Variational Gradient Descent \cite{liu2017stein}, also  leverages this analytical $\mu$.



%% AG: they claim to have shown convergence, so if we are going to criticise this, we should be specific.
%% otherwise we might just annoy the authors.
%Their relevance is demonstrated empirically but guarantees of convergence remain an open question.


%: given distributions $\nu$ and $\mu$, the MMD finds the witness function in the unit ball of a reproducing kernel Hilbert space that has the largest difference in expectation under $\nu$ and $\mu$. The difference in expectations then constitutes the MMD, which is a metric for characteristic kernels \cite{sriperumbudur2010hilbert}. 


%%%% connections in machine learning

The study of particle flows defined on the MMD relates to two important topics in modern machine learning. The first is in training Implicit Generative Models, notably generative adversarial networks \cite{gans}.  Integral probability metrics have been used extensively as critic functions in this setting: these include the Wasserstein distance \cite{towards-principled-gans,wgan-gp,sinkhorn-igm} and maximum mean discrepancy \cite{gen-mmd,Li:2015,Li:2017a,cramer-gan,Binkowski:2018,Arbel:2018}.  In \cite[Section 3.3]{Mroueh:2019}, a connection between IGMs and particle transport is proposed, where it is shown that gradient flow on the witness function of an integral probability metric takes a similar form to the generator update in a GAN. The critic IPM in this case is the Kernel Sobolev Discrepancy (KSD), which has an additional gradient norm constraint on the witness function compared with the MMD, and is intended as an approximation to the negative Sobolev distance from the optimal transport literature \cite{Otto:2000,Villani:2009,Peyre:2011}.  There remain certain differences between gradient flow and GAN training, however.  First, and most obviously, gradient flow can be approximated by representing $\nu$ as a set of particles, whereas in a GAN $\nu$ is the output of a generator network. The requirement that this generator network be a smooth function of its parameters causes a departure from pure particle flow. Second, in modern implementations \cite{Li:2017a,Binkowski:2018,Arbel:2018}, the kernel used in computing the critic witness function for an MMD GAN critic is parametrized by a deep network, and an alternating optimization between the critic parameters and the generator parameters is performed. Despite these differences, we anticipate that the theoretical study of MMD flow convergence will provide helpful insights into conditions for GAN convergence, and ultimately, improvements to GAN training algorithms.

Regarding the second topic, we note that the properties of gradient descent for large neural networks
have been modeled using the convergence towards a global optimum of particle transport in the population limit, when the number of particles goes to infinity  \cite{rotskoff2018neural,chizat2018global,mei2018mean,sirignano2018mean}. 
In particular, \cite{rotskoff2019global} show that gradient descent on the parameters of a neural network can also be seen as a particle transport problem, which has as its population limit a gradient flow of a  functional defined for probability distributions over the parameters of the network. 
This functional is in general non-convex, which makes the convergence analysis challenging.
The particular structure of the MMD allows us to relate its gradient flow to neural network optimization in a well-specified regression setting similar to \cite{rotskoff2019global} (we make this connection explicit in Appendix \ref{subsec:training_neural_networks}).

%%AG: Here I refer to the appendix, but maybe it's better to have this appendix reference somewhere in the main body?  Right now I don't see it, though I agree that the connection details should be kept to the appendix.
 


%%%% what we show, and why we show it

Our main contribution in this work is to establish conditions for convergence of MMD gradient flow to its {\em global optimum}.
%, both in the continuous-time popultion setting, and following time discretisation and sample approximation.
We give detailed descriptions of  MMD flow for both its continuous-time and discrete instantiations in \cref{sec:gradient_flow}.
In particular, the MMD flow may employ a sample approximation for the target $\mu$: unlike e.g. Langevin Monte Carlo,
it does not require  $\mu$ in analytical form.
Global convergence is especially challenging to prove: while functionals  that are \textit{displacement convex} can be shown to converge towards a global optimum \cite{ambrosio2008gradient}, non-convex functionals, like the MMD,  require different tools to analyze the behavior of their gradient flow.
 A modified gradient flow is proposed in \cite{rotskoff2019global} that uses particle birth and death to reach global optimality.
Global optimality may also be achieved simply by teleporting particles from $\nu$ to $\mu$, as occurs for the Sobolev Discrepancy absent a kernel regualriser \cite[Theorem 4, Appendix D]{Mroueh:2019}.
Note, however, that the regularised Kernel Sobolev Discrepancy does not rely on teleportation.

%% AG: this is the sentence for later - I have actually incorporated it now, below Theorem 6.

\iffalse
We note the claim of \cite[Proposition 3, Appendix B.1]{Mroueh:2019} that global convergence may be achieved for a flow on the Kernel Sobolev Discrepancy, with no barrier (the regularised KSD is related to the MMD; see Introduction). This claim requires an assumption \cite[Assumption A]{Mroueh:2019} that amounts to assuming that the algorithm is not in a local minimum, however.\agnote{maybe add a short appendix section - although  it's not urgent.}

\fi


%displacement convexity\agnote{can we add appendix ref here?}.

Our approach takes  inspiration in particular from \cite{Bottou:2017}, where it is shown that although the $1$-Wasserstein distance is non-convex, it can be optimized up to some barrier that depends on the diameter of the target distribution.
Similarly to \cite{Bottou:2017}, we provide in \cref{sec:convergence_mmd_flow} a barrier on the gradient flow of the MMD, although
the tightness of this barrier in terms of the target diameter remains to be established.
% a tighter bound remains challenging.
We  obtain a further condition on the evolution of the flow to ensure global optimality, and give rates of convergence in that case, however
the condition is a strong one: it implies that the negative Sobolev distance between the target and the current particles remains bounded at all times.

This second  condition proves useful however, since it leads to a new and modified gradient flow based on noise injection (Section \ref{sec:discretized_flow}), with more tractable
conditions for convergence. Encouragingly, the
noise injection is shown in practice to ensure convergence in a simple illustrative case where the original MMD flow fails.
Finally, while our emphasis has been on establishing conditions for convergence, we note that MMD gradient flow has a remarkably simple
$O(M+N)$ implementation for $N$ $\nu$-samples and $M$ $\mu$-samples, and requires only evaluating the gradient of the kernel $k$ on the given samples.


%% AG: not sure what to do with these two references, or how they fit in with our discussion
%In alternative approaches, \cite{chizat2018global} show that non-convex functionals with homogeneity structure can only converge towards global optima, and \cite{mei2018mean} provide global convergence guarantees for a regularized class of functionals using entropic regularization. 
 

 
%Generalization bounds for IGMs are often provided under the assumption that the global optimum is reached \cite{Uppal:2019}.
%In practice, it is hard to guarantee global convergence as the optimization problem is non-convex. Furthermore, the parametric setting introduces additional sources of non-convexity which makes such analysis hard to perform.

%Each iteration involves solving a linear system which provides an estimate of the KDS and its gradient.  
%In \cite{liu2017stein} a kernelized gradient flow of the Kullback-Liebler divergence is proposed in the setting where the score of the target distribution is accessible.  
 %Experimental evidence shows the convergence properties of such algorithms towards the target $\mu$ although guarantees remain an open question.





%	\manote{maybe say something about convergence up to a barrier : Bottou}


%When the regression problem is well-specified this functional is exactly given by an MMD with specific choice for the kernel.
%
%
%In an different context, \cite{rotskoff2018neural,rotskoff2019global} establish a connection between optimization in neural networks and particle transport. It is shown that gradient descent on the parameters of a neural network  in the limit of infinitely many network parameters 
%
% is established to analyze the convergence towards a global optimum in a regime of large networks.   
%
%The question of global convergence is also important for 
%
%
%
%In this work we are interested in analyzing the convergence properties of the gradient flow of the MMD in a non-parametric setting. $N$ particles are transported from a source distribution $\nu_0$ to a target distribution $\mu$ by iteratively minimizing the MMD between those particles and samples from the target distribution. This leads to a system of interacting particles which corresponds to a population limit when the number of particles $N$ goes to infinity. This then defines a gradient flow on the space of probability distributions endowed with the $2$-Wasserstein metric. 
%	
%%Such approach builds upon recent connections with Particle Transport which can be seen as a non-parametric analog of IGMs. Indeed, \cite{Mroueh:2019}, also define systems of interacting particles using the Kernel Sobolev Discrepancy (KSD) which is an approximation to the Negative Sobolev distance that arises in the Optimal transport literature \cite{Otto:2000,Villani:2009,Peyre:2011}. The KSD is obtained by solving a linear system at each iteration of the algorithm. The obtained system is then shown to decrease the MMD at each iteration. \cite{csimcsekli2018sliced}, uses the Sliced Wasserstein distance as a cost function which is approximated using empirical CDFs and quantile functions of random one-dimensional projections of the particles. Such approximation is then shown to be consistent with the exact gradient update. In all cases, convergence towards a global solution remains an open question.
%	On the other hand, the gradient flow of the MMD leads to a simple algorithm that only requires evaluating the gradient of a positive semi-definite kernel on samples. Moreover, the simple structure of the MMD as a sum of interaction potential and external potential allows to provide consistency guarantees of such algorithm with the flow in the population limit.
%	
%Besides the algorithmic simplicity of the gradient flow of the MMD, it also has an interesting connection to neural networks optimization. In the context of a regression with a neural network and a quadratic cost function, %\cite{Rotskoff:2018a,Rotskoff:2019}
%	 \cite{rotskoff2018neural,rotskoff2019global} show that gradient descent on the parameters of the network can be seen as a particle transport problem. In the population limit this  leads to a gradient flow of a some functional defined for probability distributions over the parameters of the network. When the regression problem is well-specified, we note that such functional is given by the MMD for a specific choice for the kernel. Thus analyzing the convergence of the flow of the MMD towards a global optimum can help understand the behavior of gradient descent algorithms for large neural networks.
%	 
%Convergence of such flow towards a global solution is closely related to some form of convexity in probability space which is not satisfied for the MMD. \cite{chizat2018global} show that non-convex functional of a certain structure can only converge towards global optima. Such structure can be obtained for the MMD under particular choices of the kernel. Moreover, in the context of neural networks optimization, \cite{rotskoff2019global} provide a similar result for a modified gradient flow which exhibits non-local behavior (particles teleportation) without structural conditions on the functional. Finally, \cite{mei2018mean} also show that global convergence can be achieved using entropic regularization. While all these approaches can be applied to the MMD as well, we further exhibit a condition on the evolution of the flow that ensures global optimality. Moreover, from this analysis we also provide a modified gradient flow based on noise injection that can help convergence in practice.   
%	
%	\cref{sec:gradient_flow} is devoted to deriving the MMD flow in both its continuous-time and discrete versions. \cref{sec:convergence_mmd_flow} investigates the convergence towards a global optimum of the MMD flow. Finally, \cref{sec:discretized_flow} proposes a new algorithm based on noise injection to help improve convergence and provide consistency guarantees for such algorithm with its particle version.  
%	\manote{maybe say something about convergence up to a barrier : Bottou}
%







%
%\iffalse
%
%
%
%
%
%
%
%%OLD INTRO ON LANGEVIN MONTE CARLO
%%This paper deals with the problem of sampling from a probability measure $\mu$ on $(\R^d,\mathcal{B}(\R^d))$ which admits a density, still denoted by $\mu$, with respect to the Lebesgue measure.
%%This problem appears in machine learning, Bayesian inference, computational physics... Classical methods to tackle this issue are Markov Chain Monte Carlo methods, for instance Metropolis-Hastings algorithm, Gibbs sampling. The main drawback of these methods is that one needs to choose an appropriate proposal distribution, which is not trivial. Consequently, other algorithms based on continuous dynamics have been proposed, such as the over-damped Langevin diffusion:
%%\begin{equation}\label{eq:langevin_diffusion}
%%dX_t= -\nabla \log \mu (X_t)dt+\sqrt{2}dB_t
%%\end{equation}
%%where $(B_t)_{t\ge0}$ is a $d$-dimensional Brownian motion. The Langevin Monte-Carlo (LMC) algorithm, or Unadjusted Langevin algorithm (ULA) considers the Markov chain $(X_k)_{k\ge1 }$ given by the Euler-Maruyama discretization of the diffusion \eqref{eq:langevin_diffusion}:
%%\begin{equation}\label{eq:langevin_algorithm}
%%X_{k+1} = X_k - \gamma_{k+1}\nabla \log \mu(X_k) + \sqrt{2\gamma_{k+1}G_{k+1}}
%%\end{equation}
%%where $(\gamma_k)_{k\ge1}$ is a sequence of step sizes (constant or convergent to zero), and
%%$(G_k)_{k \ge 1}$ is a sequence of i.i.d. standard $d$-dimensional Gaussian random variables. This algorithm has attracted a lot of attention... But....\aknote{say something about the requirement of the knowledge of gradient of log target and how it is difficult to estimate?}
%
%%Neural networks with a large number of parameters Theoretical explanation of 
%%gradient descent is a solution of a PDE that converges to a globally optimal solution for networks with a single hidden layer under appropriate assumptions. \cite{rotskoff2019global} propose a Birth-Death dynamics that leads to a modified PDE with the same
%%minimizer.
%
%
%%Recently, using mathematical tools from optimal transport theory and interacting particle systems, it was shown that gradient descent [RVE18, MMN18, SS18, CB18b] and stochastic gradient descent converge asymptotically to the target function in the large data limit
%
%
%Optimal transport   theory provides a powerful conceptual and mathematical framework for gradient flows on the space of distributions, and has thus found numerous applications in statistics and machine learning. A seminal work is surely the one of \cite{jordan1998variational}, who revealed that the Fokker-Planck equation is a gradient
%flow equation for the relative entropy functional (also known as the KL-divergence) with respect to the Wassertein metric. %Under appropriate conditions on the coefficient of the equation, it can be shown that its stationary distribution is unique and is the target distribution $\pi$ (see for instance \cite{pavliotis2011stochastic}, Chapter 4). 
%This led to the development of algorithms based on Langevin diffusions, where the goal is to build a diffusion process who admits some target distribution $\mu$ as its invariant measure. In particular, the Unadjusted
%Langevin Algorithm (ULA) and its Metropolis adjusted counterpart MALA have received much
%attention \cite{durmus2018analysis}). This consideration has given rise recently to a range of sampling
%algorithms based on the theory of gradient flows (see \cite{liu2017stein, csimcsekli2018sliced, bernton2018langevin, mroueh2018regularized}). %, taking the form of McKean-Vlasov ODEs or SDEs . On the other hand, gradient flows and interacting particle systems were also used recently to analyse the convergence of gradient descent algorithms for neural networks as the number of parameters grows \cite{chizat2018global,rotskoff2018neural,mei2018mean, sirignano2018mean, rotskoff2019global}. Indeed, during the optimization process, the parameters of the network can be seen as interacting particles whose dynamics can be described by a partial differential equation (PDE) in the population limit.% This corresponds to a Wasserstein gradient flow of some generally non-convex energy functional.
%In some cases, these energy functionals are closely related to the Maximum Mean Discrepancy (MMD) introduced in \cite{gretton2012kernel}. Unfortunately, such a functional is non-convex in the Wasserstein-2 space. This implies, in particular, that the flow could converge to a local solution. It was shown in \cite{chizat2018global,rotskoff2019global} that gradient descent would still converge asymptotically to a global solution in the large data limit but they rely on %appropriate assumptions on the functional
%restrictive assumptions on the kernel or modified dynamics. 
%
%In this paper, we investigate the gradient flow for the Maximum Mean Discrepancy. In particular, we underline the intrisic limits of the MMD flow regarding convergence and propose a regularized flow. The latter suggests a practical algorithm that can be used for optimizing neural networks, which simply consists in injecting noise to the particles before performing the gradient updates. We show theoretically and through experiments that the resulting noisy algorithm converges to the global optimum of the Maximum Mean Discrepancy unlike the original Wasserstein-2 flow, and provide rates of convergence. 
%%\asnote{We will probably reviewed by Mroueh so we need to compare their results to ours}
%
%This paper is organized as follows.  \cref{sec:gradient_flow} is devoted to deriving the MMD flow and motivate its study.
%\cref{sec:convergence_mmd_flow} investigates the convergence towards a global optimum of the MMD flow. Finally, \cref{sec:discretized_flow} proposes a new algorithm and provides guarantees for the discretized flow (in time and space) of converging towards the continuous flow. 
%
%
%
%
%
%
%
%
%
%Optimal transport theory provides a powerful conceptual and mathematical framework for gradient flows on the space of distributions, and has thus found numerous applications in statistics and machine learning~\cite{peyre2019computational,ambrosio2008gradient}. A seminal work is surely the one of \cite{jordan1998variational}, who revealed that the Fokker-Planck equation is a gradient
%flow equation for the relative entropy functional (also known as the Kullback-Liebler (KL) divergence) with respect to the Wasserstein metric. Recently, numerous sampling algorithms, whose goal is to provide a sequence of iterates approximating a target distribution $\mu$, have been interpreted as discretized gradient flows~\cite{liu2017stein, csimcsekli2018sliced, bernton2018langevin, mroueh2018regularized,durmus2018analysis,wibisono2018sampling}. In particular, Langevin algorithm can be seen as a gradient algorithm applied to the KL divergence ~\cite{bernton2018langevin,wibisono2018sampling,durmus2018analysis}), but it requires the knowledge of an analytic expression of the target distribution. %, taking the form of McKean-Vlasov ODEs or SDEs . On the other hand, gradient flows and interacting particle systems were also used recently to analyse the convergence of gradient descent algorithms for neural networks as the number of parameters grows \cite{chizat2018global,rotskoff2018neural,mei2018mean, sirignano2018mean, rotskoff2019global}. Indeed, during the optimization process, the parameters of the network can be seen as interacting particles whose dynamics can be described by a partial differential equation (PDE) in the population limit.% This corresponds to a Wasserstein gradient flow of some generally non-convex energy functional.
%Alternatively the Maximum Mean Discrepancy (MMD)~\cite{gretton2012kernel}, which has the advantage of being easily estimated with samples, can be used as a loss to match probability distributions. It has been thus extensively used to optimize neural networks (see \cite{Binkowski:2018},\cite{Arbel:2018}).\aknote{motivate more the MMD}
%
%%Unfortunately, such a functional is non-convex in the Wasserstein-2 space. This implies, in particular, that the flow could converge to a local solution. It was shown in \cite{chizat2018global,rotskoff2019global} that gradient descent would still converge asymptotically to a global solution in the large data limit but they rely on %appropriate assumptions on the functional
%%restrictive assumptions on the kernel or modified dynamics. 
%
%In this paper, we investigate the gradient flow of the Maximum Mean Discrepancy. We study the time convergence properties of this flow, provide an algorithm based on a space-time discretization to simulate such flow and state its convergence properties. In particular, we underline the close connection of this flow to the optimization of large neural networks, and highlights the theoretical and empirical limits its convergence. Therefore, we introduce a regularized dynamics and the related practical algorithm that can be used to overcome them, and thus to optimize neural networks. We show theoretically and through experiments that the regularized algorithm converges to the target distribution, and provide convergence rates. 
%
%This paper is organized as follows.  \cref{sec:gradient_flow} is devoted to deriving the MMD flow and motivate its study.
%\cref{sec:convergence_mmd_flow} investigate at length the time convergence properties of this flow. Finally, \cref{sec:discretized_flow} proposes a new algorithm and provides guarantees for the discretized flow (in time and space) of converging towards the continuous flow. \aknote{rks arthur: motivate the MMD as an IPM + clarify the content of the paper - we can clearly say what will be in it - + "not obvious that we use an algorithmic fix"}
%
%
%\fi




