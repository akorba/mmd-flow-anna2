\section{Introduction}

The Maximum Mean Discrepancy (MMD) \cite{gretton2012kernel} was successfully used to learn Implicit Generative Models (IGMs) in a parametric setting \cite{gen-mmd,Li:2015,Li:2017a,cramer-gan,Binkowski:2018,Arbel:2018}. It is defined in terms of an optimal \textit{witness function} which belongs to a Reproducing Kernel Hilbert Space. The MMD is then given by the difference in the expected \textit{witness function} under a
 proposal distribution $\nu$ and a target $\mu$. Its closed form estimator given in terms of empirical expectations of a positive semi-definite kernel $k$ allows to efficiently train IGMs using both samples from the IGM and the data. In this setting, a gradient descent algorithm is used to learn the parameters of the IGM using the MMD as a loss function.

In \cite{Li:2017a,Binkowski:2018,Arbel:2018}, the kernel is parametrized by a deep network and an alternating optimization between the kernel parameters and the IGM parameters is performed leading impressive results on image generation tasks. In \cite{gen-mmd,Li:2015} the same kernel $k$ is fixed during training. While, the first approach leads to better performance,  the latter can be seen as a time discretized gradient flow of the MMD defined on the space of possible parameters. Analyzing the convergence properties of such flow to the global optimum is crucial to characterize the generalization properties of the learned models. 
%Generalization bounds for IGMs are often provided under the assumption that the global optimum is reached \cite{Uppal:2019}.
%In practice, it is hard to guarantee global convergence as the optimization problem is non-convex. Furthermore, the parametric setting introduces additional sources of non-convexity which makes such analysis hard to perform.

In \cite{Mroueh:2019}, a connection between IGMs and Particle Transport is established where the latter is seen as a non-parametric analog of IGMs. Particles are transported from a source distribution $\nu_0$ to a target distribution $\mu$ by iteratively minimizing the Kernel Sobolev Discrepancy (KSD) between those particles and samples form $\mu$. The resulting algorithm is shown to decrease the MMD at each iteration while the loss itself is interpreted as an approximation to the Negative Sobolev distance that arises in the Optimal transport literature \cite{Otto:2000,Villani:2009,Peyre:2011}.
%Each iteration involves solving a linear system which provides an estimate of the KDS and its gradient.  
%In \cite{liu2017stein} a kernelized gradient flow of the Kullback-Liebler divergence is proposed in the setting where the score of the target distribution is accessible.  
The idea of descending a gradient flow for optimization or sampling can be traced to the seminal work of \cite{jordan1998variational} who revealed that the Fokker-Planck equation is a gradient flow of the Kullback-Leibler divergence. This leads to the famous Langevin Monte Carlo algorithm (see \cite{durmus2018analysis}), which requires the knowledge of an analytical form of the target $\mu$. 
%flow equation for the relative entropy functional
%More recently, \cite{csimcsekli2018sliced} propose the gradient flow of the Sliced Wasserstein distance as a sampling algorithm. 
%which is approximated using empirical CDFs and quantile functions of random one-dimensional projections of the particles. Such approximation is then shown to be consistent with the exact gradient update. 
In a similar perspective, recenty a range of sampling algorithms were developed based on the theory of gradient flows (see \cite{liu2017stein, csimcsekli2018sliced}). Experimental evidence shows the convergence properties of such algorithms towards the target $\mu$ although guarantees remain an open question.

In a different context, convergence towards a global optimum of such particle transport problems is addressed in the population limit when the number of particles goes to infinity  \cite{rotskoff2018neural,chizat2018global,mei2018mean,sirignano2018mean}. Such analyses are motivated by the properties of gradient descent for large neural networks. \cite{rotskoff2019global} show that gradient descent on the parameters of the network can also be seen as a particle transport problem. In the population limit when the number of parameters goes to infinity, this corresponds to a gradient flow of a some functional defined for probability distributions over the parameters of the network. %In the case of a regression with a quadratic cost, the functional that arises from this formal limit consists in a sum of an external potential term and an interaction potential term defined through a positive semi-definite kernel. 
Such functional is in general non-convex which makes the convergence analysis harder to obtain.

Indeed,  functionals  that are \textit{displacement convex} can be shown to converge towards a global optimum \cite{Villani:2004}. Non-convex functional require different tools to analyze the behavior of their gradient flow. In \cite{Bottou:2017}, it is shown that although the $1$-Wasserstein distance is non-convex, it can be optimized up to some barrier that depends on the diameter of the target distribution.    
\cite{chizat2018global} show that non-convex functionals with homogeneity structure can only converge towards global optima. Moreover, \cite{rotskoff2019global} propose a modified gradient flow that exhibits non-local behavior (particles teleportation) and is also shown to reach global optimality. Finally, \cite{mei2018mean} provide global convergence guarantees for a regularized class of functionals using entropic regularization. 

In this work we introduce the gradient flow of the MMD in a non-parametric setting and investigate its convergence properties towards a global optimum. Such flow is defined on a set of probability distributions endowed with the Wasserstein metric and leads to a simple algorithm that only requires evaluating the gradient of the kernel $k$ on the given samples.
The particular structure of the MMD allows to relate its gradient flow to neural networks optimization in a well-specified regression setting similar to \cite{rotskoff2019global}.
Similarly to \cite{Bottou:2017}, a barrier on the gradient flow of the MMD can be provided although providing a tighter bound remains challenging.
We further provide a condition on the evolution of the flow to ensure global optimality and provide rates of convergence in that case. Such condition implies that the Negative Sobolev distance between the target and the current particles remains bounded at all time.
Moreover, from this analysis we derive a modified gradient flow based on noise injection that it shown to help convergence in practice.

\cref{sec:gradient_flow} is devoted to deriving the MMD flow in both its continuous-time and discrete versions. \cref{sec:convergence_mmd_flow} investigates the convergence towards a global optimum of the MMD flow. Finally, \cref{sec:discretized_flow} proposes a new algorithm based on noise injection to help improve convergence and provide consistency guarantees for such algorithm with its particle version.  
%	\manote{maybe say something about convergence up to a barrier : Bottou}


%When the regression problem is well-specified this functional is exactly given by an MMD with specific choice for the kernel.
%
%
%In an different context, \cite{rotskoff2018neural,rotskoff2019global} establish a connection between optimization in neural networks and particle transport. It is shown that gradient descent on the parameters of a neural network  in the limit of infinitely many network parameters 
%
% is established to analyze the convergence towards a global optimum in a regime of large networks.   
%
%The question of global convergence is also important for 
%
%
%
%In this work we are interested in analyzing the convergence properties of the gradient flow of the MMD in a non-parametric setting. $N$ particles are transported from a source distribution $\nu_0$ to a target distribution $\mu$ by iteratively minimizing the MMD between those particles and samples from the target distribution. This leads to a system of interacting particles which corresponds to a population limit when the number of particles $N$ goes to infinity. This then defines a gradient flow on the space of probability distributions endowed with the $2$-Wasserstein metric. 
%	
%%Such approach builds upon recent connections with Particle Transport which can be seen as a non-parametric analog of IGMs. Indeed, \cite{Mroueh:2019}, also define systems of interacting particles using the Kernel Sobolev Discrepancy (KSD) which is an approximation to the Negative Sobolev distance that arises in the Optimal transport literature \cite{Otto:2000,Villani:2009,Peyre:2011}. The KSD is obtained by solving a linear system at each iteration of the algorithm. The obtained system is then shown to decrease the MMD at each iteration. \cite{csimcsekli2018sliced}, uses the Sliced Wasserstein distance as a cost function which is approximated using empirical CDFs and quantile functions of random one-dimensional projections of the particles. Such approximation is then shown to be consistent with the exact gradient update. In all cases, convergence towards a global solution remains an open question.
%	On the other hand, the gradient flow of the MMD leads to a simple algorithm that only requires evaluating the gradient of a positive semi-definite kernel on samples. Moreover, the simple structure of the MMD as a sum of interaction potential and external potential allows to provide consistency guarantees of such algorithm with the flow in the population limit.
%	
%Besides the algorithmic simplicity of the gradient flow of the MMD, it also has an interesting connection to neural networks optimization. In the context of a regression with a neural network and a quadratic cost function, %\cite{Rotskoff:2018a,Rotskoff:2019}
%	 \cite{rotskoff2018neural,rotskoff2019global} show that gradient descent on the parameters of the network can be seen as a particle transport problem. In the population limit this  leads to a gradient flow of a some functional defined for probability distributions over the parameters of the network. When the regression problem is well-specified, we note that such functional is given by the MMD for a specific choice for the kernel. Thus analyzing the convergence of the flow of the MMD towards a global optimum can help understand the behavior of gradient descent algorithms for large neural networks.
%	 
%Convergence of such flow towards a global solution is closely related to some form of convexity in probability space which is not satisfied for the MMD. \cite{chizat2018global} show that non-convex functional of a certain structure can only converge towards global optima. Such structure can be obtained for the MMD under particular choices of the kernel. Moreover, in the context of neural networks optimization, \cite{rotskoff2019global} provide a similar result for a modified gradient flow which exhibits non-local behavior (particles teleportation) without structural conditions on the functional. Finally, \cite{mei2018mean} also show that global convergence can be achieved using entropic regularization. While all these approaches can be applied to the MMD as well, we further exhibit a condition on the evolution of the flow that ensures global optimality. Moreover, from this analysis we also provide a modified gradient flow based on noise injection that can help convergence in practice.   
%	
%	\cref{sec:gradient_flow} is devoted to deriving the MMD flow in both its continuous-time and discrete versions. \cref{sec:convergence_mmd_flow} investigates the convergence towards a global optimum of the MMD flow. Finally, \cref{sec:discretized_flow} proposes a new algorithm based on noise injection to help improve convergence and provide consistency guarantees for such algorithm with its particle version.  
%	\manote{maybe say something about convergence up to a barrier : Bottou}
%







%
%\iffalse
%
%
%
%
%
%
%
%%OLD INTRO ON LANGEVIN MONTE CARLO
%%This paper deals with the problem of sampling from a probability measure $\mu$ on $(\R^d,\mathcal{B}(\R^d))$ which admits a density, still denoted by $\mu$, with respect to the Lebesgue measure.
%%This problem appears in machine learning, Bayesian inference, computational physics... Classical methods to tackle this issue are Markov Chain Monte Carlo methods, for instance Metropolis-Hastings algorithm, Gibbs sampling. The main drawback of these methods is that one needs to choose an appropriate proposal distribution, which is not trivial. Consequently, other algorithms based on continuous dynamics have been proposed, such as the over-damped Langevin diffusion:
%%\begin{equation}\label{eq:langevin_diffusion}
%%dX_t= -\nabla \log \mu (X_t)dt+\sqrt{2}dB_t
%%\end{equation}
%%where $(B_t)_{t\ge0}$ is a $d$-dimensional Brownian motion. The Langevin Monte-Carlo (LMC) algorithm, or Unadjusted Langevin algorithm (ULA) considers the Markov chain $(X_k)_{k\ge1 }$ given by the Euler-Maruyama discretization of the diffusion \eqref{eq:langevin_diffusion}:
%%\begin{equation}\label{eq:langevin_algorithm}
%%X_{k+1} = X_k - \gamma_{k+1}\nabla \log \mu(X_k) + \sqrt{2\gamma_{k+1}G_{k+1}}
%%\end{equation}
%%where $(\gamma_k)_{k\ge1}$ is a sequence of step sizes (constant or convergent to zero), and
%%$(G_k)_{k \ge 1}$ is a sequence of i.i.d. standard $d$-dimensional Gaussian random variables. This algorithm has attracted a lot of attention... But....\aknote{say something about the requirement of the knowledge of gradient of log target and how it is difficult to estimate?}
%
%%Neural networks with a large number of parameters Theoretical explanation of 
%%gradient descent is a solution of a PDE that converges to a globally optimal solution for networks with a single hidden layer under appropriate assumptions. \cite{rotskoff2019global} propose a Birth-Death dynamics that leads to a modified PDE with the same
%%minimizer.
%
%
%%Recently, using mathematical tools from optimal transport theory and interacting particle systems, it was shown that gradient descent [RVE18, MMN18, SS18, CB18b] and stochastic gradient descent converge asymptotically to the target function in the large data limit
%
%
%Optimal transport   theory provides a powerful conceptual and mathematical framework for gradient flows on the space of distributions, and has thus found numerous applications in statistics and machine learning. A seminal work is surely the one of \cite{jordan1998variational}, who revealed that the Fokker-Planck equation is a gradient
%flow equation for the relative entropy functional (also known as the KL-divergence) with respect to the Wassertein metric. %Under appropriate conditions on the coefficient of the equation, it can be shown that its stationary distribution is unique and is the target distribution $\pi$ (see for instance \cite{pavliotis2011stochastic}, Chapter 4). 
%This led to the development of algorithms based on Langevin diffusions, where the goal is to build a diffusion process who admits some target distribution $\mu$ as its invariant measure. In particular, the Unadjusted
%Langevin Algorithm (ULA) and its Metropolis adjusted counterpart MALA have received much
%attention \cite{durmus2018analysis}). This consideration has given rise recently to a range of sampling
%algorithms based on the theory of gradient flows (see \cite{liu2017stein, csimcsekli2018sliced, bernton2018langevin, mroueh2018regularized}). %, taking the form of McKean-Vlasov ODEs or SDEs . On the other hand, gradient flows and interacting particle systems were also used recently to analyse the convergence of gradient descent algorithms for neural networks as the number of parameters grows \cite{chizat2018global,rotskoff2018neural,mei2018mean, sirignano2018mean, rotskoff2019global}. Indeed, during the optimization process, the parameters of the network can be seen as interacting particles whose dynamics can be described by a partial differential equation (PDE) in the population limit.% This corresponds to a Wasserstein gradient flow of some generally non-convex energy functional.
%In some cases, these energy functionals are closely related to the Maximum Mean Discrepancy (MMD) introduced in \cite{gretton2012kernel}. Unfortunately, such a functional is non-convex in the Wasserstein-2 space. This implies, in particular, that the flow could converge to a local solution. It was shown in \cite{chizat2018global,rotskoff2019global} that gradient descent would still converge asymptotically to a global solution in the large data limit but they rely on %appropriate assumptions on the functional
%restrictive assumptions on the kernel or modified dynamics. 
%
%In this paper, we investigate the gradient flow for the Maximum Mean Discrepancy. In particular, we underline the intrisic limits of the MMD flow regarding convergence and propose a regularized flow. The latter suggests a practical algorithm that can be used for optimizing neural networks, which simply consists in injecting noise to the particles before performing the gradient updates. We show theoretically and through experiments that the resulting noisy algorithm converges to the global optimum of the Maximum Mean Discrepancy unlike the original Wasserstein-2 flow, and provide rates of convergence. 
%%\asnote{We will probably reviewed by Mroueh so we need to compare their results to ours}
%
%This paper is organized as follows.  \cref{sec:gradient_flow} is devoted to deriving the MMD flow and motivate its study.
%\cref{sec:convergence_mmd_flow} investigates the convergence towards a global optimum of the MMD flow. Finally, \cref{sec:discretized_flow} proposes a new algorithm and provides guarantees for the discretized flow (in time and space) of converging towards the continuous flow. 
%
%
%
%
%
%
%
%
%
%Optimal transport theory provides a powerful conceptual and mathematical framework for gradient flows on the space of distributions, and has thus found numerous applications in statistics and machine learning~\cite{peyre2019computational,ambrosio2008gradient}. A seminal work is surely the one of \cite{jordan1998variational}, who revealed that the Fokker-Planck equation is a gradient
%flow equation for the relative entropy functional (also known as the Kullback-Liebler (KL) divergence) with respect to the Wasserstein metric. Recently, numerous sampling algorithms, whose goal is to provide a sequence of iterates approximating a target distribution $\mu$, have been interpreted as discretized gradient flows~\cite{liu2017stein, csimcsekli2018sliced, bernton2018langevin, mroueh2018regularized,durmus2018analysis,wibisono2018sampling}. In particular, Langevin algorithm can be seen as a gradient algorithm applied to the KL divergence ~\cite{bernton2018langevin,wibisono2018sampling,durmus2018analysis}), but it requires the knowledge of an analytic expression of the target distribution. %, taking the form of McKean-Vlasov ODEs or SDEs . On the other hand, gradient flows and interacting particle systems were also used recently to analyse the convergence of gradient descent algorithms for neural networks as the number of parameters grows \cite{chizat2018global,rotskoff2018neural,mei2018mean, sirignano2018mean, rotskoff2019global}. Indeed, during the optimization process, the parameters of the network can be seen as interacting particles whose dynamics can be described by a partial differential equation (PDE) in the population limit.% This corresponds to a Wasserstein gradient flow of some generally non-convex energy functional.
%Alternatively the Maximum Mean Discrepancy (MMD)~\cite{gretton2012kernel}, which has the advantage of being easily estimated with samples, can be used as a loss to match probability distributions. It has been thus extensively used to optimize neural networks (see \cite{Binkowski:2018},\cite{Arbel:2018}).\aknote{motivate more the MMD}
%
%%Unfortunately, such a functional is non-convex in the Wasserstein-2 space. This implies, in particular, that the flow could converge to a local solution. It was shown in \cite{chizat2018global,rotskoff2019global} that gradient descent would still converge asymptotically to a global solution in the large data limit but they rely on %appropriate assumptions on the functional
%%restrictive assumptions on the kernel or modified dynamics. 
%
%In this paper, we investigate the gradient flow of the Maximum Mean Discrepancy. We study the time convergence properties of this flow, provide an algorithm based on a space-time discretization to simulate such flow and state its convergence properties. In particular, we underline the close connection of this flow to the optimization of large neural networks, and highlights the theoretical and empirical limits its convergence. Therefore, we introduce a regularized dynamics and the related practical algorithm that can be used to overcome them, and thus to optimize neural networks. We show theoretically and through experiments that the regularized algorithm converges to the target distribution, and provide convergence rates. 
%
%This paper is organized as follows.  \cref{sec:gradient_flow} is devoted to deriving the MMD flow and motivate its study.
%\cref{sec:convergence_mmd_flow} investigate at length the time convergence properties of this flow. Finally, \cref{sec:discretized_flow} proposes a new algorithm and provides guarantees for the discretized flow (in time and space) of converging towards the continuous flow. \aknote{rks arthur: motivate the MMD as an IPM + clarify the content of the paper - we can clearly say what will be in it - + "not obvious that we use an algorithmic fix"}
%
%
%\fi




