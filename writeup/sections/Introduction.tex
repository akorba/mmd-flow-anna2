\section{Introduction}

The Maximum Mean Discrepancy (MMD) \cite{gretton2012kernel} was successfully used to learn Implicit Generative Models (IGMs) in a parametric setting \cite{gen-mmd,Li:2015,Li:2017a,cramer-gan,Binkowski:2018,Arbel:2018}. It is defined in terms of an optimal \textit{witness function} which belongs to a Reproducing Kernel Hilbert Space. The MMD is then given by the difference in the expected \textit{witness function} under a proposal distribution $\nu$ and a target $\mu$. Its closed form estimator given in terms of empirical expectations of a positive semi-definite kernel $k$ allows to efficiently train IGMs using both samples form the IGM and the data. In this setting, a gradient descent algorithm is used to learn the parameters of the IGM using the MMD as a loss function.

In \cite{Li:2017a,Binkowski:2018,Arbel:2018}, the kernel is parametrized by a deep network and an alternating optimization between the kernel parameters and the IGM parameters is performed leading impressive results on image generation tasks. In \cite{gen-mmd,Li:2015} the same kernel $k$ is fixed during training. While, the first approach leads to better performance,  the latter can be seen as a time discretized gradient flow of the MMD defined on the space of possible parameters. Analyzing the convergence properties of such flow to the global optimum is crucial to characterize the generalization properties of the learned models. 
Generalization bounds for IGMs are often provided under the assumption that the global optimum is reached \cite{Uppal:2019}.
In practice, it is hard to guarantee global convergence as the optimization problem is non-convex. Furthermore, the parametric setting introduces additional sources of non-convexity which makes such analysis hard to perform.

In \cite{Mroueh:2019}, a connection between IGMs and Particle Transport is established where the latter is seen a non-parametric analog of IGMs. $N$ particles are transported from a source distribution $\nu_0$ to a target distribution $\mu$ by iteratively minimizing the Kernel Sobolev Discrepancy (KSD) between those particles and samples form $\mu$. Each iteration involves solving a linear system which provides an estimate of the KDS and its gradient.  The resulting algorithm is shown to decrease the MMD between the particles and the target samples at each iteration and the KSD is interpreted as approximation to the Negative Sobolev distance that arises in the Optimal transport literature \cite{Otto:2000,Villani:2009,Peyre:2011}. In \cite{csimcsekli2018sliced}, the Sliced Wasserstein distance is used as a cost function which is approximated using empirical CDFs and quantile functions of random one-dimensional projections of the particles. Such approximation is then shown to be consistent with the exact gradient update. Experimental evidence shows the convergence properties of such algorithms towards the target $\mu$ although guarantees remain an open question.
 
In a different context, convergence towards a global optimum of such particle transport problems is addressed in \cite{rotskoff2018neural,chizat2018global,mei2018mean,sirignano2018mean} in the population limit when $N$ goes to infinity. Such analyses is motivated by the properties of gradient descent for large neural networks. \cite{rotskoff2018neural,rotskoff2019global} show that gradient descent on the parameters of the network can also be seen as a particle transport problem. In the population limit when the number of parameters goes to infinity, this corresponds to a gradient flow of a some functional defined for probability distributions over the parameters of the network. In the case of a regression with a quadratic cost, the functional that arises from this formal limit consists in a sum of an external potential term and an interaction potential term defined through a positive semi-definite kernel. Such functional is in general non-convex which makes the convergence analysis harder to obtain.

Indeed,  functionals  that are \textit{displacement convex} can be shown to converge towards a global optimum \cite{Villani:2004}. Non-convex functional require different tools to analyze the behavior of their gradient flow. In \cite{Bottou:2017}, it is shown that although the $1$-Wasserstein distance is non-convex, it can be optimized up to some barrier that depends on the diameter of the target distribution.    
\cite{chizat2018global} show that non-convex functional of a certain structure can only converge towards global optima. Moreover, \cite{rotskoff2019global} provide a similar results provided that the gradient flow is altered to exhibits non-local behavior (particles teleportation). Finally, \cite{mei2018mean} also show provide global convergence guarantees for a regularized class of functionals. 

In this work we introduce the gradient flow of the MMD in a non-parametric setting and investigate its convergence properties towards a global optimum. Such flow is defined on a set of probability distributions endowed with the Wasserstein metric and leads to a simple algorithm that only requires evaluating the gradient of the kernel $k$ on the given samples.
The particular structure of the MMD allows to view gradient descent on the parameters of a neural networks in a well-specified regression problem as a particle transport version of the gradient flow of the MMD with a suitable kernel.

Similarly to \cite{Bottou:2017}, a barrier on the gradient flow of the MMD can be provided although providing a tighter bound remains challenging.
We further provide a condition on the evolution of the flow to ensure global optimality and provide rates of convergence in that case. Such condition implies that the Negative Sobolev distance between the target and the current particles remains bounded at all time. 
Moreover, from this analysis we derive a modified gradient flow based on noise injection that it shown to help convergence in practice.

\cref{sec:gradient_flow} is devoted to deriving the MMD flow in both its continuous-time and discrete versions. \cref{sec:convergence_mmd_flow} investigates the convergence towards a global optimum of the MMD flow. Finally, \cref{sec:discretized_flow} proposes a new algorithm based on noise injection to help improve convergence and provide consistency guarantees for such algorithm with its particle version.  
%	\manote{maybe say something about convergence up to a barrier : Bottou}


%When the regression problem is well-specified this functional is exactly given by an MMD with specific choice for the kernel.
%
%
%In an different context, \cite{rotskoff2018neural,rotskoff2019global} establish a connection between optimization in neural networks and particle transport. It is shown that gradient descent on the parameters of a neural network  in the limit of infinitely many network parameters 
%
% is established to analyze the convergence towards a global optimum in a regime of large networks.   
%
%The question of global convergence is also important for 
%
%
%
%In this work we are interested in analyzing the convergence properties of the gradient flow of the MMD in a non-parametric setting. $N$ particles are transported from a source distribution $\nu_0$ to a target distribution $\mu$ by iteratively minimizing the MMD between those particles and samples from the target distribution. This leads to a system of interacting particles which corresponds to a population limit when the number of particles $N$ goes to infinity. This then defines a gradient flow on the space of probability distributions endowed with the $2$-Wasserstein metric. 
%	
%%Such approach builds upon recent connections with Particle Transport which can be seen as a non-parametric analog of IGMs. Indeed, \cite{Mroueh:2019}, also define systems of interacting particles using the Kernel Sobolev Discrepancy (KSD) which is an approximation to the Negative Sobolev distance that arises in the Optimal transport literature \cite{Otto:2000,Villani:2009,Peyre:2011}. The KSD is obtained by solving a linear system at each iteration of the algorithm. The obtained system is then shown to decrease the MMD at each iteration. \cite{csimcsekli2018sliced}, uses the Sliced Wasserstein distance as a cost function which is approximated using empirical CDFs and quantile functions of random one-dimensional projections of the particles. Such approximation is then shown to be consistent with the exact gradient update. In all cases, convergence towards a global solution remains an open question.
%	On the other hand, the gradient flow of the MMD leads to a simple algorithm that only requires evaluating the gradient of a positive semi-definite kernel on samples. Moreover, the simple structure of the MMD as a sum of interaction potential and external potential allows to provide consistency guarantees of such algorithm with the flow in the population limit.
%	
%Besides the algorithmic simplicity of the gradient flow of the MMD, it also has an interesting connection to neural networks optimization. In the context of a regression with a neural network and a quadratic cost function, %\cite{Rotskoff:2018a,Rotskoff:2019}
%	 \cite{rotskoff2018neural,rotskoff2019global} show that gradient descent on the parameters of the network can be seen as a particle transport problem. In the population limit this  leads to a gradient flow of a some functional defined for probability distributions over the parameters of the network. When the regression problem is well-specified, we note that such functional is given by the MMD for a specific choice for the kernel. Thus analyzing the convergence of the flow of the MMD towards a global optimum can help understand the behavior of gradient descent algorithms for large neural networks.
%	 
%Convergence of such flow towards a global solution is closely related to some form of convexity in probability space which is not satisfied for the MMD. \cite{chizat2018global} show that non-convex functional of a certain structure can only converge towards global optima. Such structure can be obtained for the MMD under particular choices of the kernel. Moreover, in the context of neural networks optimization, \cite{rotskoff2019global} provide a similar result for a modified gradient flow which exhibits non-local behavior (particles teleportation) without structural conditions on the functional. Finally, \cite{mei2018mean} also show that global convergence can be achieved using entropic regularization. While all these approaches can be applied to the MMD as well, we further exhibit a condition on the evolution of the flow that ensures global optimality. Moreover, from this analysis we also provide a modified gradient flow based on noise injection that can help convergence in practice.   
%	
%	\cref{sec:gradient_flow} is devoted to deriving the MMD flow in both its continuous-time and discrete versions. \cref{sec:convergence_mmd_flow} investigates the convergence towards a global optimum of the MMD flow. Finally, \cref{sec:discretized_flow} proposes a new algorithm based on noise injection to help improve convergence and provide consistency guarantees for such algorithm with its particle version.  
%	\manote{maybe say something about convergence up to a barrier : Bottou}
%







%
%\iffalse
%
%
%
%
%
%
%
%%OLD INTRO ON LANGEVIN MONTE CARLO
%%This paper deals with the problem of sampling from a probability measure $\mu$ on $(\R^d,\mathcal{B}(\R^d))$ which admits a density, still denoted by $\mu$, with respect to the Lebesgue measure.
%%This problem appears in machine learning, Bayesian inference, computational physics... Classical methods to tackle this issue are Markov Chain Monte Carlo methods, for instance Metropolis-Hastings algorithm, Gibbs sampling. The main drawback of these methods is that one needs to choose an appropriate proposal distribution, which is not trivial. Consequently, other algorithms based on continuous dynamics have been proposed, such as the over-damped Langevin diffusion:
%%\begin{equation}\label{eq:langevin_diffusion}
%%dX_t= -\nabla \log \mu (X_t)dt+\sqrt{2}dB_t
%%\end{equation}
%%where $(B_t)_{t\ge0}$ is a $d$-dimensional Brownian motion. The Langevin Monte-Carlo (LMC) algorithm, or Unadjusted Langevin algorithm (ULA) considers the Markov chain $(X_k)_{k\ge1 }$ given by the Euler-Maruyama discretization of the diffusion \eqref{eq:langevin_diffusion}:
%%\begin{equation}\label{eq:langevin_algorithm}
%%X_{k+1} = X_k - \gamma_{k+1}\nabla \log \mu(X_k) + \sqrt{2\gamma_{k+1}G_{k+1}}
%%\end{equation}
%%where $(\gamma_k)_{k\ge1}$ is a sequence of step sizes (constant or convergent to zero), and
%%$(G_k)_{k \ge 1}$ is a sequence of i.i.d. standard $d$-dimensional Gaussian random variables. This algorithm has attracted a lot of attention... But....\aknote{say something about the requirement of the knowledge of gradient of log target and how it is difficult to estimate?}
%
%%Neural networks with a large number of parameters Theoretical explanation of 
%%gradient descent is a solution of a PDE that converges to a globally optimal solution for networks with a single hidden layer under appropriate assumptions. \cite{rotskoff2019global} propose a Birth-Death dynamics that leads to a modified PDE with the same
%%minimizer.
%
%
%%Recently, using mathematical tools from optimal transport theory and interacting particle systems, it was shown that gradient descent [RVE18, MMN18, SS18, CB18b] and stochastic gradient descent converge asymptotically to the target function in the large data limit
%
%
%Optimal transport   theory provides a powerful conceptual and mathematical framework for gradient flows on the space of distributions, and has thus found numerous applications in statistics and machine learning. A seminal work is surely the one of \cite{jordan1998variational}, who revealed that the Fokker-Planck equation is a gradient
%flow equation for the relative entropy functional (also known as the KL-divergence) with respect to the Wassertein metric. %Under appropriate conditions on the coefficient of the equation, it can be shown that its stationary distribution is unique and is the target distribution $\pi$ (see for instance \cite{pavliotis2011stochastic}, Chapter 4). 
%This led to the development of algorithms based on Langevin diffusions, where the goal is to build a diffusion process who admits some target distribution $\mu$ as its invariant measure. In particular, the Unadjusted
%Langevin Algorithm (ULA) and its Metropolis adjusted counterpart MALA have received much
%attention \cite{durmus2018analysis}). This consideration has given rise recently to a range of sampling
%algorithms based on the theory of gradient flows (see \cite{liu2017stein, csimcsekli2018sliced, bernton2018langevin, mroueh2018regularized}). %, taking the form of McKean-Vlasov ODEs or SDEs . On the other hand, gradient flows and interacting particle systems were also used recently to analyse the convergence of gradient descent algorithms for neural networks as the number of parameters grows \cite{chizat2018global,rotskoff2018neural,mei2018mean, sirignano2018mean, rotskoff2019global}. Indeed, during the optimization process, the parameters of the network can be seen as interacting particles whose dynamics can be described by a partial differential equation (PDE) in the population limit.% This corresponds to a Wasserstein gradient flow of some generally non-convex energy functional.
%In some cases, these energy functionals are closely related to the Maximum Mean Discrepancy (MMD) introduced in \cite{gretton2012kernel}. Unfortunately, such a functional is non-convex in the Wasserstein-2 space. This implies, in particular, that the flow could converge to a local solution. It was shown in \cite{chizat2018global,rotskoff2019global} that gradient descent would still converge asymptotically to a global solution in the large data limit but they rely on %appropriate assumptions on the functional
%restrictive assumptions on the kernel or modified dynamics. 
%
%In this paper, we investigate the gradient flow for the Maximum Mean Discrepancy. In particular, we underline the intrisic limits of the MMD flow regarding convergence and propose a regularized flow. The latter suggests a practical algorithm that can be used for optimizing neural networks, which simply consists in injecting noise to the particles before performing the gradient updates. We show theoretically and through experiments that the resulting noisy algorithm converges to the global optimum of the Maximum Mean Discrepancy unlike the original Wasserstein-2 flow, and provide rates of convergence. 
%%\asnote{We will probably reviewed by Mroueh so we need to compare their results to ours}
%
%This paper is organized as follows.  \cref{sec:gradient_flow} is devoted to deriving the MMD flow and motivate its study.
%\cref{sec:convergence_mmd_flow} investigates the convergence towards a global optimum of the MMD flow. Finally, \cref{sec:discretized_flow} proposes a new algorithm and provides guarantees for the discretized flow (in time and space) of converging towards the continuous flow. 
%
%
%
%
%
%
%
%
%
%Optimal transport theory provides a powerful conceptual and mathematical framework for gradient flows on the space of distributions, and has thus found numerous applications in statistics and machine learning~\cite{peyre2019computational,ambrosio2008gradient}. A seminal work is surely the one of \cite{jordan1998variational}, who revealed that the Fokker-Planck equation is a gradient
%flow equation for the relative entropy functional (also known as the Kullback-Liebler (KL) divergence) with respect to the Wasserstein metric. Recently, numerous sampling algorithms, whose goal is to provide a sequence of iterates approximating a target distribution $\mu$, have been interpreted as discretized gradient flows~\cite{liu2017stein, csimcsekli2018sliced, bernton2018langevin, mroueh2018regularized,durmus2018analysis,wibisono2018sampling}. In particular, Langevin algorithm can be seen as a gradient algorithm applied to the KL divergence ~\cite{bernton2018langevin,wibisono2018sampling,durmus2018analysis}), but it requires the knowledge of an analytic expression of the target distribution. %, taking the form of McKean-Vlasov ODEs or SDEs . On the other hand, gradient flows and interacting particle systems were also used recently to analyse the convergence of gradient descent algorithms for neural networks as the number of parameters grows \cite{chizat2018global,rotskoff2018neural,mei2018mean, sirignano2018mean, rotskoff2019global}. Indeed, during the optimization process, the parameters of the network can be seen as interacting particles whose dynamics can be described by a partial differential equation (PDE) in the population limit.% This corresponds to a Wasserstein gradient flow of some generally non-convex energy functional.
%Alternatively the Maximum Mean Discrepancy (MMD)~\cite{gretton2012kernel}, which has the advantage of being easily estimated with samples, can be used as a loss to match probability distributions. It has been thus extensively used to optimize neural networks (see \cite{Binkowski:2018},\cite{Arbel:2018}).\aknote{motivate more the MMD}
%
%%Unfortunately, such a functional is non-convex in the Wasserstein-2 space. This implies, in particular, that the flow could converge to a local solution. It was shown in \cite{chizat2018global,rotskoff2019global} that gradient descent would still converge asymptotically to a global solution in the large data limit but they rely on %appropriate assumptions on the functional
%%restrictive assumptions on the kernel or modified dynamics. 
%
%In this paper, we investigate the gradient flow of the Maximum Mean Discrepancy. We study the time convergence properties of this flow, provide an algorithm based on a space-time discretization to simulate such flow and state its convergence properties. In particular, we underline the close connection of this flow to the optimization of large neural networks, and highlights the theoretical and empirical limits its convergence. Therefore, we introduce a regularized dynamics and the related practical algorithm that can be used to overcome them, and thus to optimize neural networks. We show theoretically and through experiments that the regularized algorithm converges to the target distribution, and provide convergence rates. 
%
%This paper is organized as follows.  \cref{sec:gradient_flow} is devoted to deriving the MMD flow and motivate its study.
%\cref{sec:convergence_mmd_flow} investigate at length the time convergence properties of this flow. Finally, \cref{sec:discretized_flow} proposes a new algorithm and provides guarantees for the discretized flow (in time and space) of converging towards the continuous flow. \aknote{rks arthur: motivate the MMD as an IPM + clarify the content of the paper - we can clearly say what will be in it - + "not obvious that we use an algorithmic fix"}
%
%
%\fi




%\subsection{Training neural networks as flow of the MMD}\label{subsec:training_neural_networks}
%In this sub-section we establish a formal connection between the MMD gradient flow defined in \cref{eq:continuity_mmd} and neural networks optimization in the limit of infinitely many neurons.
%The MMD was successfully used to train generative networks \cite{Arbel:2018,Binkowski:2018}\manote{cite others}\aknote{Arthur's note: Cramer gan, MMD gan, Zoubin Gan +?}, where it is used as a loss functional between the data distribution and a parametric model distribution.
%Here we consider a different problem which arises naturally from optimizing neural networks using gradient descent. We show that this problem can be lifted to a non-parametric problem with the MMD as a cost function in a similar way as in \cite{rotskoff2018neural}. To remain consistent with the rest of the paper, the parameters of a network will be denoted by $x\in \X$ while the input and outputs will be denoted as $z$ and $y$.
% Given a neural network or any parametric function $(z,x)\mapsto \psi(z,x)$ with parameter $x \in \X $ and input data $z$ we consider the supervised learning problem:
%\begin{align}\label{eq:regression_network}
%	\min_{(x_1,...,x_m )\in \X} \frac{1}{2}\mathbb{E}_{(y,z)\sim p  } \left[ \Big\Vert y - \frac{1}{m}\sum_{i=1}^m\psi(z,x_i) \Big\Vert^2 \right ]
%\end{align}
%where $(y,z) \sim p$ are samples from the data distribution and the regression function is an average of $m$ different networks. The formulation in \cref{eq:regression_network} includes any type of networks. Indeed, the averaged function can itself be seen as one network with augmented parameters $(x_1,...,x_m)$ and any network can be written as an average of sub-networks with potentially shared weights. In the limit $m\rightarrow \infty$, the average can be seen as an expectation over the parameters under some probability distribution $\nu$. This leads to an expected network $\Psi(z,\nu) =  \int \psi(z,x) \diff \nu(x) $ and the optimization problem in \cref{eq:regression_network} can be lifted to an optimization problem in $\mathcal{P}_2(\X)$ the space of probability distributions:
%\begin{align}\label{eq:lifted_regression}
%	\min_{\nu \in \mathcal{P}_2(\X)}  \mathcal{L}(\nu) :=  \mathbb{E}_{(y,z)\sim p} \left [ \big\Vert y - \int \psi(z,x) \diff \nu(x) \big\Vert^2 \right ]
%\end{align} 
%For convenience, we consider $\bar{\mathcal{L}}(\nu)$ the function obtained by subtracting the variance of $y$ from $\mathcal{L}(\nu)$, i.e.: $\bar{\mathcal{L}}(\nu) = \mathcal{L}(\nu) - var(y) $. When the model is well specified, there exists $\mu \in \mathcal{P}_2(\X) $ such that $\mathbb{E}_{y\sim \mathbb{P}(.|z)}[y] =  \int \psi(z,x) \diff \mu(x)$. In that case, the cost function $\bar{\mathcal{L}}$ matches  the functional $\F$ defined in \cref{eq:mmd_as_free_energy}  for a particular choice of the kernel $k$. More generally, as soon as a global minimizer for  \cref{eq:lifted_regression} exists,  \cref{prop:inequality_mmd_loss} relates the two losses $\bar{\mathcal{L}}$ and $\mathcal{F}$:
%\begin{proposition}\label{prop:inequality_mmd_loss}
%	Assuming a global minimizer of \cref{eq:lifted_regression} is achieved by some $\mu\in \mathcal{P}_2(\X)$, the following inequality holds for any $\nu \in \mathcal{P}_2(\X)$:
%	\begin{align}\label{eq:inequality_mmd_nn}
%		\left(\bar{\mathcal{L}}(\mu)^{\frac{1}{2}} + \F^{\frac{1}{2}}(\nu)\right)^2
%		\geq 
%		\bar{\mathcal{L}}(\nu)
%		\geq
%		\mathcal{F}(\nu) + \bar{\mathcal{L}}(\mu)
%	\end{align}
%	where $\F(\nu)$ is defined by \cref{eq:mmd_as_free_energy} with  a kernel $k$  constructed from the data as an expected product of networks:
%\begin{align}\label{eq:kernel_NN}
%	k(x,x') = \mathbb{E}_{z\sim \mathbb{P}} [\psi(z,x)^T\psi(z,x')]
%\end{align}
%Moreover, $\bar{\mathcal{L}} = \F$ iif $\bar{\mathcal{L}}(\mu)=0$, which means that the model is well-specified. 
%\end{proposition}
%\cref{prop:inequality_mmd_loss} is shown in \cref{proof:prop:inequality_mmd_loss} and is a consequence of the optimality of $\mu$ after rearranging the terms obtained by expanding $\bar{\mathcal{L}}(\nu)$.
%The framing \cref{eq:inequality_mmd_nn} implies that optimizing $\mathcal{F}$ can decrease  $\bar{\mathcal{L}}$ and vice-versa. However, the two functionals do not generally share the same local minima although they share the same global optima in general.\aknote{not clear to me..} One interesting class of problems where \cref{eq:lifted_regression} corresponds exactly to minimizing the MMD is the student-teacher problem or the problem of distilling a pre-trained network into another network with the same architecture (see \cite{rotskoff2019global})\manote{some references}. In this case the gradient flow of the MMD defined in \cref{eq:continuity_mmd} corresponds to the population limit of the usual gradient flow of \cref{eq:regression_network} when the final layer becomes infinitely wide\aknote{same, globally we should rewrite this paragraph}. Indeed, solving \cref{eq:regression_network} is usually done using gradient descent. When the step-size approaches $0$, the parameters $(x_1,...,x_m)$ satisfy the continuous-time system of equations:
%\begin{align}\label{eq:particle_dynamics}
%	\dot{x}_i(t)= -\nabla \mathcal{L}(x_1(t),...,x_m(t)) \text{ for } i=1, \dots, m
%\end{align}  
%As pointed out in \cite{chizat2018global,rotskoff2018neural}, the dynamics in \cref{eq:particle_dynamics} can be analyzed in the "mean-field" limit when $m\rightarrow \infty$. For \cref{eq:particle_dynamics}, this leads to the continuity equation \cref{eq:continuity_mmd}. A formal statement that relates the finite population dynamics to the mean-field limit is provided in \manote{statement here}. 
%
