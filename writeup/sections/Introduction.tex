\section{Introduction}

The Maximum Mean Discrepancy (MMD) \cite{gretton2012kernel} was successfully used in many applications to learn Implicit Generative Models (IGMs) in a parametric setting \cite{gen-mmd,Li:2017a,cramer-gan,Binkowski:2018,Arbel:2018}. Its closed form estimator allows to efficiently train IGMs in a mini-batch setting. In that case the MMD is used as a loss function and gradient descent is performed on the parameters of the generative model. This can be seen as a time discretized gradient flow of the MMD defined on the space of possible parameters. Analyzing the convergence properties of such flow to the global optimum is crucial to understand the generalization properties of IGMs. However, the parametric setting introduces additional sources of non-convexity which makes such analysis hard to perform.
	
In this work we are interested in analyzing the convergence properties of such gradient flow in a non-parametric setting. Particles are transported from a source distribution $\nu_0$ to a target distribution $\mu$ by iteratively minimizing the MMD between those particles and samples from the target distribution. This leads to a system of interacting particles since the update of each of those particles depends on the configuration of all the remaining ones through the MMD. We will be particularly interested in the population limit when the number of particles goes to infinity. This then defines a gradient flow on the space of probability distributions endowed with the $2$-Wasserstein metric.	
	
Such approach builds upon recent connections with Particle Transport which can be seen as a non-parametric analog of IGMs. Indeed, \cite{Mroueh:2019}, also define systems of interacting particles using the Kernel Sobolev Discrepancy (KSD) which is an approximation to the Negative Sobolev distance that arises in the Optimal transport literature \cite{Otto:2000,Villani:2009,Peyre:2011}. The KSD is obtained by solving a linear system at each iteration of the algorithm. The obtained system is then shown to decrease the MMD at each iteration. \cite{csimcsekli2018sliced}, uses the Sliced Wasserstein distance as a cost function which is approximated using empirical CDFs and quantile functions of random one-dimensional projections of the particles. Such approximation is then shown to be consistent with the exact gradient update. In all cases, convergence towards a global solution remains an open question.
	On the other hand, the gradient flow of the MMD leads to a simple algorithm that only requires evaluating the gradient of a positive semi-definite kernel on samples. Moreover, the simple structure of the MMD as a sum of interaction potential and external potential allows to provide consistency guarantees of such algorithm with the flow in the population limit.
	
Besides the algorithmic simplicity of the gradient flow of the MMD, it also has an interesting connection to neural networks optimization. In the context of a regression with a neural network and a quadratic cost function, %\cite{Rotskoff:2018a,Rotskoff:2019}
	 \cite{rotskoff2018neural,rotskoff2019global} show that gradient descent on the parameters of the network can be seen as a particle transport problem. In the population limit this  leads to a gradient flow of a some functional defined for probability distributions over the parameters of the network. When the regression problem is well-specified, we note that such functional is given by the MMD for a specific choice for the kernel. Thus analyzing the convergence of the flow of the MMD towards a global optimum can help understand the behavior of gradient descent algorithms for large neural networks.
	 
Convergence of such flow towards a global solution is closely related to some form of convexity in probability space which is not satisfied for the MMD. \cite{chizat2018global} show that non-convex functional of a certain structure can only converge towards global optima. Such structure can be obtained for the MMD under particular choices of the kernel. Moreover, in the context of neural networks optimization, \cite{rotskoff2019global} provide a similar result for a modified gradient flow which exhibits non-local behavior (particles teleportation) without structural conditions on the functional. Finally, \cite{mei2018mean} also show that global convergence can be achieved using entropic regularization. While all these approaches can be applied to the MMD as well, we further exhibit a condition on the evolution of the flow that ensures global optimality. Moreover, from this analysis we also provide a modified gradient flow based on noise injection that can help convergence in practice.   
	
	\cref{sec:gradient_flow} is devoted to deriving the MMD flow in both its continuous-time and discrete versions. \cref{sec:convergence_mmd_flow} investigates the convergence towards a global optimum of the MMD flow. Finally, \cref{sec:discretized_flow} proposes a new algorithm based on noise injection to help improve convergence and provide consistency guarantees for such algorithm with its particle version.  
	\manote{maybe say something about convergence up to a barrier : Bottou}








%
%\iffalse
%
%
%
%
%
%
%
%%OLD INTRO ON LANGEVIN MONTE CARLO
%%This paper deals with the problem of sampling from a probability measure $\mu$ on $(\R^d,\mathcal{B}(\R^d))$ which admits a density, still denoted by $\mu$, with respect to the Lebesgue measure.
%%This problem appears in machine learning, Bayesian inference, computational physics... Classical methods to tackle this issue are Markov Chain Monte Carlo methods, for instance Metropolis-Hastings algorithm, Gibbs sampling. The main drawback of these methods is that one needs to choose an appropriate proposal distribution, which is not trivial. Consequently, other algorithms based on continuous dynamics have been proposed, such as the over-damped Langevin diffusion:
%%\begin{equation}\label{eq:langevin_diffusion}
%%dX_t= -\nabla \log \mu (X_t)dt+\sqrt{2}dB_t
%%\end{equation}
%%where $(B_t)_{t\ge0}$ is a $d$-dimensional Brownian motion. The Langevin Monte-Carlo (LMC) algorithm, or Unadjusted Langevin algorithm (ULA) considers the Markov chain $(X_k)_{k\ge1 }$ given by the Euler-Maruyama discretization of the diffusion \eqref{eq:langevin_diffusion}:
%%\begin{equation}\label{eq:langevin_algorithm}
%%X_{k+1} = X_k - \gamma_{k+1}\nabla \log \mu(X_k) + \sqrt{2\gamma_{k+1}G_{k+1}}
%%\end{equation}
%%where $(\gamma_k)_{k\ge1}$ is a sequence of step sizes (constant or convergent to zero), and
%%$(G_k)_{k \ge 1}$ is a sequence of i.i.d. standard $d$-dimensional Gaussian random variables. This algorithm has attracted a lot of attention... But....\aknote{say something about the requirement of the knowledge of gradient of log target and how it is difficult to estimate?}
%
%%Neural networks with a large number of parameters Theoretical explanation of 
%%gradient descent is a solution of a PDE that converges to a globally optimal solution for networks with a single hidden layer under appropriate assumptions. \cite{rotskoff2019global} propose a Birth-Death dynamics that leads to a modified PDE with the same
%%minimizer.
%
%
%%Recently, using mathematical tools from optimal transport theory and interacting particle systems, it was shown that gradient descent [RVE18, MMN18, SS18, CB18b] and stochastic gradient descent converge asymptotically to the target function in the large data limit
%
%
%Optimal transport   theory provides a powerful conceptual and mathematical framework for gradient flows on the space of distributions, and has thus found numerous applications in statistics and machine learning. A seminal work is surely the one of \cite{jordan1998variational}, who revealed that the Fokker-Planck equation is a gradient
%flow equation for the relative entropy functional (also known as the KL-divergence) with respect to the Wassertein metric. %Under appropriate conditions on the coefficient of the equation, it can be shown that its stationary distribution is unique and is the target distribution $\pi$ (see for instance \cite{pavliotis2011stochastic}, Chapter 4). 
%This led to the development of algorithms based on Langevin diffusions, where the goal is to build a diffusion process who admits some target distribution $\mu$ as its invariant measure. In particular, the Unadjusted
%Langevin Algorithm (ULA) and its Metropolis adjusted counterpart MALA have received much
%attention \cite{durmus2018analysis}). This consideration has given rise recently to a range of sampling
%algorithms based on the theory of gradient flows (see \cite{liu2017stein, csimcsekli2018sliced, bernton2018langevin, mroueh2018regularized}). %, taking the form of McKean-Vlasov ODEs or SDEs . On the other hand, gradient flows and interacting particle systems were also used recently to analyse the convergence of gradient descent algorithms for neural networks as the number of parameters grows \cite{chizat2018global,rotskoff2018neural,mei2018mean, sirignano2018mean, rotskoff2019global}. Indeed, during the optimization process, the parameters of the network can be seen as interacting particles whose dynamics can be described by a partial differential equation (PDE) in the population limit.% This corresponds to a Wasserstein gradient flow of some generally non-convex energy functional.
%In some cases, these energy functionals are closely related to the Maximum Mean Discrepancy (MMD) introduced in \cite{gretton2012kernel}. Unfortunately, such a functional is non-convex in the Wasserstein-2 space. This implies, in particular, that the flow could converge to a local solution. It was shown in \cite{chizat2018global,rotskoff2019global} that gradient descent would still converge asymptotically to a global solution in the large data limit but they rely on %appropriate assumptions on the functional
%restrictive assumptions on the kernel or modified dynamics. 
%
%In this paper, we investigate the gradient flow for the Maximum Mean Discrepancy. In particular, we underline the intrisic limits of the MMD flow regarding convergence and propose a regularized flow. The latter suggests a practical algorithm that can be used for optimizing neural networks, which simply consists in injecting noise to the particles before performing the gradient updates. We show theoretically and through experiments that the resulting noisy algorithm converges to the global optimum of the Maximum Mean Discrepancy unlike the original Wasserstein-2 flow, and provide rates of convergence. 
%%\asnote{We will probably reviewed by Mroueh so we need to compare their results to ours}
%
%This paper is organized as follows.  \cref{sec:gradient_flow} is devoted to deriving the MMD flow and motivate its study.
%\cref{sec:convergence_mmd_flow} investigates the convergence towards a global optimum of the MMD flow. Finally, \cref{sec:discretized_flow} proposes a new algorithm and provides guarantees for the discretized flow (in time and space) of converging towards the continuous flow. 
%
%
%
%
%
%
%
%
%
%Optimal transport theory provides a powerful conceptual and mathematical framework for gradient flows on the space of distributions, and has thus found numerous applications in statistics and machine learning~\cite{peyre2019computational,ambrosio2008gradient}. A seminal work is surely the one of \cite{jordan1998variational}, who revealed that the Fokker-Planck equation is a gradient
%flow equation for the relative entropy functional (also known as the Kullback-Liebler (KL) divergence) with respect to the Wasserstein metric. Recently, numerous sampling algorithms, whose goal is to provide a sequence of iterates approximating a target distribution $\mu$, have been interpreted as discretized gradient flows~\cite{liu2017stein, csimcsekli2018sliced, bernton2018langevin, mroueh2018regularized,durmus2018analysis,wibisono2018sampling}. In particular, Langevin algorithm can be seen as a gradient algorithm applied to the KL divergence ~\cite{bernton2018langevin,wibisono2018sampling,durmus2018analysis}), but it requires the knowledge of an analytic expression of the target distribution. %, taking the form of McKean-Vlasov ODEs or SDEs . On the other hand, gradient flows and interacting particle systems were also used recently to analyse the convergence of gradient descent algorithms for neural networks as the number of parameters grows \cite{chizat2018global,rotskoff2018neural,mei2018mean, sirignano2018mean, rotskoff2019global}. Indeed, during the optimization process, the parameters of the network can be seen as interacting particles whose dynamics can be described by a partial differential equation (PDE) in the population limit.% This corresponds to a Wasserstein gradient flow of some generally non-convex energy functional.
%Alternatively the Maximum Mean Discrepancy (MMD)~\cite{gretton2012kernel}, which has the advantage of being easily estimated with samples, can be used as a loss to match probability distributions. It has been thus extensively used to optimize neural networks (see \cite{Binkowski:2018},\cite{Arbel:2018}).\aknote{motivate more the MMD}
%
%%Unfortunately, such a functional is non-convex in the Wasserstein-2 space. This implies, in particular, that the flow could converge to a local solution. It was shown in \cite{chizat2018global,rotskoff2019global} that gradient descent would still converge asymptotically to a global solution in the large data limit but they rely on %appropriate assumptions on the functional
%%restrictive assumptions on the kernel or modified dynamics. 
%
%In this paper, we investigate the gradient flow of the Maximum Mean Discrepancy. We study the time convergence properties of this flow, provide an algorithm based on a space-time discretization to simulate such flow and state its convergence properties. In particular, we underline the close connection of this flow to the optimization of large neural networks, and highlights the theoretical and empirical limits its convergence. Therefore, we introduce a regularized dynamics and the related practical algorithm that can be used to overcome them, and thus to optimize neural networks. We show theoretically and through experiments that the regularized algorithm converges to the target distribution, and provide convergence rates. 
%
%This paper is organized as follows.  \cref{sec:gradient_flow} is devoted to deriving the MMD flow and motivate its study.
%\cref{sec:convergence_mmd_flow} investigate at length the time convergence properties of this flow. Finally, \cref{sec:discretized_flow} proposes a new algorithm and provides guarantees for the discretized flow (in time and space) of converging towards the continuous flow. \aknote{rks arthur: motivate the MMD as an IPM + clarify the content of the paper - we can clearly say what will be in it - + "not obvious that we use an algorithmic fix"}
%
%
%\fi




%\subsection{Training neural networks as flow of the MMD}\label{subsec:training_neural_networks}
%In this sub-section we establish a formal connection between the MMD gradient flow defined in \cref{eq:continuity_mmd} and neural networks optimization in the limit of infinitely many neurons.
%The MMD was successfully used to train generative networks \cite{Arbel:2018,Binkowski:2018}\manote{cite others}\aknote{Arthur's note: Cramer gan, MMD gan, Zoubin Gan +?}, where it is used as a loss functional between the data distribution and a parametric model distribution.
%Here we consider a different problem which arises naturally from optimizing neural networks using gradient descent. We show that this problem can be lifted to a non-parametric problem with the MMD as a cost function in a similar way as in \cite{rotskoff2018neural}. To remain consistent with the rest of the paper, the parameters of a network will be denoted by $x\in \X$ while the input and outputs will be denoted as $z$ and $y$.
% Given a neural network or any parametric function $(z,x)\mapsto \psi(z,x)$ with parameter $x \in \X $ and input data $z$ we consider the supervised learning problem:
%\begin{align}\label{eq:regression_network}
%	\min_{(x_1,...,x_m )\in \X} \frac{1}{2}\mathbb{E}_{(y,z)\sim p  } \left[ \Big\Vert y - \frac{1}{m}\sum_{i=1}^m\psi(z,x_i) \Big\Vert^2 \right ]
%\end{align}
%where $(y,z) \sim p$ are samples from the data distribution and the regression function is an average of $m$ different networks. The formulation in \cref{eq:regression_network} includes any type of networks. Indeed, the averaged function can itself be seen as one network with augmented parameters $(x_1,...,x_m)$ and any network can be written as an average of sub-networks with potentially shared weights. In the limit $m\rightarrow \infty$, the average can be seen as an expectation over the parameters under some probability distribution $\nu$. This leads to an expected network $\Psi(z,\nu) =  \int \psi(z,x) \diff \nu(x) $ and the optimization problem in \cref{eq:regression_network} can be lifted to an optimization problem in $\mathcal{P}_2(\X)$ the space of probability distributions:
%\begin{align}\label{eq:lifted_regression}
%	\min_{\nu \in \mathcal{P}_2(\X)}  \mathcal{L}(\nu) :=  \mathbb{E}_{(y,z)\sim p} \left [ \big\Vert y - \int \psi(z,x) \diff \nu(x) \big\Vert^2 \right ]
%\end{align} 
%For convenience, we consider $\bar{\mathcal{L}}(\nu)$ the function obtained by subtracting the variance of $y$ from $\mathcal{L}(\nu)$, i.e.: $\bar{\mathcal{L}}(\nu) = \mathcal{L}(\nu) - var(y) $. When the model is well specified, there exists $\mu \in \mathcal{P}_2(\X) $ such that $\mathbb{E}_{y\sim \mathbb{P}(.|z)}[y] =  \int \psi(z,x) \diff \mu(x)$. In that case, the cost function $\bar{\mathcal{L}}$ matches  the functional $\F$ defined in \cref{eq:mmd_as_free_energy}  for a particular choice of the kernel $k$. More generally, as soon as a global minimizer for  \cref{eq:lifted_regression} exists,  \cref{prop:inequality_mmd_loss} relates the two losses $\bar{\mathcal{L}}$ and $\mathcal{F}$:
%\begin{proposition}\label{prop:inequality_mmd_loss}
%	Assuming a global minimizer of \cref{eq:lifted_regression} is achieved by some $\mu\in \mathcal{P}_2(\X)$, the following inequality holds for any $\nu \in \mathcal{P}_2(\X)$:
%	\begin{align}\label{eq:inequality_mmd_nn}
%		\left(\bar{\mathcal{L}}(\mu)^{\frac{1}{2}} + \F^{\frac{1}{2}}(\nu)\right)^2
%		\geq 
%		\bar{\mathcal{L}}(\nu)
%		\geq
%		\mathcal{F}(\nu) + \bar{\mathcal{L}}(\mu)
%	\end{align}
%	where $\F(\nu)$ is defined by \cref{eq:mmd_as_free_energy} with  a kernel $k$  constructed from the data as an expected product of networks:
%\begin{align}\label{eq:kernel_NN}
%	k(x,x') = \mathbb{E}_{z\sim \mathbb{P}} [\psi(z,x)^T\psi(z,x')]
%\end{align}
%Moreover, $\bar{\mathcal{L}} = \F$ iif $\bar{\mathcal{L}}(\mu)=0$, which means that the model is well-specified. 
%\end{proposition}
%\cref{prop:inequality_mmd_loss} is shown in \cref{proof:prop:inequality_mmd_loss} and is a consequence of the optimality of $\mu$ after rearranging the terms obtained by expanding $\bar{\mathcal{L}}(\nu)$.
%The framing \cref{eq:inequality_mmd_nn} implies that optimizing $\mathcal{F}$ can decrease  $\bar{\mathcal{L}}$ and vice-versa. However, the two functionals do not generally share the same local minima although they share the same global optima in general.\aknote{not clear to me..} One interesting class of problems where \cref{eq:lifted_regression} corresponds exactly to minimizing the MMD is the student-teacher problem or the problem of distilling a pre-trained network into another network with the same architecture (see \cite{rotskoff2019global})\manote{some references}. In this case the gradient flow of the MMD defined in \cref{eq:continuity_mmd} corresponds to the population limit of the usual gradient flow of \cref{eq:regression_network} when the final layer becomes infinitely wide\aknote{same, globally we should rewrite this paragraph}. Indeed, solving \cref{eq:regression_network} is usually done using gradient descent. When the step-size approaches $0$, the parameters $(x_1,...,x_m)$ satisfy the continuous-time system of equations:
%\begin{align}\label{eq:particle_dynamics}
%	\dot{x}_i(t)= -\nabla \mathcal{L}(x_1(t),...,x_m(t)) \text{ for } i=1, \dots, m
%\end{align}  
%As pointed out in \cite{chizat2018global,rotskoff2018neural}, the dynamics in \cref{eq:particle_dynamics} can be analyzed in the "mean-field" limit when $m\rightarrow \infty$. For \cref{eq:particle_dynamics}, this leads to the continuity equation \cref{eq:continuity_mmd}. A formal statement that relates the finite population dynamics to the mean-field limit is provided in \manote{statement here}. 
%
