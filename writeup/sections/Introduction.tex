

\section{Introduction}

%OLD INTRO ON LANGEVIN MONTE CARLO
%This paper deals with the problem of sampling from a probability measure $\mu$ on $(\R^d,\mathcal{B}(\R^d))$ which admits a density, still denoted by $\mu$, with respect to the Lebesgue measure.
%This problem appears in machine learning, Bayesian inference, computational physics... Classical methods to tackle this issue are Markov Chain Monte Carlo methods, for instance Metropolis-Hastings algorithm, Gibbs sampling. The main drawback of these methods is that one needs to choose an appropriate proposal distribution, which is not trivial. Consequently, other algorithms based on continuous dynamics have been proposed, such as the over-damped Langevin diffusion:
%\begin{equation}\label{eq:langevin_diffusion}
%dX_t= -\nabla \log \mu (X_t)dt+\sqrt{2}dB_t
%\end{equation}
%where $(B_t)_{t\ge0}$ is a $d$-dimensional Brownian motion. The Langevin Monte-Carlo (LMC) algorithm, or Unadjusted Langevin algorithm (ULA) considers the Markov chain $(X_k)_{k\ge1 }$ given by the Euler-Maruyama discretization of the diffusion \eqref{eq:langevin_diffusion}:
%\begin{equation}\label{eq:langevin_algorithm}
%X_{k+1} = X_k - \gamma_{k+1}\nabla \log \mu(X_k) + \sqrt{2\gamma_{k+1}G_{k+1}}
%\end{equation}
%where $(\gamma_k)_{k\ge1}$ is a sequence of step sizes (constant or convergent to zero), and
%$(G_k)_{k \ge 1}$ is a sequence of i.i.d. standard $d$-dimensional Gaussian random variables. This algorithm has attracted a lot of attention... But....\aknote{say something about the requirement of the knowledge of gradient of log target and how it is difficult to estimate?}

%Neural networks with a large number of parameters Theoretical explanation of 
%gradient descent is a solution of a PDE that converges to a globally optimal solution for networks with a single hidden layer under appropriate assumptions. \cite{rotskoff2019global} propose a Birth-Death dynamics that leads to a modified PDE with the same
%minimizer.


%Recently, using mathematical tools from optimal transport theory and interacting particle systems, it was shown that gradient descent [RVE18, MMN18, SS18, CB18b] and stochastic gradient descent converge asymptotically to the target function in the large data limit


Optimal transport   theory provides a powerful conceptual and mathematical framework for gradient flows on the space of distributions, and has thus found numerous applications in statistics and machine learning. A seminal work is surely the one of \cite{jordan1998variational}, who revealed that the Fokker-Planck equation is a gradient
flow equation for the relative entropy functional (also known as the KL-divergence) with respect to the Wassertein metric. %Under appropriate conditions on the coefficient of the equation, it can be shown that its stationary distribution is unique and is the target distribution $\pi$ (see for instance \cite{pavliotis2011stochastic}, Chapter 4). 
This led to the development of algorithms based on Langevin diffusions, where the goal is to build a diffusion process who admits some target distribution $\mu$ as its invariant measure. In particular, the Unadjusted
Langevin Algorithm (ULA) and its Metropolis adjusted counterpart MALA have received much
attention \cite{durmus2018analysis}). This consideration has given rise recently to a range of sampling
algorithms based on the theory of gradient flows (see \cite{liu2017stein, csimcsekli2018sliced, bernton2018langevin, mroueh2018regularized}). %, taking the form of McKean-Vlasov ODEs or SDEs . On the other hand, gradient flows and interacting particle systems were also used recently to analyse the convergence of gradient descent algorithms for neural networks as the number of parameters grows \cite{chizat2018global,rotskoff2018neural,mei2018mean, sirignano2018mean, rotskoff2019global}. Indeed, during the optimization process, the parameters of the network can be seen as interacting particles whose dynamics can be described by a partial differential equation (PDE) in the population limit.% This corresponds to a Wasserstein gradient flow of some generally non-convex energy functional.
In some cases, these energy functionals are closely related to the Maximum Mean Discrepancy (MMD) introduced in \cite{gretton2012kernel}. Unfortunately, such a functional is non-convex in the Wasserstein-2 space. This implies, in particular, that the flow could converge to a local solution. It was shown in \cite{chizat2018global,rotskoff2019global} that gradient descent would still converge asymptotically to a global solution in the large data limit but they rely on %appropriate assumptions on the functional
restrictive assumptions on the kernel or modified dynamics. 

In this paper, we investigate the gradient flow for the Maximum Mean Discrepancy. In particular, we underline the intrisic limits of the MMD flow regarding convergence and propose a regularized flow. The latter suggests a practical algorithm that can be used for optimizing neural networks, which simply consists in injecting noise to the particles before performing the gradient updates. We show theoretically and through experiments that the resulting noisy algorithm converges to the global optimum of the Maximum Mean Discrepancy unlike the original Wasserstein-2 flow, and provide rates of convergence. 
%\asnote{We will probably reviewed by Mroueh so we need to compare their results to ours}

This paper is organized as follows.  \cref{sec:gradient_flow} is devoted to deriving the MMD flow and motivate its study.
\cref{sec:convergence_mmd_flow} investigates the convergence towards a global optimum of the MMD flow. Finally, \cref{sec:discretized_flow} proposes a new algorithm and provides guarantees for the discretized flow (in time and space) of converging towards the continuous flow. 