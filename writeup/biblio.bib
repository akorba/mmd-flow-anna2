%% This BibTeX bibliography file was created using BibDesk.
%% http://bibdesk.sourceforge.net/

%% Created for Michael N. Arbel at 2019-05-21 20:47:48 +0100 


%% Saved with string encoding Unicode (UTF-8) 

@article{oguntuase2001inequality,
	title={On an inequality of Gronwall},
	author={Oguntuase, James Adedayo},
	journal={J. Ineq. Pure and Appl. Math},
	volume={2},
	number={1},
	year={2001}
}

@article{carrillo2006contractions,
	title={Contractions in the 2-Wasserstein length space and thermalization of granular media},
	author={Carrillo, Jos{\'e} A and McCann, Robert J and Villani, C{\'e}dric},
	journal={Archive for Rational Mechanics and Analysis},
	volume={179},
	number={2},
	pages={217--263},
	year={2006},
	publisher={Springer}
}

@article{benamou2000computational,
	title={A computational fluid mechanics solution to the Monge-Kantorovich mass transfer problem},
	author={Benamou, Jean-David and Brenier, Yann},
	journal={Numerische Mathematik},
	volume={84},
	number={3},
	pages={375--393},
	year={2000},
	publisher={Springer}
}


@article{Li:2017a,
	Abstract = {Generative moment matching network (GMMN) is a deep generative model that differs from Generative Adversarial Network (GAN) by replacing the discriminator in GAN with a two-sample test based on kernel maximum mean discrepancy (MMD). Although some theoretical guarantees of MMD have been studied, the empirical performance of GMMN is still not as competitive as that of GAN on challenging and large benchmark datasets. The computational efficiency of GMMN is also less desirable in comparison with GAN, partially due to its requirement for a rather large batch size during the training. In this paper, we propose to improve both the model expressiveness of GMMN and its computational efficiency by introducing adversarial kernel learning techniques, as the replacement of a fixed Gaussian kernel in the original GMMN. The new approach combines the key ideas in both GMMN and GAN, hence we name it MMD GAN. The new distance measure in MMD GAN is a meaningful loss that enjoys the advantage of weak topology and can be optimized via gradient descent with relatively small batch sizes. In our evaluation on multiple benchmark datasets, including MNIST, CIFAR- 10, CelebA and LSUN, the performance of MMD-GAN significantly outperforms GMMN, and is competitive with other representative GAN works.},
	Annote = {Comment: In the Proceedings of Thirty-first Annual Conference on Neural Information Processing Systems (NIPS 2017)},
	Author = {Li, Chun-Liang and Chang, Wei-Cheng and Cheng, Yu and Yang, Yiming and P{\'o}czos, Barnab{\'a}s},
	Date-Added = {2019-05-21 16:17:09 +0000},
	Date-Modified = {2019-05-21 16:17:09 +0000},
	Journal = {arXiv:1705.08584 [cs, stat]},
	Keywords = {Statistics - Machine Learning, Computer Science - Artificial Intelligence, Computer Science - Machine Learning},
	Month = may,
	Note = {arXiv: 1705.08584},
	Shorttitle = {{MMD} {GAN}},
	Title = {{MMD} {GAN}: {Towards} {Deeper} {Understanding} of {Moment} {Matching} {Network}},
	Url = {http://arxiv.org/abs/1705.08584},
	Urldate = {2018-11-13},
	Year = {2017},
	Bdsk-File-1 = {YnBsaXN0MDDUAQIDBAUGJCVYJHZlcnNpb25YJG9iamVjdHNZJGFyY2hpdmVyVCR0b3ASAAGGoKgHCBMUFRYaIVUkbnVsbNMJCgsMDxJXTlMua2V5c1pOUy5vYmplY3RzViRjbGFzc6INDoACgAOiEBGABIAFgAdccmVsYXRpdmVQYXRoWWFsaWFzRGF0YV8QMS4uLy4uLy4uL2xpdHRlcmF0dXJlL0xpKDIwMTdhKU1NRCBHQU4gVG93YXJkcy5wZGbSFwsYGVdOUy5kYXRhTxECDAAAAAACDAACAAAMTWFjaW50b3NoIEhEAAAAAAAAAAAAAAAAAAAAzDEhAkgrAAAFVlu0HExpKDIwMTdhKU1NRCBHQU4gVG93YXJkcy5wZGYAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAhOJ4rYtwdqAAAAAAAAAAAAAwACAAAJIAAAAAAAAAAAAAAAAAAAAAtsaXR0ZXJhdHVyZQAAEAAIAADMMRLyAAAAEQAIAADYtwdqAAAAAQAYBVZbtAVWW34FVlscAzpZgAM6WX8AAg35AAIAaE1hY2ludG9zaCBIRDpVc2VyczoATWljaGFlbEFyYmVsOgBEb2N1bWVudHM6AEdhdHNieToAUmVzZWFyY2g6AGxpdHRlcmF0dXJlOgBMaSgyMDE3YSlNTUQgR0FOIFRvd2FyZHMucGRmAA4AOgAcAEwAaQAoADIAMAAxADcAYQApAE0ATQBEACAARwBBAE4AIABUAG8AdwBhAHIAZABzAC4AcABkAGYADwAaAAwATQBhAGMAaQBuAHQAbwBzAGgAIABIAEQAEgBVVXNlcnMvTWljaGFlbEFyYmVsL0RvY3VtZW50cy9HYXRzYnkvUmVzZWFyY2gvbGl0dGVyYXR1cmUvTGkoMjAxN2EpTU1EIEdBTiBUb3dhcmRzLnBkZgAAEwABLwAAFQACABP//wAAgAbSGxwdHlokY2xhc3NuYW1lWCRjbGFzc2VzXU5TTXV0YWJsZURhdGGjHR8gVk5TRGF0YVhOU09iamVjdNIbHCIjXE5TRGljdGlvbmFyeaIiIF8QD05TS2V5ZWRBcmNoaXZlctEmJ1Ryb290gAEACAARABoAIwAtADIANwBAAEYATQBVAGAAZwBqAGwAbgBxAHMAdQB3AIQAjgDCAMcAzwLfAuEC5gLxAvoDCAMMAxMDHAMhAy4DMQNDA0YDSwAAAAAAAAIBAAAAAAAAACgAAAAAAAAAAAAAAAAAAANN},
	Bdsk-File-2 = {YnBsaXN0MDDUAQIDBAUGJCVYJHZlcnNpb25YJG9iamVjdHNZJGFyY2hpdmVyVCR0b3ASAAGGoKgHCBMUFRYaIVUkbnVsbNMJCgsMDxJXTlMua2V5c1pOUy5vYmplY3RzViRjbGFzc6INDoACgAOiEBGABIAFgAdccmVsYXRpdmVQYXRoWWFsaWFzRGF0YV8QMS4uLy4uLy4uL2xpdHRlcmF0dXJlL0xpKDIwMTdiKU1NRCBHQU4gVG93YXJkcy5wZGbSFwsYGVdOUy5kYXRhTxECDAAAAAACDAACAAAMTWFjaW50b3NoIEhEAAAAAAAAAAAAAAAAAAAAzDEhAkgrAAAFVlu0HExpKDIwMTdiKU1NRCBHQU4gVG93YXJkcy5wZGYAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAf1jenYMxNNAAAAAAAAAAAAAwACAAAJIAAAAAAAAAAAAAAAAAAAAAtsaXR0ZXJhdHVyZQAAEAAIAADMMRLyAAAAEQAIAADYMxNNAAAAAQAYBVZbtAVWW34FVlscAzpZgAM6WX8AAg35AAIAaE1hY2ludG9zaCBIRDpVc2VyczoATWljaGFlbEFyYmVsOgBEb2N1bWVudHM6AEdhdHNieToAUmVzZWFyY2g6AGxpdHRlcmF0dXJlOgBMaSgyMDE3YilNTUQgR0FOIFRvd2FyZHMucGRmAA4AOgAcAEwAaQAoADIAMAAxADcAYgApAE0ATQBEACAARwBBAE4AIABUAG8AdwBhAHIAZABzAC4AcABkAGYADwAaAAwATQBhAGMAaQBuAHQAbwBzAGgAIABIAEQAEgBVVXNlcnMvTWljaGFlbEFyYmVsL0RvY3VtZW50cy9HYXRzYnkvUmVzZWFyY2gvbGl0dGVyYXR1cmUvTGkoMjAxN2IpTU1EIEdBTiBUb3dhcmRzLnBkZgAAEwABLwAAFQACABP//wAAgAbSGxwdHlokY2xhc3NuYW1lWCRjbGFzc2VzXU5TTXV0YWJsZURhdGGjHR8gVk5TRGF0YVhOU09iamVjdNIbHCIjXE5TRGljdGlvbmFyeaIiIF8QD05TS2V5ZWRBcmNoaXZlctEmJ1Ryb290gAEACAARABoAIwAtADIANwBAAEYATQBVAGAAZwBqAGwAbgBxAHMAdQB3AIQAjgDCAMcAzwLfAuEC5gLxAvoDCAMMAxMDHAMhAy4DMQNDA0YDSwAAAAAAAAIBAAAAAAAAACgAAAAAAAAAAAAAAAAAAANN},
	Bdsk-Url-1 = {http://arxiv.org/abs/1705.08584}}

@article{Bolte:2016,
	Abstract = {For displacement convex functionals in the probability space equip{\textbackslash}-ped with the Monge-Kantorovich metric we prove the equivalence between the gradient and functional type {\textbackslash}L oja{\textbackslash}-sie{\textbackslash}-wicz inequalities. {\textbackslash}chg\{\vphantom{\}}We also discuss the more general case of \${\textbackslash}lambda\$-convex functions and we provide a general convergence theorem for the corresponding gradient dynamics. Specialising our results to the Boltzmann entropy, we recover Otto-Villani's theorem asserting the equivalence between logarithmic Sobolev and Talagrand's inequalities. The choice of power-type entropies shows a new equivalence between Gagliardo-Nirenberg inequality and a nonlinear Talagrand inequality. Some nonconvex results and other types of equivalences are discussed.},
	Author = {Bolte, J{\'e}r{\^o}me and Blanchet, Adrien},
	Date-Added = {2019-05-19 17:04:20 +0000},
	Date-Modified = {2019-05-19 17:04:20 +0000},
	Journal = {arXiv:1612.02619 [math]},
	Keywords = {Mathematics - Analysis of PDEs},
	Month = dec,
	Note = {arXiv: 1612.02619},
	Shorttitle = {A family of functional inequalities},
	Title = {A family of functional inequalities: {Lojasiewicz} inequalities and displacement convex functions},
	Url = {http://arxiv.org/abs/1612.02619},
	Urldate = {2019-04-24},
	Year = {2016},
	Bdsk-File-1 = {YnBsaXN0MDDUAQIDBAUGJCVYJHZlcnNpb25YJG9iamVjdHNZJGFyY2hpdmVyVCR0b3ASAAGGoKgHCBMUFRYaIVUkbnVsbNMJCgsMDxJXTlMua2V5c1pOUy5vYmplY3RzViRjbGFzc6INDoACgAOiEBGABIAFgAdccmVsYXRpdmVQYXRoWWFsaWFzRGF0YV8QLS4uLy4uLy4uL2xpdHRlcmF0dXJlL0JvbHRlKDIwMTZhKUEgZmFtaWx5LnBkZtIXCxgZV05TLmRhdGFPEQH8AAAAAAH8AAIAAAxNYWNpbnRvc2ggSEQAAAAAAAAAAAAAAAAAAADMMSECSCsAAAVWW7QYQm9sdGUoMjAxNmEpQSBmYW1pbHkucGRmAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAACGmZN9jmMQoAAAAAAAAAAAADAAIAAAkgAAAAAAAAAAAAAAAAAAAAC2xpdHRlcmF0dXJlAAAQAAgAAMwxEvIAAAARAAgAANjmIvoAAAABABgFVlu0BVZbfgVWWxwDOlmAAzpZfwACDfkAAgBkTWFjaW50b3NoIEhEOlVzZXJzOgBNaWNoYWVsQXJiZWw6AERvY3VtZW50czoAR2F0c2J5OgBSZXNlYXJjaDoAbGl0dGVyYXR1cmU6AEJvbHRlKDIwMTZhKUEgZmFtaWx5LnBkZgAOADIAGABCAG8AbAB0AGUAKAAyADAAMQA2AGEAKQBBACAAZgBhAG0AaQBsAHkALgBwAGQAZgAPABoADABNAGEAYwBpAG4AdABvAHMAaAAgAEgARAASAFFVc2Vycy9NaWNoYWVsQXJiZWwvRG9jdW1lbnRzL0dhdHNieS9SZXNlYXJjaC9saXR0ZXJhdHVyZS9Cb2x0ZSgyMDE2YSlBIGZhbWlseS5wZGYAABMAAS8AABUAAgAT//8AAIAG0hscHR5aJGNsYXNzbmFtZVgkY2xhc3Nlc11OU011dGFibGVEYXRhox0fIFZOU0RhdGFYTlNPYmplY3TSGxwiI1xOU0RpY3Rpb25hcnmiIiBfEA9OU0tleWVkQXJjaGl2ZXLRJidUcm9vdIABAAgAEQAaACMALQAyADcAQABGAE0AVQBgAGcAagBsAG4AcQBzAHUAdwCEAI4AvgDDAMsCywLNAtIC3QLmAvQC+AL/AwgDDQMaAx0DLwMyAzcAAAAAAAACAQAAAAAAAAAoAAAAAAAAAAAAAAAAAAADOQ==}}

@book{Steinwart:2008a,
	Author = {Steinwart, Ingo and Christmann, Andreas},
	Date-Added = {2019-05-19 11:35:35 +0000},
	Date-Modified = {2019-05-19 11:35:35 +0000},
	Edition = {1st},
	Isbn = {0387772413},
	Publisher = {Springer Publishing Company, Incorporated},
	Title = {Support Vector Machines},
	Year = {2008},
	Bdsk-File-1 = {YnBsaXN0MDDUAQIDBAUGJCVYJHZlcnNpb25YJG9iamVjdHNZJGFyY2hpdmVyVCR0b3ASAAGGoKgHCBMUFRYaIVUkbnVsbNMJCgsMDxJXTlMua2V5c1pOUy5vYmplY3RzViRjbGFzc6INDoACgAOiEBGABIAFgAdccmVsYXRpdmVQYXRoWWFsaWFzRGF0YV8QMC4uLy4uLy4uL2xpdHRlcmF0dXJlL1N0ZWlud2FydCgyMDA4YSlTdXBwb3J0LnBkZtIXCxgZV05TLmRhdGFPEQIIAAAAAAIIAAIAAAxNYWNpbnRvc2ggSEQAAAAAAAAAAAAAAAAAAADMMSECSCsAAAVWW7QbU3RlaW53YXJ0KDIwMDhhKVN1cHBvcnQucGRmAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAABoOeM9X4AG0AAAAAAAAAAAADAAIAAAkgAAAAAAAAAAAAAAAAAAAAC2xpdHRlcmF0dXJlAAAQAAgAAMwxEvIAAAARAAgAANX38l0AAAABABgFVlu0BVZbfgVWWxwDOlmAAzpZfwACDfkAAgBnTWFjaW50b3NoIEhEOlVzZXJzOgBNaWNoYWVsQXJiZWw6AERvY3VtZW50czoAR2F0c2J5OgBSZXNlYXJjaDoAbGl0dGVyYXR1cmU6AFN0ZWlud2FydCgyMDA4YSlTdXBwb3J0LnBkZgAADgA4ABsAUwB0AGUAaQBuAHcAYQByAHQAKAAyADAAMAA4AGEAKQBTAHUAcABwAG8AcgB0AC4AcABkAGYADwAaAAwATQBhAGMAaQBuAHQAbwBzAGgAIABIAEQAEgBUVXNlcnMvTWljaGFlbEFyYmVsL0RvY3VtZW50cy9HYXRzYnkvUmVzZWFyY2gvbGl0dGVyYXR1cmUvU3RlaW53YXJ0KDIwMDhhKVN1cHBvcnQucGRmABMAAS8AABUAAgAT//8AAIAG0hscHR5aJGNsYXNzbmFtZVgkY2xhc3Nlc11OU011dGFibGVEYXRhox0fIFZOU0RhdGFYTlNPYmplY3TSGxwiI1xOU0RpY3Rpb25hcnmiIiBfEA9OU0tleWVkQXJjaGl2ZXLRJidUcm9vdIABAAgAEQAaACMALQAyADcAQABGAE0AVQBgAGcAagBsAG4AcQBzAHUAdwCEAI4AwQDGAM4C2gLcAuEC7AL1AwMDBwMOAxcDHAMpAywDPgNBA0YAAAAAAAACAQAAAAAAAAAoAAAAAAAAAAAAAAAAAAADSA==}}

@inproceedings{Mroueh:2019,
	Abstract = {We study a simplification of GAN training: the problem of transporting particles from a source to a target distribution. Starting from the Sobolev GAN critic, part of the gradient  regularized GAN ...},
	Author = {Mroueh, Youssef and Sercu, Tom and Raj, Anant},
	Booktitle = {The 22nd {International} {Conference} on {Artificial} {Intelligence} and {Statistics}},
	Date-Added = {2019-05-18 12:40:05 +0000},
	Date-Modified = {2019-05-18 12:40:05 +0000},
	Language = {en},
	Month = apr,
	Pages = {2976--2985},
	Title = {Sobolev {Descent}},
	Url = {http://proceedings.mlr.press/v89/mroueh19a.html},
	Urldate = {2019-05-16},
	Year = {2019},
	Bdsk-File-1 = {YnBsaXN0MDDUAQIDBAUGJCVYJHZlcnNpb25YJG9iamVjdHNZJGFyY2hpdmVyVCR0b3ASAAGGoKgHCBMUFRYaIVUkbnVsbNMJCgsMDxJXTlMua2V5c1pOUy5vYmplY3RzViRjbGFzc6INDoACgAOiEBGABIAFgAdccmVsYXRpdmVQYXRoWWFsaWFzRGF0YV8QLS4uLy4uLy4uL2xpdHRlcmF0dXJlL01yb3VlaCgyMDE5YSlTb2JvbGV2LnBkZtIXCxgZV05TLmRhdGFPEQH8AAAAAAH8AAIAAAxNYWNpbnRvc2ggSEQAAAAAAAAAAAAAAAAAAADMMSECSCsAAAVWW7QYTXJvdWVoKDIwMTlhKVNvYm9sZXYucGRmAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAACIH2aNkDTbMAAAAAAAAAAAADAAIAAAkgAAAAAAAAAAAAAAAAAAAAC2xpdHRlcmF0dXJlAAAQAAgAAMwxEvIAAAARAAgAANkDP6MAAAABABgFVlu0BVZbfgVWWxwDOlmAAzpZfwACDfkAAgBkTWFjaW50b3NoIEhEOlVzZXJzOgBNaWNoYWVsQXJiZWw6AERvY3VtZW50czoAR2F0c2J5OgBSZXNlYXJjaDoAbGl0dGVyYXR1cmU6AE1yb3VlaCgyMDE5YSlTb2JvbGV2LnBkZgAOADIAGABNAHIAbwB1AGUAaAAoADIAMAAxADkAYQApAFMAbwBiAG8AbABlAHYALgBwAGQAZgAPABoADABNAGEAYwBpAG4AdABvAHMAaAAgAEgARAASAFFVc2Vycy9NaWNoYWVsQXJiZWwvRG9jdW1lbnRzL0dhdHNieS9SZXNlYXJjaC9saXR0ZXJhdHVyZS9Ncm91ZWgoMjAxOWEpU29ib2xldi5wZGYAABMAAS8AABUAAgAT//8AAIAG0hscHR5aJGNsYXNzbmFtZVgkY2xhc3Nlc11OU011dGFibGVEYXRhox0fIFZOU0RhdGFYTlNPYmplY3TSGxwiI1xOU0RpY3Rpb25hcnmiIiBfEA9OU0tleWVkQXJjaGl2ZXLRJidUcm9vdIABAAgAEQAaACMALQAyADcAQABGAE0AVQBgAGcAagBsAG4AcQBzAHUAdwCEAI4AvgDDAMsCywLNAtIC3QLmAvQC+AL/AwgDDQMaAx0DLwMyAzcAAAAAAAACAQAAAAAAAAAoAAAAAAAAAAAAAAAAAAADOQ==}}

@article{Peyre:2011,
	Abstract = {It is well known that the quadratic Wasserstein distance \$W\_2 ({\textbackslash}mathord\{{\textbackslash}boldsymbol\{{\textbackslash}cdot\}\}, {\textbackslash}mathord\{{\textbackslash}boldsymbol\{{\textbackslash}cdot\}\})\$ is formally equivalent, for infinitesimally small perturbations, to some weighted \$H{\textasciicircum}\{-1\}\$ homogeneous Sobolev norm. In this article I show that this equivalence can be integrated to get non-asymptotic comparison results between these distances. Then I give an application of these results to prove that the \$W\_2\$ distance exhibits some localisation phenomenon: if \${\textbackslash}mu\$ and \${\textbackslash}nu\$ are measures on \${\textbackslash}mathbf\{R\}{\textasciicircum}n\$ and \${\textbackslash}varphi {\textbackslash}colon {\textbackslash}mathbf\{R\}{\textasciicircum}n {\textbackslash}to {\textbackslash}mathbf\{R\}\_+\$ is some bump function with compact support, then under mild hypotheses, you can bound above the Wasserstein distance between \${\textbackslash}varphi {\textbackslash}cdot {\textbackslash}mu\$ and \${\textbackslash}varphi {\textbackslash}cdot {\textbackslash}nu\$ by an explicit multiple of \$W\_2 ({\textbackslash}mu, {\textbackslash}nu)\$.},
	Annote = {Comment: Added a new section about application to the localisation of Wasserstein distance},
	Author = {Peyre, R{\'e}mi},
	Date-Added = {2019-05-18 12:09:19 +0000},
	Date-Modified = {2019-05-18 12:09:19 +0000},
	Journal = {arXiv:1104.4631 [math]},
	Keywords = {28A75, 46E35, Mathematics - Functional Analysis},
	Month = apr,
	Note = {arXiv: 1104.4631},
	Title = {Comparison between \${W}\_2\$ distance and \${\textbackslash}dot\{{H}\}{\textasciicircum}\{-1\}\$ norm, and localisation of {Wasserstein} distance},
	Url = {http://arxiv.org/abs/1104.4631},
	Urldate = {2019-04-30},
	Year = {2011},
	Bdsk-File-1 = {YnBsaXN0MDDUAQIDBAUGJCVYJHZlcnNpb25YJG9iamVjdHNZJGFyY2hpdmVyVCR0b3ASAAGGoKgHCBMUFRYaIVUkbnVsbNMJCgsMDxJXTlMua2V5c1pOUy5vYmplY3RzViRjbGFzc6INDoACgAOiEBGABIAFgAdccmVsYXRpdmVQYXRoWWFsaWFzRGF0YV8QLy4uLy4uLy4uL2xpdHRlcmF0dXJlL1BleXJlKDIwMTFhKUNvbXBhcmlzb24ucGRm0hcLGBlXTlMuZGF0YU8RAgQAAAAAAgQAAgAADE1hY2ludG9zaCBIRAAAAAAAAAAAAAAAAAAAAMwxIQJIKwAABVZbtBpQZXlyZSgyMDExYSlDb21wYXJpc29uLnBkZgAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAIcQlB2O1frAAAAAAAAAAAAAMAAgAACSAAAAAAAAAAAAAAAAAAAAALbGl0dGVyYXR1cmUAABAACAAAzDES8gAAABEACAAA2O1RnAAAAAEAGAVWW7QFVlt+BVZbHAM6WYADOll/AAIN+QACAGZNYWNpbnRvc2ggSEQ6VXNlcnM6AE1pY2hhZWxBcmJlbDoARG9jdW1lbnRzOgBHYXRzYnk6AFJlc2VhcmNoOgBsaXR0ZXJhdHVyZToAUGV5cmUoMjAxMWEpQ29tcGFyaXNvbi5wZGYADgA2ABoAUABlAHkAcgBlACgAMgAwADEAMQBhACkAQwBvAG0AcABhAHIAaQBzAG8AbgAuAHAAZABmAA8AGgAMAE0AYQBjAGkAbgB0AG8AcwBoACAASABEABIAU1VzZXJzL01pY2hhZWxBcmJlbC9Eb2N1bWVudHMvR2F0c2J5L1Jlc2VhcmNoL2xpdHRlcmF0dXJlL1BleXJlKDIwMTFhKUNvbXBhcmlzb24ucGRmAAATAAEvAAAVAAIAE///AACABtIbHB0eWiRjbGFzc25hbWVYJGNsYXNzZXNdTlNNdXRhYmxlRGF0YaMdHyBWTlNEYXRhWE5TT2JqZWN00hscIiNcTlNEaWN0aW9uYXJ5oiIgXxAPTlNLZXllZEFyY2hpdmVy0SYnVHJvb3SAAQAIABEAGgAjAC0AMgA3AEAARgBNAFUAYABnAGoAbABuAHEAcwB1AHcAhACOAMAAxQDNAtUC1wLcAucC8AL+AwIDCQMSAxcDJAMnAzkDPANBAAAAAAAAAgEAAAAAAAAAKAAAAAAAAAAAAAAAAAAAA0M=}}

@article{Otto:2000,
	Abstract = {We show that transport inequalities, similar to the one derived by Talagrand [30] for the Gaussian measure, are implied by logarithmic Sobolev inequalities. Conversely, Talagrand's inequality implies a logarithmic Sobolev inequality if the density of the measure is approximately log-concave, in a precise sense. All constants are independent of the dimension, and optimal in certain cases. The proofs are based on partial differential equations, and an interpolation inequality involving the Wasserstein distance, the entropy functional and the Fisher information.},
	Author = {Otto, F. and Villani, C.},
	Date-Added = {2019-05-18 12:07:52 +0000},
	Date-Modified = {2019-05-18 12:07:52 +0000},
	Doi = {10.1006/jfan.1999.3557},
	Issn = {00221236},
	Journal = {Journal of Functional Analysis},
	Language = {en},
	Month = jun,
	Number = {2},
	Pages = {361--400},
	Title = {Generalization of an {Inequality} by {Talagrand} and {Links} with the {Logarithmic} {Sobolev} {Inequality}},
	Url = {http://linkinghub.elsevier.com/retrieve/pii/S0022123699935577},
	Urldate = {2018-05-11},
	Volume = {173},
	Year = {2000},
	Bdsk-File-1 = {YnBsaXN0MDDUAQIDBAUGJCVYJHZlcnNpb25YJG9iamVjdHNZJGFyY2hpdmVyVCR0b3ASAAGGoKgHCBMUFRYaIVUkbnVsbNMJCgsMDxJXTlMua2V5c1pOUy5vYmplY3RzViRjbGFzc6INDoACgAOiEBGABIAFgAdccmVsYXRpdmVQYXRoWWFsaWFzRGF0YV8QMi4uLy4uLy4uL2xpdHRlcmF0dXJlL090dG8oMjAwMGEpR2VuZXJhbGl6YXRpb24ucGRm0hcLGBlXTlMuZGF0YU8RAhAAAAAAAhAAAgAADE1hY2ludG9zaCBIRAAAAAAAAAAAAAAAAAAAAMwxIQJIKwAABVZbtB1PdHRvKDIwMDBhKUdlbmVyYWxpemF0aW9uLnBkZgAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAHRprB1xtMVgAAAAAAAAAAAAMAAgAACSAAAAAAAAAAAAAAAAAAAAALbGl0dGVyYXR1cmUAABAACAAAzDES8gAAABEACAAA1xs+RgAAAAEAGAVWW7QFVlt+BVZbHAM6WYADOll/AAIN+QACAGlNYWNpbnRvc2ggSEQ6VXNlcnM6AE1pY2hhZWxBcmJlbDoARG9jdW1lbnRzOgBHYXRzYnk6AFJlc2VhcmNoOgBsaXR0ZXJhdHVyZToAT3R0bygyMDAwYSlHZW5lcmFsaXphdGlvbi5wZGYAAA4APAAdAE8AdAB0AG8AKAAyADAAMAAwAGEAKQBHAGUAbgBlAHIAYQBsAGkAegBhAHQAaQBvAG4ALgBwAGQAZgAPABoADABNAGEAYwBpAG4AdABvAHMAaAAgAEgARAASAFZVc2Vycy9NaWNoYWVsQXJiZWwvRG9jdW1lbnRzL0dhdHNieS9SZXNlYXJjaC9saXR0ZXJhdHVyZS9PdHRvKDIwMDBhKUdlbmVyYWxpemF0aW9uLnBkZgATAAEvAAAVAAIAE///AACABtIbHB0eWiRjbGFzc25hbWVYJGNsYXNzZXNdTlNNdXRhYmxlRGF0YaMdHyBWTlNEYXRhWE5TT2JqZWN00hscIiNcTlNEaWN0aW9uYXJ5oiIgXxAPTlNLZXllZEFyY2hpdmVy0SYnVHJvb3SAAQAIABEAGgAjAC0AMgA3AEAARgBNAFUAYABnAGoAbABuAHEAcwB1AHcAhACOAMMAyADQAuQC5gLrAvYC/wMNAxEDGAMhAyYDMwM2A0gDSwNQAAAAAAAAAgEAAAAAAAAAKAAAAAAAAAAAAAAAAAAAA1I=},
	Bdsk-Url-1 = {http://linkinghub.elsevier.com/retrieve/pii/S0022123699935577},
	Bdsk-Url-2 = {https://doi.org/10.1006/jfan.1999.3557}}

@article{carrillo2019blob,
	Author = {Carrillo, Jos{\'e} Antonio and Craig, Katy and Patacchini, Francesco S},
	Date-Added = {2019-05-17 17:13:25 +0000},
	Date-Modified = {2019-05-17 17:13:25 +0000},
	Journal = {Calculus of Variations and Partial Differential Equations},
	Number = {2},
	Pages = {53},
	Publisher = {Springer},
	Title = {A blob method for diffusion},
	Volume = {58},
	Year = {2019}}

@article{craig2016blob,
	Author = {Craig, Katy and Bertozzi, Andrea},
	Date-Added = {2019-05-17 17:13:11 +0000},
	Date-Modified = {2019-05-17 17:13:11 +0000},
	Journal = {Mathematics of computation},
	Number = {300},
	Pages = {1681--1717},
	Title = {A blob method for the aggregation equation},
	Volume = {85},
	Year = {2016}}

@article{Chaudhari:2017,
	Abstract = {In this paper we establish a connection between non-convex optimization methods for training deep neural networks and nonlinear partial differential equations (PDEs). Relaxation techniques arising in statistical physics which have already been used successfully in this context are reinterpreted as solutions of a viscous Hamilton-Jacobi PDE. Using a stochastic control interpretation allows we prove that the modified algorithm performs better in expectation that stochastic gradient descent. Well-known PDE regularity results allow us to analyze the geometry of the relaxed energy landscape, confirming empirical evidence. The PDE is derived from a stochastic homogenization problem, which arises in the implementation of the algorithm. The algorithms scale well in practice and can effectively tackle the high dimensionality of modern neural networks.},
	Author = {Chaudhari, Pratik and Oberman, Adam and Osher, Stanley and Soatto, Stefano and Carlier, Guillaume},
	Date-Added = {2019-05-17 16:55:23 +0000},
	Date-Modified = {2019-05-17 16:55:23 +0000},
	Journal = {arXiv:1704.04932 [cs, math]},
	Keywords = {Computer Science - Machine Learning, Mathematics - Analysis of PDEs, Mathematics - Optimization and Control},
	Month = apr,
	Note = {arXiv: 1704.04932},
	Shorttitle = {Deep {Relaxation}},
	Title = {Deep {Relaxation}: partial differential equations for optimizing deep neural networks},
	Url = {http://arxiv.org/abs/1704.04932},
	Urldate = {2019-05-09},
	Year = {2017},
	Bdsk-File-1 = {YnBsaXN0MDDUAQIDBAUGJCVYJHZlcnNpb25YJG9iamVjdHNZJGFyY2hpdmVyVCR0b3ASAAGGoKgHCBMUFRYaIVUkbnVsbNMJCgsMDxJXTlMua2V5c1pOUy5vYmplY3RzViRjbGFzc6INDoACgAOiEBGABIAFgAdccmVsYXRpdmVQYXRoWWFsaWFzRGF0YV8QLS4uLy4uLy4uL2xpdHRlcmF0dXJlL0NoYXVkaGFyaSgyMDE3YSlEZWVwLnBkZtIXCxgZV05TLmRhdGFPEQH8AAAAAAH8AAIAAAxNYWNpbnRvc2ggSEQAAAAAAAAAAAAAAAAAAADMMSECSCsAAAVWW7QYQ2hhdWRoYXJpKDIwMTdhKURlZXAucGRmAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAACHpjQdj6BkgAAAAAAAAAAAADAAIAAAkgAAAAAAAAAAAAAAAAAAAAC2xpdHRlcmF0dXJlAAAQAAgAAMwxEvIAAAARAAgAANj5+DgAAAABABgFVlu0BVZbfgVWWxwDOlmAAzpZfwACDfkAAgBkTWFjaW50b3NoIEhEOlVzZXJzOgBNaWNoYWVsQXJiZWw6AERvY3VtZW50czoAR2F0c2J5OgBSZXNlYXJjaDoAbGl0dGVyYXR1cmU6AENoYXVkaGFyaSgyMDE3YSlEZWVwLnBkZgAOADIAGABDAGgAYQB1AGQAaABhAHIAaQAoADIAMAAxADcAYQApAEQAZQBlAHAALgBwAGQAZgAPABoADABNAGEAYwBpAG4AdABvAHMAaAAgAEgARAASAFFVc2Vycy9NaWNoYWVsQXJiZWwvRG9jdW1lbnRzL0dhdHNieS9SZXNlYXJjaC9saXR0ZXJhdHVyZS9DaGF1ZGhhcmkoMjAxN2EpRGVlcC5wZGYAABMAAS8AABUAAgAT//8AAIAG0hscHR5aJGNsYXNzbmFtZVgkY2xhc3Nlc11OU011dGFibGVEYXRhox0fIFZOU0RhdGFYTlNPYmplY3TSGxwiI1xOU0RpY3Rpb25hcnmiIiBfEA9OU0tleWVkQXJjaGl2ZXLRJidUcm9vdIABAAgAEQAaACMALQAyADcAQABGAE0AVQBgAGcAagBsAG4AcQBzAHUAdwCEAI4AvgDDAMsCywLNAtIC3QLmAvQC+AL/AwgDDQMaAx0DLwMyAzcAAAAAAAACAQAAAAAAAAAoAAAAAAAAAAAAAAAAAAADOQ==}}

@article{Hazan:2015,
	Abstract = {The graduated optimization approach, also known as the continuation method, is a popular heuristic to solving non-convex problems that has received renewed interest over the last decade. Despite its popularity, very little is known in terms of theoretical convergence analysis. In this paper we describe a new first-order algorithm based on graduated optimiza- tion and analyze its performance. We characterize a parameterized family of non- convex functions for which this algorithm provably converges to a global optimum. In particular, we prove that the algorithm converges to an \{{\textbackslash}epsilon\}-approximate solution within O(1/{\textbackslash}epsilon{\textasciicircum}2) gradient-based steps. We extend our algorithm and analysis to the setting of stochastic non-convex optimization with noisy gradient feedback, attaining the same convergence rate. Additionally, we discuss the setting of zero-order optimization, and devise a a variant of our algorithm which converges at rate of O(d{\textasciicircum}2/{\textbackslash}epsilon{\textasciicircum}4).},
	Annote = {Comment: 17 pages},
	Author = {Hazan, Elad and Levy, Kfir Y. and Shalev-Shwartz, Shai},
	Date-Added = {2019-05-17 16:54:02 +0000},
	Date-Modified = {2019-05-17 16:54:02 +0000},
	Journal = {arXiv:1503.03712 [cs, math]},
	Keywords = {68, Computer Science - Machine Learning, Mathematics - Optimization and Control},
	Month = mar,
	Note = {arXiv: 1503.03712},
	Title = {On {Graduated} {Optimization} for {Stochastic} {Non}-{Convex} {Problems}},
	Url = {http://arxiv.org/abs/1503.03712},
	Urldate = {2019-05-09},
	Year = {2015},
	Bdsk-File-1 = {YnBsaXN0MDDUAQIDBAUGJCVYJHZlcnNpb25YJG9iamVjdHNZJGFyY2hpdmVyVCR0b3ASAAGGoKgHCBMUFRYaIVUkbnVsbNMJCgsMDxJXTlMua2V5c1pOUy5vYmplY3RzViRjbGFzc6INDoACgAOiEBGABIAFgAdccmVsYXRpdmVQYXRoWWFsaWFzRGF0YV8QMS4uLy4uLy4uL2xpdHRlcmF0dXJlL0hhemFuKDIwMTVhKU9uIEdyYWR1YXRlZC5wZGbSFwsYGVdOUy5kYXRhTxECDAAAAAACDAACAAAMTWFjaW50b3NoIEhEAAAAAAAAAAAAAAAAAAAAzDEhAkgrAAAFVlu0HEhhemFuKDIwMTVhKU9uIEdyYWR1YXRlZC5wZGYAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAh6opTY+mmIAAAAAAAAAAAAAwACAAAJIAAAAAAAAAAAAAAAAAAAAAtsaXR0ZXJhdHVyZQAAEAAIAADMMRLyAAAAEQAIAADY+lt4AAAAAQAYBVZbtAVWW34FVlscAzpZgAM6WX8AAg35AAIAaE1hY2ludG9zaCBIRDpVc2VyczoATWljaGFlbEFyYmVsOgBEb2N1bWVudHM6AEdhdHNieToAUmVzZWFyY2g6AGxpdHRlcmF0dXJlOgBIYXphbigyMDE1YSlPbiBHcmFkdWF0ZWQucGRmAA4AOgAcAEgAYQB6AGEAbgAoADIAMAAxADUAYQApAE8AbgAgAEcAcgBhAGQAdQBhAHQAZQBkAC4AcABkAGYADwAaAAwATQBhAGMAaQBuAHQAbwBzAGgAIABIAEQAEgBVVXNlcnMvTWljaGFlbEFyYmVsL0RvY3VtZW50cy9HYXRzYnkvUmVzZWFyY2gvbGl0dGVyYXR1cmUvSGF6YW4oMjAxNWEpT24gR3JhZHVhdGVkLnBkZgAAEwABLwAAFQACABP//wAAgAbSGxwdHlokY2xhc3NuYW1lWCRjbGFzc2VzXU5TTXV0YWJsZURhdGGjHR8gVk5TRGF0YVhOU09iamVjdNIbHCIjXE5TRGljdGlvbmFyeaIiIF8QD05TS2V5ZWRBcmNoaXZlctEmJ1Ryb290gAEACAARABoAIwAtADIANwBAAEYATQBVAGAAZwBqAGwAbgBxAHMAdQB3AIQAjgDCAMcAzwLfAuEC5gLxAvoDCAMMAxMDHAMhAy4DMQNDA0YDSwAAAAAAAAIBAAAAAAAAACgAAAAAAAAAAAAAAAAAAANN}}

@article{Gulcehre:2016,
	Abstract = {The optimization of deep neural networks can be more challenging than traditional convex optimization problems due to the highly non-convex nature of the loss function, e.g. it can involve pathological landscapes such as saddle-surfaces that can be difficult to escape for algorithms based on simple gradient descent. In this paper, we attack the problem of optimization of highly non-convex neural networks by starting with a smoothed -- or {\textbackslash}textit\{mollified\} -- objective function that gradually has a more non-convex energy landscape during the training. Our proposition is inspired by the recent studies in continuation methods: similar to curriculum methods, we begin learning an easier (possibly convex) objective function and let it evolve during the training, until it eventually goes back to being the original, difficult to optimize, objective function. The complexity of the mollified networks is controlled by a single hyperparameter which is annealed during the training. We show improvements on various difficult optimization tasks and establish a relationship with recent works on continuation methods for neural networks and mollifiers.},
	Author = {Gulcehre, Caglar and Moczulski, Marcin and Visin, Francesco and Bengio, Yoshua},
	Date-Added = {2019-05-17 16:53:53 +0000},
	Date-Modified = {2019-05-17 16:53:53 +0000},
	Journal = {arXiv:1608.04980 [cs]},
	Keywords = {Computer Science - Machine Learning, Computer Science - Neural and Evolutionary Computing},
	Month = aug,
	Note = {arXiv: 1608.04980},
	Title = {Mollifying {Networks}},
	Url = {http://arxiv.org/abs/1608.04980},
	Urldate = {2019-05-09},
	Year = {2016},
	Bdsk-File-1 = {YnBsaXN0MDDUAQIDBAUGJCVYJHZlcnNpb25YJG9iamVjdHNZJGFyY2hpdmVyVCR0b3ASAAGGoKgHCBMUFRYaIVUkbnVsbNMJCgsMDxJXTlMua2V5c1pOUy5vYmplY3RzViRjbGFzc6INDoACgAOiEBGABIAFgAdccmVsYXRpdmVQYXRoWWFsaWFzRGF0YV8QMi4uLy4uLy4uL2xpdHRlcmF0dXJlL0d1bGNlaHJlKDIwMTZhKU1vbGxpZnlpbmcucGRm0hcLGBlXTlMuZGF0YU8RAhAAAAAAAhAAAgAADE1hY2ludG9zaCBIRAAAAAAAAAAAAAAAAAAAAMwxIQJIKwAABVZbtB1HdWxjZWhyZSgyMDE2YSlNb2xsaWZ5aW5nLnBkZgAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAIeqec2PpuSQAAAAAAAAAAAAMAAgAACSAAAAAAAAAAAAAAAAAAAAALbGl0dGVyYXR1cmUAABAACAAAzDES8gAAABEACAAA2PpgOQAAAAEAGAVWW7QFVlt+BVZbHAM6WYADOll/AAIN+QACAGlNYWNpbnRvc2ggSEQ6VXNlcnM6AE1pY2hhZWxBcmJlbDoARG9jdW1lbnRzOgBHYXRzYnk6AFJlc2VhcmNoOgBsaXR0ZXJhdHVyZToAR3VsY2VocmUoMjAxNmEpTW9sbGlmeWluZy5wZGYAAA4APAAdAEcAdQBsAGMAZQBoAHIAZQAoADIAMAAxADYAYQApAE0AbwBsAGwAaQBmAHkAaQBuAGcALgBwAGQAZgAPABoADABNAGEAYwBpAG4AdABvAHMAaAAgAEgARAASAFZVc2Vycy9NaWNoYWVsQXJiZWwvRG9jdW1lbnRzL0dhdHNieS9SZXNlYXJjaC9saXR0ZXJhdHVyZS9HdWxjZWhyZSgyMDE2YSlNb2xsaWZ5aW5nLnBkZgATAAEvAAAVAAIAE///AACABtIbHB0eWiRjbGFzc25hbWVYJGNsYXNzZXNdTlNNdXRhYmxlRGF0YaMdHyBWTlNEYXRhWE5TT2JqZWN00hscIiNcTlNEaWN0aW9uYXJ5oiIgXxAPTlNLZXllZEFyY2hpdmVy0SYnVHJvb3SAAQAIABEAGgAjAC0AMgA3AEAARgBNAFUAYABnAGoAbABuAHEAcwB1AHcAhACOAMMAyADQAuQC5gLrAvYC/wMNAxEDGAMhAyYDMwM2A0gDSwNQAAAAAAAAAgEAAAAAAAAAKAAAAAAAAAAAAAAAAAAAA1I=}}

@article{Mobahi:2016,
	Abstract = {This work presents a new algorithm for training recurrent neural networks (although ideas are applicable to feedforward networks as well). The algorithm is derived from a theory in nonconvex optimization related to the diffusion equation. The contributions made in this work are two fold. First, we show how some seemingly disconnected mechanisms used in deep learning such as smart initialization, annealed learning rate, layerwise pretraining, and noise injection (as done in dropout and SGD) arise naturally and automatically from this framework, without manually crafting them into the algorithms. Second, we present some preliminary results on comparing the proposed method against SGD. It turns out that the new algorithm can achieve similar level of generalization accuracy of SGD in much fewer number of epochs.},
	Author = {Mobahi, Hossein},
	Date-Added = {2019-05-17 16:53:44 +0000},
	Date-Modified = {2019-05-17 16:53:44 +0000},
	Journal = {arXiv:1601.04114 [cs]},
	Keywords = {Computer Science - Machine Learning},
	Month = jan,
	Note = {arXiv: 1601.04114},
	Title = {Training {Recurrent} {Neural} {Networks} by {Diffusion}},
	Url = {http://arxiv.org/abs/1601.04114},
	Urldate = {2019-05-17},
	Year = {2016},
	Bdsk-File-1 = {YnBsaXN0MDDUAQIDBAUGJCVYJHZlcnNpb25YJG9iamVjdHNZJGFyY2hpdmVyVCR0b3ASAAGGoKgHCBMUFRYaIVUkbnVsbNMJCgsMDxJXTlMua2V5c1pOUy5vYmplY3RzViRjbGFzc6INDoACgAOiEBGABIAFgAdccmVsYXRpdmVQYXRoWWFsaWFzRGF0YV8QLi4uLy4uLy4uL2xpdHRlcmF0dXJlL01vYmFoaSgyMDE2YSlUcmFpbmluZy5wZGbSFwsYGVdOUy5kYXRhTxECAAAAAAACAAACAAAMTWFjaW50b3NoIEhEAAAAAAAAAAAAAAAAAAAAzDEhAkgrAAAFVlu0GU1vYmFoaSgyMDE2YSlUcmFpbmluZy5wZGYAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAiC273ZBKRBAAAAAAAAAAAAAwACAAAJIAAAAAAAAAAAAAAAAAAAAAtsaXR0ZXJhdHVyZQAAEAAIAADMMRLyAAAAEQAIAADZBJYxAAAAAQAYBVZbtAVWW34FVlscAzpZgAM6WX8AAg35AAIAZU1hY2ludG9zaCBIRDpVc2VyczoATWljaGFlbEFyYmVsOgBEb2N1bWVudHM6AEdhdHNieToAUmVzZWFyY2g6AGxpdHRlcmF0dXJlOgBNb2JhaGkoMjAxNmEpVHJhaW5pbmcucGRmAAAOADQAGQBNAG8AYgBhAGgAaQAoADIAMAAxADYAYQApAFQAcgBhAGkAbgBpAG4AZwAuAHAAZABmAA8AGgAMAE0AYQBjAGkAbgB0AG8AcwBoACAASABEABIAUlVzZXJzL01pY2hhZWxBcmJlbC9Eb2N1bWVudHMvR2F0c2J5L1Jlc2VhcmNoL2xpdHRlcmF0dXJlL01vYmFoaSgyMDE2YSlUcmFpbmluZy5wZGYAEwABLwAAFQACABP//wAAgAbSGxwdHlokY2xhc3NuYW1lWCRjbGFzc2VzXU5TTXV0YWJsZURhdGGjHR8gVk5TRGF0YVhOU09iamVjdNIbHCIjXE5TRGljdGlvbmFyeaIiIF8QD05TS2V5ZWRBcmNoaXZlctEmJ1Ryb290gAEACAARABoAIwAtADIANwBAAEYATQBVAGAAZwBqAGwAbgBxAHMAdQB3AIQAjgC/AMQAzALQAtIC1wLiAusC+QL9AwQDDQMSAx8DIgM0AzcDPAAAAAAAAAIBAAAAAAAAACgAAAAAAAAAAAAAAAAAAAM+}}

@article{Gulcehre:2016a,
	Abstract = {Common nonlinear activation functions used in neural networks can cause training difficulties due to the saturation behavior of the activation function, which may hide dependencies that are not visible to vanilla-SGD (using first order gradients only). Gating mechanisms that use softly saturating activation functions to emulate the discrete switching of digital logic circuits are good examples of this. We propose to exploit the injection of appropriate noise so that the gradients may flow easily, even if the noiseless application of the activation function would yield zero gradient. Large noise will dominate the noise-free gradient and allow stochastic gradient descent toexplore more. By adding noise only to the problematic parts of the activation function, we allow the optimization procedure to explore the boundary between the degenerate (saturating) and the well-behaved parts of the activation function. We also establish connections to simulated annealing, when the amount of noise is annealed down, making it easier to optimize hard objective functions. We find experimentally that replacing such saturating activation functions by noisy variants helps training in many contexts, yielding state-of-the-art or competitive results on different datasets and task, especially when training seems to be the most difficult, e.g., when curriculum learning is necessary to obtain good results.},
	Author = {Gulcehre, Caglar and Moczulski, Marcin and Denil, Misha and Bengio, Yoshua},
	Date-Added = {2019-05-17 16:53:41 +0000},
	Date-Modified = {2019-05-17 16:53:41 +0000},
	Journal = {arXiv:1603.00391 [cs, stat]},
	Keywords = {Computer Science - Machine Learning, Computer Science - Neural and Evolutionary Computing, Statistics - Machine Learning},
	Month = mar,
	Note = {arXiv: 1603.00391},
	Title = {Noisy {Activation} {Functions}},
	Url = {http://arxiv.org/abs/1603.00391},
	Urldate = {2019-05-17},
	Year = {2016},
	Bdsk-File-1 = {YnBsaXN0MDDUAQIDBAUGJCVYJHZlcnNpb25YJG9iamVjdHNZJGFyY2hpdmVyVCR0b3ASAAGGoKgHCBMUFRYaIVUkbnVsbNMJCgsMDxJXTlMua2V5c1pOUy5vYmplY3RzViRjbGFzc6INDoACgAOiEBGABIAFgAdccmVsYXRpdmVQYXRoWWFsaWFzRGF0YV8QLS4uLy4uLy4uL2xpdHRlcmF0dXJlL0d1bGNlaHJlKDIwMTZhKU5vaXN5LnBkZtIXCxgZV05TLmRhdGFPEQH8AAAAAAH8AAIAAAxNYWNpbnRvc2ggSEQAAAAAAAAAAAAAAAAAAADMMSECSCsAAAVWW7QYR3VsY2VocmUoMjAxNmEpTm9pc3kucGRmAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAACILchdkEpLQAAAAAAAAAAAADAAIAAAkgAAAAAAAAAAAAAAAAAAAAC2xpdHRlcmF0dXJlAAAQAAgAAMwxEvIAAAARAAgAANkElqQAAAABABgFVlu0BVZbfgVWWxwDOlmAAzpZfwACDfkAAgBkTWFjaW50b3NoIEhEOlVzZXJzOgBNaWNoYWVsQXJiZWw6AERvY3VtZW50czoAR2F0c2J5OgBSZXNlYXJjaDoAbGl0dGVyYXR1cmU6AEd1bGNlaHJlKDIwMTZhKU5vaXN5LnBkZgAOADIAGABHAHUAbABjAGUAaAByAGUAKAAyADAAMQA2AGEAKQBOAG8AaQBzAHkALgBwAGQAZgAPABoADABNAGEAYwBpAG4AdABvAHMAaAAgAEgARAASAFFVc2Vycy9NaWNoYWVsQXJiZWwvRG9jdW1lbnRzL0dhdHNieS9SZXNlYXJjaC9saXR0ZXJhdHVyZS9HdWxjZWhyZSgyMDE2YSlOb2lzeS5wZGYAABMAAS8AABUAAgAT//8AAIAG0hscHR5aJGNsYXNzbmFtZVgkY2xhc3Nlc11OU011dGFibGVEYXRhox0fIFZOU0RhdGFYTlNPYmplY3TSGxwiI1xOU0RpY3Rpb25hcnmiIiBfEA9OU0tleWVkQXJjaGl2ZXLRJidUcm9vdIABAAgAEQAaACMALQAyADcAQABGAE0AVQBgAGcAagBsAG4AcQBzAHUAdwCEAI4AvgDDAMsCywLNAtIC3QLmAvQC+AL/AwgDDQMaAx0DLwMyAzcAAAAAAAACAQAAAAAAAAAoAAAAAAAAAAAAAAAAAAADOQ==}}

@article{Simsekli:2018,
	Abstract = {By building up on the recent theory that established the connection between implicit generative modeling and optimal transport, in this study, we propose a novel parameter-free algorithm for learning the underlying distributions of complicated datasets and sampling from them. The proposed algorithm is based on a functional optimization problem, which aims at finding a measure that is close to the data distribution as much as possible and also expressive enough for generative modeling purposes. We formulate the problem as a gradient flow in the space of probability measures. The connections between gradient flows and stochastic differential equations let us develop a computationally efficient algorithm for solving the optimization problem, where the resulting algorithm resembles the recent dynamics-based Markov Chain Monte Carlo algorithms. We provide formal theoretical analysis where we prove finite-time error guarantees for the proposed algorithm. Our experimental results support our theory and shows that our algorithm is able to capture the structure of challenging distributions.},
	Annote = {Comment: 27 pages},
	Author = {{\c S}im{\c s}ekli, Umut and Liutkus, Antoine and Majewski, Szymon and Durmus, Alain},
	Date-Added = {2019-05-16 16:26:38 +0000},
	Date-Modified = {2019-05-16 16:26:38 +0000},
	Journal = {arXiv:1806.08141 [cs, stat]},
	Keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
	Month = jun,
	Note = {arXiv: 1806.08141},
	Shorttitle = {Sliced-{Wasserstein} {Flows}},
	Title = {Sliced-{Wasserstein} {Flows}: {Nonparametric} {Generative} {Modeling} via {Optimal} {Transport} and {Diffusions}},
	Url = {http://arxiv.org/abs/1806.08141},
	Urldate = {2019-05-15},
	Year = {2018},
	Bdsk-File-1 = {YnBsaXN0MDDUAQIDBAUGJCVYJHZlcnNpb25YJG9iamVjdHNZJGFyY2hpdmVyVCR0b3ASAAGGoKgHCBMUFRYaIVUkbnVsbNMJCgsMDxJXTlMua2V5c1pOUy5vYmplY3RzViRjbGFzc6INDoACgAOiEBGABIAFgAdccmVsYXRpdmVQYXRoWWFsaWFzRGF0YW8QPAAuAC4ALwAuAC4ALwAuAC4ALwBsAGkAdAB0AGUAcgBhAHQAdQByAGUALwBTAycAaQBtAHMDJwBlAGsAbABpACgAMgAwADEAOABhACkAUwBsAGkAYwBlAGQALQBXAGEAcwBzAGUAcgBzAHQAZQBpAG4ALgBwAGQAZtIXCxgZV05TLmRhdGFPEQIiAAAAAAIiAAIAAAxNYWNpbnRvc2ggSEQAAAAAAAAAAAAAAAAAAADMMSECSCsAAAVWW7QPPz8/Izg3RkNCREIucGRmAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAACH/L29kBI5YAAAAAAAAAAAADAAIAAAkgAAAAAAAAAAAAAAAAAAAAC2xpdHRlcmF0dXJlAAAQAAgAAMwxEvIAAAARAAgAANkBFYYAAAABABgFVlu0BVZbfgVWWxwDOlmAAzpZfwACDfkAAgBbTWFjaW50b3NoIEhEOlVzZXJzOgBNaWNoYWVsQXJiZWw6AERvY3VtZW50czoAR2F0c2J5OgBSZXNlYXJjaDoAbGl0dGVyYXR1cmU6AD8/PyM4N0ZDQkRCLnBkZgAADgBQACcAUwMnAGkAbQBzAycAZQBrAGwAaQAoADIAMAAxADgAYQApAFMAbABpAGMAZQBkAC0AVwBhAHMAcwBlAHIAcwB0AGUAaQBuAC4AcABkAGYADwAaAAwATQBhAGMAaQBuAHQAbwBzAGgAIABIAEQAEgBiVXNlcnMvTWljaGFlbEFyYmVsL0RvY3VtZW50cy9HYXRzYnkvUmVzZWFyY2gvbGl0dGVyYXR1cmUvU8ynaW1zzKdla2xpKDIwMThhKVNsaWNlZC1XYXNzZXJzdGVpbi5wZGYAEwABLwAAFQACABP//wAAgAbSGxwdHlokY2xhc3NuYW1lWCRjbGFzc2VzXU5TTXV0YWJsZURhdGGjHR8gVk5TRGF0YVhOU09iamVjdNIbHCIjXE5TRGljdGlvbmFyeaIiIF8QD05TS2V5ZWRBcmNoaXZlctEmJ1Ryb290gAEACAARABoAIwAtADIANwBAAEYATQBVAGAAZwBqAGwAbgBxAHMAdQB3AIQAjgEJAQ4BFgM8Az4DQwNOA1cDZQNpA3ADeQN+A4sDjgOgA6MDqAAAAAAAAAIBAAAAAAAAACgAAAAAAAAAAAAAAAAAAAOq}}

@article{tolstikhin2017minimax,
	Author = {Tolstikhin, Ilya and Sriperumbudur, Bharath K and Muandet, Krikamol},
	Journal = {The Journal of Machine Learning Research},
	Number = {1},
	Pages = {3002--3048},
	Publisher = {JMLR. org},
	Title = {Minimax estimation of kernel mean embeddings},
	Volume = {18},
	Year = {2017}}

@article{herrmann2010non,
	Author = {Herrmann, Samuel and Tugaut, Julian},
	Journal = {Stochastic Processes and their Applications},
	Number = {7},
	Pages = {1215--1246},
	Publisher = {Elsevier},
	Title = {Non-uniqueness of stationary measures for self-stabilizing processes},
	Volume = {120},
	Year = {2010}}

@article{tugaut2014phase,
	Author = {Tugaut, Julian},
	Journal = {Stochastics An International Journal of Probability and Stochastic Processes},
	Number = {2},
	Pages = {257--284},
	Publisher = {Taylor \& Francis},
	Title = {Phase transitions of McKean--Vlasov processes in double-wells landscape},
	Volume = {86},
	Year = {2014}}

@article{tugaut2013convergence,
	Author = {Tugaut, Julian and others},
	Journal = {The Annals of Probability},
	Number = {3A},
	Pages = {1427--1460},
	Publisher = {Institute of Mathematical Statistics},
	Title = {Convergence to the equilibria for self-stabilizing processes in double-well landscape},
	Volume = {41},
	Year = {2013}}

@article{tugaut2014self,
	Author = {Tugaut, Julian},
	Journal = {Journal of Theoretical Probability},
	Number = {1},
	Pages = {57--79},
	Publisher = {Springer},
	Title = {Self-stabilizing Processes in Multi-wells Landscape in $\mathbb{R}^d$-Invariant Probabilities},
	Volume = {27},
	Year = {2014}}

@article{sutherland2017efficient,
	Author = {Sutherland, Dougal J and Strathmann, Heiko and Arbel, Michael and Gretton, Arthur},
	Journal = {AISTATS},
	Title = {Efficient and principled score estimation with Nystr$\backslash$" om kernel exponential families},
	Year = {2018}}

@article{csimcsekli2018sliced,
	Author = {{\c{S}}im{\c{s}}ekli, Umut and Liutkus, Antoine and Majewski, Szymon and Durmus, Alain},
	Booktitle = {ICML},
	Title = {Sliced-Wasserstein flows: Nonparametric generative modeling via optimal transport and diffusions},
	Year = {2019}}

@inproceedings{liu2017stein,
	Author = {Liu, Qiang},
	Booktitle = {Advances in neural information processing systems},
	Pages = {3115--3123},
	Title = {Stein variational gradient descent as gradient flow},
	Year = {2017}}

@article{rotskoff2018neural,
	Author = {Rotskoff, Grant M and Vanden-Eijnden, Eric},
	Journal = {arXiv preprint arXiv:1805.00915},
	Title = {Neural networks as interacting particle systems: Asymptotic convexity of the loss landscape and universal scaling of the approximation error},
	Year = {2018}}

@article{mei2018mean,
	Author = {Mei, Song and Montanari, Andrea and Nguyen, Phan-Minh},
	Journal = {Proceedings of the National Academy of Sciences},
	Number = {33},
	Pages = {E7665--E7671},
	Publisher = {National Acad Sciences},
	Title = {A mean field view of the landscape of two-layer neural networks},
	Volume = {115},
	Year = {2018}}

@article{sirignano2018mean,
	Author = {Sirignano, Justin and Spiliopoulos, Konstantinos},
	Journal = {arXiv preprint arXiv:1808.09372},
	Title = {Mean field analysis of neural networks: A central limit theorem},
	Year = {2018}}

@book{smola1998learning,
	Author = {Smola, Alex J and Scholkopf, Bernhard},
	Publisher = {Citeseer},
	Title = {Learning with kernels},
	Volume = {4},
	Year = {1998}}

@misc{pavliotis2011stochastic,
	Author = {Pavliotis, Grigorios A},
	Publisher = {Springer},
	Title = {Stochastic processes and applications},
	Year = {2011}}

@article{mckean1966class,
	Author = {McKean Jr, HP},
	Journal = {Proceedings of the National Academy of Sciences of the United States of America},
	Number = {6},
	Pages = {1907},
	Publisher = {National Academy of Sciences},
	Title = {A class of Markov processes associated with nonlinear parabolic equations},
	Volume = {56},
	Year = {1966}}

@article{durmus2018elementary,
	Author = {Durmus, Alain and Eberle, Andreas and Guillin, Arnaud and Zimmer, Raphael},
	Journal = {arXiv preprint arXiv:1805.11387},
	Title = {An elementary approach to uniform in time propagation of chaos},
	Year = {2018}}

@inproceedings{kac1956foundations,
	Author = {Kac, Mark},
	Booktitle = {Proceedings of The third Berkeley symposium on mathematical statistics and probability},
	Organization = {University of California Press Berkeley and Los Angeles, California},
	Pages = {171--197},
	Title = {Foundations of kinetic theory},
	Volume = {3},
	Year = {1956}}

@inproceedings{chizat2018global,
	Author = {Chizat, Lenaic and Bach, Francis},
	Booktitle = {Advances in neural information processing systems},
	Pages = {3036--3046},
	Title = {On the global convergence of gradient descent for over-parameterized models using optimal transport},
	Year = {2018}}

@article{dudley1969speed,
	Author = {Dudley, RM},
	Journal = {The Annals of Mathematical Statistics},
	Number = {1},
	Pages = {40--50},
	Publisher = {JSTOR},
	Title = {The speed of mean Glivenko-Cantelli convergence},
	Volume = {40},
	Year = {1969}}

@article{weed2017sharp,
	Author = {Weed, Jonathan and Bach, Francis},
	Journal = {arXiv preprint arXiv:1707.00087},
	Title = {Sharp asymptotic and finite-sample rates of convergence of empirical measures in Wasserstein distance},
	Year = {2017}}

@article{Rotskoff:2019,
	Abstract = {Neural networks with a large number of parameters admit a mean-field description, which has recently served as a theoretical explanation for the favorable training properties of "overparameterized" models. In this regime, gradient descent obeys a deterministic partial differential equation (PDE) that converges to a globally optimal solution for networks with a single hidden layer under appropriate assumptions. In this work, we propose a non-local mass transport dynamics that leads to a modified PDE with the same minimizer. We implement this non-local dynamics as a stochastic neuronal birth-death process and we prove that it accelerates the rate of convergence in the mean-field limit. We subsequently realize this PDE with two classes of numerical schemes that converge to the mean-field equation, each of which can easily be implemented for neural networks with finite numbers of parameters. We illustrate our algorithms with two models to provide intuition for the mechanism through which convergence is accelerated.},
	Author = {Rotskoff, Grant and Jelassi, Samy and Bruna, Joan and Vanden-Eijnden, Eric},
	Date-Added = {2019-04-30 03:28:04 +0000},
	Date-Modified = {2019-04-30 03:28:04 +0000},
	Journal = {arXiv:1902.01843 [cs, stat]},
	Keywords = {Statistics - Machine Learning, Computer Science - Machine Learning},
	Month = feb,
	Note = {arXiv: 1902.01843},
	Title = {Global convergence of neuron birth-death dynamics},
	Url = {http://arxiv.org/abs/1902.01843},
	Urldate = {2019-03-01},
	Year = {2019},
	Bdsk-File-1 = {YnBsaXN0MDDUAQIDBAUGJCVYJHZlcnNpb25YJG9iamVjdHNZJGFyY2hpdmVyVCR0b3ASAAGGoKgHCBMUFRYaIVUkbnVsbNMJCgsMDxJXTlMua2V5c1pOUy5vYmplY3RzViRjbGFzc6INDoACgAOiEBGABIAFgAdccmVsYXRpdmVQYXRoWWFsaWFzRGF0YV8QLi4uLy4uLy4uL2xpdHRlcmF0dXJlL1JvdHNrb2ZmKDIwMTlhKUdsb2JhbC5wZGbSFwsYGVdOUy5kYXRhTxECAAAAAAACAAACAAAMTWFjaW50b3NoIEhEAAAAAAAAAAAAAAAAAAAAzDEhAkgrAAAFVlu0GVJvdHNrb2ZmKDIwMTlhKUdsb2JhbC5wZGYAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAg7Er3YnjMeAAAAAAAAAAAAAwACAAAJIAAAAAAAAAAAAAAAAAAAAAtsaXR0ZXJhdHVyZQAAEAAIAADMMRLyAAAAEQAIAADYnjMeAAAAAQAYBVZbtAVWW34FVlscAzpZgAM6WX8AAg35AAIAZU1hY2ludG9zaCBIRDpVc2VyczoATWljaGFlbEFyYmVsOgBEb2N1bWVudHM6AEdhdHNieToAUmVzZWFyY2g6AGxpdHRlcmF0dXJlOgBSb3Rza29mZigyMDE5YSlHbG9iYWwucGRmAAAOADQAGQBSAG8AdABzAGsAbwBmAGYAKAAyADAAMQA5AGEAKQBHAGwAbwBiAGEAbAAuAHAAZABmAA8AGgAMAE0AYQBjAGkAbgB0AG8AcwBoACAASABEABIAUlVzZXJzL01pY2hhZWxBcmJlbC9Eb2N1bWVudHMvR2F0c2J5L1Jlc2VhcmNoL2xpdHRlcmF0dXJlL1JvdHNrb2ZmKDIwMTlhKUdsb2JhbC5wZGYAEwABLwAAFQACABP//wAAgAbSGxwdHlokY2xhc3NuYW1lWCRjbGFzc2VzXU5TTXV0YWJsZURhdGGjHR8gVk5TRGF0YVhOU09iamVjdNIbHCIjXE5TRGljdGlvbmFyeaIiIF8QD05TS2V5ZWRBcmNoaXZlctEmJ1Ryb290gAEACAARABoAIwAtADIANwBAAEYATQBVAGAAZwBqAGwAbgBxAHMAdQB3AIQAjgC/AMQAzALQAtIC1wLiAusC+QL9AwQDDQMSAx8DIgM0AzcDPAAAAAAAAAIBAAAAAAAAACgAAAAAAAAAAAAAAAAAAAM+},
	Bdsk-File-2 = {YnBsaXN0MDDUAQIDBAUGJCVYJHZlcnNpb25YJG9iamVjdHNZJGFyY2hpdmVyVCR0b3ASAAGGoKgHCBMUFRYaIVUkbnVsbNMJCgsMDxJXTlMua2V5c1pOUy5vYmplY3RzViRjbGFzc6INDoACgAOiEBGABIAFgAdccmVsYXRpdmVQYXRoWWFsaWFzRGF0YV8QLi4uLy4uLy4uL2xpdHRlcmF0dXJlL1JvdHNrb2ZmKDIwMTliKUdsb2JhbC5wZGbSFwsYGVdOUy5kYXRhTxECAAAAAAACAAACAAAMTWFjaW50b3NoIEhEAAAAAAAAAAAAAAAAAAAAzDEhAkgrAAAFVlu0GVJvdHNrb2ZmKDIwMTliKUdsb2JhbC5wZGYAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAhvJzLY660+AAAAAAAAAAAAAwACAAAJIAAAAAAAAAAAAAAAAAAAAAtsaXR0ZXJhdHVyZQAAEAAIAADMMRLyAAAAEQAIAADY658uAAAAAQAYBVZbtAVWW34FVlscAzpZgAM6WX8AAg35AAIAZU1hY2ludG9zaCBIRDpVc2VyczoATWljaGFlbEFyYmVsOgBEb2N1bWVudHM6AEdhdHNieToAUmVzZWFyY2g6AGxpdHRlcmF0dXJlOgBSb3Rza29mZigyMDE5YilHbG9iYWwucGRmAAAOADQAGQBSAG8AdABzAGsAbwBmAGYAKAAyADAAMQA5AGIAKQBHAGwAbwBiAGEAbAAuAHAAZABmAA8AGgAMAE0AYQBjAGkAbgB0AG8AcwBoACAASABEABIAUlVzZXJzL01pY2hhZWxBcmJlbC9Eb2N1bWVudHMvR2F0c2J5L1Jlc2VhcmNoL2xpdHRlcmF0dXJlL1JvdHNrb2ZmKDIwMTliKUdsb2JhbC5wZGYAEwABLwAAFQACABP//wAAgAbSGxwdHlokY2xhc3NuYW1lWCRjbGFzc2VzXU5TTXV0YWJsZURhdGGjHR8gVk5TRGF0YVhOU09iamVjdNIbHCIjXE5TRGljdGlvbmFyeaIiIF8QD05TS2V5ZWRBcmNoaXZlctEmJ1Ryb290gAEACAARABoAIwAtADIANwBAAEYATQBVAGAAZwBqAGwAbgBxAHMAdQB3AIQAjgC/AMQAzALQAtIC1wLiAusC+QL9AwQDDQMSAx8DIgM0AzcDPAAAAAAAAAIBAAAAAAAAACgAAAAAAAAAAAAAAAAAAAM+}}

@article{jourdain2007nonlinear,
	Author = {Jourdain, Benjamin and M{\'e}l{\'e}ard, Sylvie and Woyczynski, Wojbor},
	Journal = {arXiv preprint arXiv:0707.2723},
	Title = {Nonlinear SDEs driven by L$\backslash$'evy processes and related PDEs},
	Year = {2007}}

@article{sriperumbudur2010hilbert,
	Author = {Sriperumbudur, Bharath K and Gretton, Arthur and Fukumizu, Kenji and Sch{\"o}lkopf, Bernhard and Lanckriet, Gert RG},
	Journal = {Journal of Machine Learning Research},
	Number = {Apr},
	Pages = {1517--1561},
	Title = {Hilbert space embeddings and metrics on probability measures},
	Volume = {11},
	Year = {2010}}

@incollection{sznitman1991topics,
	Author = {Sznitman, Alain-Sol},
	Booktitle = {Ecole d'{\'e}t{\'e} de probabilit{\'e}s de Saint-Flour},
	Pages = {165--251},
	Publisher = {Springer},
	Title = {Topics in propagation of chaos},
	Year = {1991}}

@article{rotskoff2019global,
	Author = {Rotskoff, Grant and Jelassi, Samy and Bruna, Joan and Vanden-Eijnden, Eric},
	Journal = {arXiv preprint arXiv:1902.01843},
	Title = {Global convergence of neuron birth-death dynamics},
	Year = {2019}}

@article{mroueh2018regularized,
	Author = {Mroueh, Youssef and Sercu, Tom and Raj, Anant},
	Journal = {arXiv preprint arXiv:1805.12062},
	Title = {Regularized Kernel and Neural Sobolev Descent: Dynamic MMD Transport},
	Year = {2018}}

@article{shi2018spectral,
	Author = {Shi, Jiaxin and Sun, Shengyang and Zhu, Jun},
	Journal = {ICML},
	Title = {A spectral approach to gradient estimation for implicit distributions},
	Year = {2018}}

@article{li2018gradient,
	Author = {Li, Yingzhen and Turner, Richard E},
	Journal = {ICLR},
	Title = {Gradient estimators for implicit models},
	Year = {2018}}

@book{ito1951stochastic,
	Author = {It{\^o}, Kiyosi},
	Publisher = {American Mathematical Soc.},
	Title = {On stochastic differential equations},
	Volume = {4},
	Year = {1951}}

@article{Arbel:2018,
	Abstract = {We propose a principled method for gradient-based regularization of the critic of GAN-like models trained by adversarially optimizing the kernel of a Maximum Mean Discrepancy (MMD). We show that controlling the gradient of the critic is vital to having a sensible loss function, and devise a method to enforce exact, analytical gradient constraints at no additional cost compared to existing approximate techniques based on additive regularizers. The new loss function is provably continuous, and experiments show that it stabilizes and accelerates training, giving image generation models that outperform state-of-the art methods on \$160 {\textbackslash}times 160\$ CelebA and \$64 {\textbackslash}times 64\$ unconditional ImageNet.},
	Annote = {Comment: Code available at https://github.com/MichaelArbel/Scaled-MMD-GAN . v2: NIPS camera-ready version},
	Author = {Arbel, Michael and Sutherland, Dougal J. and Bi{\'n}kowski, Miko{\l}aj and Gretton, Arthur},
	Date-Added = {2019-04-19 13:59:35 +0000},
	Date-Modified = {2019-04-19 13:59:35 +0000},
	Journal = {arXiv:1805.11565 [cs, stat]},
	Keywords = {Statistics - Machine Learning, Computer Science - Machine Learning},
	Month = may,
	Note = {arXiv: 1805.11565},
	Title = {On gradient regularizers for {MMD} {GANs}},
	Url = {http://arxiv.org/abs/1805.11565},
	Urldate = {2018-11-01},
	Year = {2018},
	Bdsk-File-1 = {YnBsaXN0MDDUAQIDBAUGJCVYJHZlcnNpb25YJG9iamVjdHNZJGFyY2hpdmVyVCR0b3ASAAGGoKgHCBMUFRYaIVUkbnVsbNMJCgsMDxJXTlMua2V5c1pOUy5vYmplY3RzViRjbGFzc6INDoACgAOiEBGABIAFgAdccmVsYXRpdmVQYXRoWWFsaWFzRGF0YV8QMC4uLy4uLy4uL2xpdHRlcmF0dXJlL0FyYmVsKDIwMThhKU9uIGdyYWRpZW50LnBkZtIXCxgZV05TLmRhdGFPEQIIAAAAAAIIAAIAAAxNYWNpbnRvc2ggSEQAAAAAAAAAAAAAAAAAAADMMSECSCsAAAVWW7QbQXJiZWwoMjAxOGEpT24gZ3JhZGllbnQucGRmAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAB93u+9gAoowAAAAAAAAAAAADAAIAAAkgAAAAAAAAAAAAAAAAAAAAC2xpdHRlcmF0dXJlAAAQAAgAAMwxEvIAAAARAAgAANgAoowAAAABABgFVlu0BVZbfgVWWxwDOlmAAzpZfwACDfkAAgBnTWFjaW50b3NoIEhEOlVzZXJzOgBNaWNoYWVsQXJiZWw6AERvY3VtZW50czoAR2F0c2J5OgBSZXNlYXJjaDoAbGl0dGVyYXR1cmU6AEFyYmVsKDIwMThhKU9uIGdyYWRpZW50LnBkZgAADgA4ABsAQQByAGIAZQBsACgAMgAwADEAOABhACkATwBuACAAZwByAGEAZABpAGUAbgB0AC4AcABkAGYADwAaAAwATQBhAGMAaQBuAHQAbwBzAGgAIABIAEQAEgBUVXNlcnMvTWljaGFlbEFyYmVsL0RvY3VtZW50cy9HYXRzYnkvUmVzZWFyY2gvbGl0dGVyYXR1cmUvQXJiZWwoMjAxOGEpT24gZ3JhZGllbnQucGRmABMAAS8AABUAAgAT//8AAIAG0hscHR5aJGNsYXNzbmFtZVgkY2xhc3Nlc11OU011dGFibGVEYXRhox0fIFZOU0RhdGFYTlNPYmplY3TSGxwiI1xOU0RpY3Rpb25hcnmiIiBfEA9OU0tleWVkQXJjaGl2ZXLRJidUcm9vdIABAAgAEQAaACMALQAyADcAQABGAE0AVQBgAGcAagBsAG4AcQBzAHUAdwCEAI4AwQDGAM4C2gLcAuEC7AL1AwMDBwMOAxcDHAMpAywDPgNBA0YAAAAAAAACAQAAAAAAAAAoAAAAAAAAAAAAAAAAAAADSA==},
	Bdsk-Url-1 = {http://arxiv.org/abs/1805.11565}}

@misc{Santambrogio:2015,
	Author = {Filippo Santambrogio},
	Date-Added = {2019-04-18 15:39:38 +0000},
	Date-Modified = {2019-04-18 15:39:38 +0000},
	Title = {Optimal Transport for Applied Mathematicians},
	Year = {2015},
	Bdsk-File-1 = {YnBsaXN0MDDUAQIDBAUGJCVYJHZlcnNpb25YJG9iamVjdHNZJGFyY2hpdmVyVCR0b3ASAAGGoKgHCBMUFRYaIVUkbnVsbNMJCgsMDxJXTlMua2V5c1pOUy5vYmplY3RzViRjbGFzc6INDoACgAOiEBGABIAFgAdccmVsYXRpdmVQYXRoWWFsaWFzRGF0YV8QMy4uLy4uLy4uL2xpdHRlcmF0dXJlL1NhbnRhbWJyb2dpbygyMDE1YSlPcHRpbWFsLnBkZtIXCxgZV05TLmRhdGFPEQIUAAAAAAIUAAIAAAxNYWNpbnRvc2ggSEQAAAAAAAAAAAAAAAAAAADMMSECSCsAAAVWW7QeU2FudGFtYnJvZ2lvKDIwMTVhKU9wdGltYWwucGRmAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAACHIYDdap8nhQREYgAAAAAAADAAIAAAkgAAAAAAAAAAAAAAAAAAAAC2xpdHRlcmF0dXJlAAAQAAgAAMwxEvIAAAARAAgAANap8ngAAAABABgFVlu0BVZbfgVWWxwDOlmAAzpZfwACDfkAAgBqTWFjaW50b3NoIEhEOlVzZXJzOgBNaWNoYWVsQXJiZWw6AERvY3VtZW50czoAR2F0c2J5OgBSZXNlYXJjaDoAbGl0dGVyYXR1cmU6AFNhbnRhbWJyb2dpbygyMDE1YSlPcHRpbWFsLnBkZgAOAD4AHgBTAGEAbgB0AGEAbQBiAHIAbwBnAGkAbwAoADIAMAAxADUAYQApAE8AcAB0AGkAbQBhAGwALgBwAGQAZgAPABoADABNAGEAYwBpAG4AdABvAHMAaAAgAEgARAASAFdVc2Vycy9NaWNoYWVsQXJiZWwvRG9jdW1lbnRzL0dhdHNieS9SZXNlYXJjaC9saXR0ZXJhdHVyZS9TYW50YW1icm9naW8oMjAxNWEpT3B0aW1hbC5wZGYAABMAAS8AABUAAgAT//8AAIAG0hscHR5aJGNsYXNzbmFtZVgkY2xhc3Nlc11OU011dGFibGVEYXRhox0fIFZOU0RhdGFYTlNPYmplY3TSGxwiI1xOU0RpY3Rpb25hcnmiIiBfEA9OU0tleWVkQXJjaGl2ZXLRJidUcm9vdIABAAgAEQAaACMALQAyADcAQABGAE0AVQBgAGcAagBsAG4AcQBzAHUAdwCEAI4AxADJANEC6QLrAvAC+wMEAxIDFgMdAyYDKwM4AzsDTQNQA1UAAAAAAAACAQAAAAAAAAAoAAAAAAAAAAAAAAAAAAADVw==}}

@article{santambrogio2017euclidean,
	Author = {Santambrogio, Filippo},
	Journal = {Bulletin of Mathematical Sciences},
	Number = {1},
	Pages = {87--154},
	Publisher = {Springer},
	Title = {$\{$Euclidean, metric, and Wasserstein$\}$ gradient flows: an overview},
	Volume = {7},
	Year = {2017}}

@book{ambrosio2008gradient,
	Author = {Ambrosio, Luigi and Gigli, Nicola and Savar{\'e}, Giuseppe},
	Publisher = {Springer Science \& Business Media},
	Title = {Gradient flows: in metric spaces and in the space of probability measures},
	Year = {2008}}

@article{wibisono2018sampling,
	Author = {Wibisono, Andre},
	Journal = {arXiv preprint arXiv:1802.08089},
	Title = {Sampling as optimization in the space of measures: The Langevin dynamics as a composite optimization problem},
	Year = {2018}}

@article{mmd-jmlr,
	Author = {Gretton, A. and Borgwardt, K. M. and Rasch, M. J. and Sch{\"{o}}lkopf, B. and Smola, A. J.},
	Date-Added = {2019-04-02 12:33:53 +0000},
	Date-Modified = {2019-04-02 12:33:53 +0000},
	Journal = {JMLR},
	Title = {A Kernel Two-Sample Test},
	Volume = {13},
	Year = {2012}}

@inproceedings{likelihoods-vs-samples,
	Archiveprefix = {arXiv},
	Author = {Theis, L. and van den Oord, A. and Bethge, M.},
	Booktitle = {ICLR},
	Date-Added = {2019-04-02 12:33:53 +0000},
	Date-Modified = {2019-04-02 12:33:53 +0000},
	Eprint = {1511.01844},
	Title = {A note on the evaluation of generative models},
	Year = {2016}}

@misc{note-on-inception,
	Archiveprefix = {arXiv},
	Author = {Shane Barratt and Rishi Sharma},
	Date-Added = {2019-04-02 12:33:53 +0000},
	Date-Modified = {2019-04-02 12:33:53 +0000},
	Eprint = {1801.01973},
	Title = {A Note on the Inception Score},
	Year = {2018}}

@article{owen:integrals,
	Author = {Owen, D. B.},
	Date-Added = {2019-04-02 12:33:53 +0000},
	Date-Modified = {2019-04-02 12:33:53 +0000},
	Number = {4},
	Pages = {389--419},
	Title = {A table of normal integrals},
	Volume = {9},
	Year = {1980}}

@inproceedings{3sample,
	Archiveprefix = {arXiv},
	Author = {W. Bounliphone and E. Belilovsky and M. B. Blaschko and I. Antonoglou and A. Gretton},
	Booktitle = {ICLR},
	Date-Added = {2019-04-02 12:33:53 +0000},
	Date-Modified = {2019-04-02 12:33:53 +0000},
	Eprint = {1511.04581},
	Title = {A Test of Relative Similarity For Model Selection in Generative Models},
	Year = {2016}}

@inproceedings{adam,
	Archiveprefix = {arXiv},
	Author = {Kingma, D. and Ba, J.},
	Booktitle = {ICLR},
	Date-Added = {2019-04-02 12:33:53 +0000},
	Date-Modified = {2019-04-02 12:33:53 +0000},
	Eprint = {1412.6980},
	Title = {Adam: A Method for Stochastic Optimization},
	Year = {2015}}

@misc{empirical-evaluation,
	Author = {Gao Huang and Yang Yuan and Qiantong Xu and Chuan Guo and Yu Sun and Felix Wu and Kilian Weinberger},
	Date-Added = {2019-04-02 12:33:53 +0000},
	Date-Modified = {2019-04-02 12:33:53 +0000},
	Title = {An empirical study on evaluation metrics of generative adversarial networks},
	Url = {https://openreview.net/forum?id=Sy1f0e-R-},
	Year = {2018},
	Bdsk-Url-1 = {https://openreview.net/forum?id=Sy1f0e-R-}}

@inproceedings{approx-convergence-props,
	Archiveprefix = {arXiv},
	Author = {Liu, S. and Bousquet, O. and Chaudhuri, K.},
	Booktitle = {NIPS},
	Date-Added = {2019-04-02 12:33:53 +0000},
	Date-Modified = {2019-04-02 12:33:53 +0000},
	Eprint = {1705.08991},
	Title = {Approximation and Convergence Properties of Generative Adversarial Learning},
	Year = {2017}}

@misc{began,
	Archiveprefix = {arXiv},
	Author = {D. Berthelot and T. Schumm and L. Metz},
	Date-Added = {2019-04-02 12:33:53 +0000},
	Date-Modified = {2019-04-02 12:33:53 +0000},
	Eprint = {1703.10717},
	Title = {{BEGAN}: Boundary Equilibrium Generative Adversarial Networks},
	Year = {2017}}

@inproceedings{BenMesDauRif13,
	Archiveprefix = {arXiv},
	Author = {Y. Bengio and G. Mesnil and Y. Dauphin and S. Rifai},
	Booktitle = {ICML},
	Date-Added = {2019-04-02 12:33:53 +0000},
	Date-Modified = {2019-04-02 12:33:53 +0000},
	Eprint = {1207.4404},
	Title = {Better mixing via deep representations},
	Year = {2013}}

@inproceedings{beyond-face-rotation,
	Archiveprefix = {arXiv},
	Author = {R. Huang and S. Zhang and T. Li and R. He},
	Booktitle = {ICCV},
	Date-Added = {2019-04-02 12:33:53 +0000},
	Date-Modified = {2019-04-02 12:33:53 +0000},
	Eprint = {1704.04086},
	Title = {Beyond Face Rotation: Global and Local Perception {GAN} for Photorealistic and Identity Preserving Frontal View Synthesis},
	Year = {2017}}

@inproceedings{Hjelm:2018,
	Author = {Hjelm, {R. Devon} and Jacob, {Athul Paul} and Che, Tong and Trischler, Adam and Cho, Kyunghyun and Bengio,Yoshua},
	Booktitle = {ICLR},
	Date-Added = {2019-04-02 12:33:53 +0000},
	Date-Modified = {2019-04-02 12:33:53 +0000},
	Title = {Boundary-Seeking Generative Adversarial Networks},
	Year = {2018}}

@inproceedings{b-test,
	Archiveprefix = {arXiv},
	Author = {Zaremba, W. and Gretton, A. and Blaschko, M. B.},
	Booktitle = {NIPS},
	Date-Added = {2019-04-02 12:33:53 +0000},
	Date-Modified = {2019-04-02 12:33:53 +0000},
	Eprint = {1307.1954},
	Title = {B-tests: Low Variance Kernel Two-Sample Tests},
	Year = {2013}}

@misc{gan-vs-ml-realnvp,
	Archiveprefix = {arXiv},
	Author = {Danihelka, I. and Lakshminarayanan, B. and Uria, B. and Wierstra, D. and Dayan, P.},
	Date-Added = {2019-04-02 12:33:53 +0000},
	Date-Modified = {2019-04-02 12:33:53 +0000},
	Eprint = {1705.05263},
	Title = {Comparison of Maximum Likelihood and {GAN}-based training of {R}eal {NVP}s},
	Year = {2017}}

@inproceedings{coulomb-gan,
	Archiveprefix = {arXiv},
	Arxivid = {1708.08819},
	Author = {Unterthiner, Thomas and Nessler, Bernhard and Seward, Calvin and Klambauer, G{\"{u}}nter and Heusel, Martin and Ramsauer, Hubert and Hochreiter, Sepp},
	Booktitle = {ICLR},
	Date-Added = {2019-04-02 12:33:53 +0000},
	Date-Modified = {2019-04-02 12:33:53 +0000},
	Eprint = {1708.08819},
	Title = {{C}oulomb {GAN}s: Provably Optimal Nash Equilibria via Potential Fields},
	Year = {2018}}

@inproceedings{celeba,
	Author = {Liu, Z. and Luo, P. and Wang, X. and Tang, X.},
	Booktitle = {ICCV},
	Date-Added = {2019-04-02 12:33:53 +0000},
	Date-Modified = {2019-04-02 12:33:53 +0000},
	Title = {Deep learning face attributes in the wild},
	Year = {2015}}

@inproceedings{Binkowski:2018,
	Archiveprefix = {arXiv},
	Author = {Bi{\'n}kowski, Miko{\l}aj and Sutherland, Dougal J. and Arbel, Michael and Gretton, Arthur},
	Booktitle = {ICLR},
	Date-Added = {2019-04-02 12:33:53 +0000},
	Date-Modified = {2019-04-02 12:33:53 +0000},
	Eprint = {1801.01401},
	Title = {Demystifying {MMD} {GANs}},
	Year = {2018}}

@inproceedings{real-nvp,
	Archiveprefix = {arXiv},
	Author = {Dinh, L. and Sohl-Dickstein, J. and Bengio, S.},
	Booktitle = {ICLR},
	Date-Added = {2019-04-02 12:33:53 +0000},
	Date-Modified = {2019-04-02 12:33:53 +0000},
	Eprint = {1605.08803},
	Title = {Density estimation using Real {NVP}},
	Year = {2017}}

@incollection{Bouchacourt:2016,
	Author = {Bouchacourt, D. and Mudigonda, P. K. and Nowozin, S.},
	Booktitle = {NIPS},
	Date-Added = {2019-04-02 12:33:53 +0000},
	Date-Modified = {2019-04-02 12:33:53 +0000},
	Pages = {352--360},
	Title = {{DISCO} Nets: {DIS}similarity {CO}efficients Networks},
	Year = {2016}}

@article{Lyons13,
	Author = {R. Lyons},
	Date-Added = {2019-04-02 12:33:53 +0000},
	Date-Modified = {2019-04-02 12:33:53 +0000},
	Journal = {The Annals of Probability},
	Number = {5},
	Pages = {3051--3696},
	Title = {Distance covariance in metric spaces},
	Volume = {41},
	Year = {2013}}

@misc{dan,
	Archiveprefix = {arXiv},
	Author = {Li, C. and Alvarez-Melis, D. and Xu, K. and Jegelka, S. and Sra, S.},
	Date-Added = {2019-04-02 12:33:53 +0000},
	Date-Modified = {2019-04-02 12:33:53 +0000},
	Eprint = {1706.09549},
	Title = {Distributional Adversarial Networks},
	Year = {2017}}

@misc{birthday-test,
	Archiveprefix = {arXiv},
	Author = {Arora, S. and Zhang, Y.},
	Date-Added = {2019-04-02 12:33:53 +0000},
	Date-Modified = {2019-04-02 12:33:53 +0000},
	Eprint = {1706.08224},
	Title = {Do {GAN}s actually learn the distribution? {A}n empirical study},
	Year = {2017}}

@article{envelope-thm,
	Author = {P. Milgrom and I. Segal},
	Date-Added = {2019-04-02 12:33:53 +0000},
	Date-Modified = {2019-04-02 12:33:53 +0000},
	Journal = {Econometrica},
	Number = {2},
	Pages = {583--601},
	Title = {Envelope theorems for arbitrary choice sets},
	Volume = {70},
	Year = {2002}}

@article{energy-dist-is-mmd,
	Archiveprefix = {arXiv},
	Author = {Sejdinovic, D. and Sriperumbudur, B. K. and Gretton, A. and Fukumizu, K.},
	Date-Added = {2019-04-02 12:33:53 +0000},
	Date-Modified = {2019-04-02 12:33:53 +0000},
	Eprint = {1207.6076},
	Journal = {The Annals of Stastistics},
	Number = {5},
	Pages = {2263--2291},
	Title = {Equivalence of distance-based and {RKHS}-based statistics in hypothesis testing},
	Volume = {41},
	Year = {2013}}

@inproceedings{elu,
	Archiveprefix = {arXiv},
	Author = {Clevert, D.-A. and Unterthiner, T. and Hochreiter, S.},
	Booktitle = {ICLR},
	Date-Added = {2019-04-02 12:33:53 +0000},
	Date-Modified = {2019-04-02 12:33:53 +0000},
	Eprint = {1511.07289},
	Title = {Fast and Accurate Deep Network Learning by Exponential Linear Units ({ELU}s)},
	Year = {2016}}

@inproceedings{NowBotRyo16,
	Archiveprefix = {arXiv},
	Author = {S. Nowozin and B. Cseke and R. Tomioka},
	Booktitle = {NIPS},
	Date-Added = {2019-04-02 12:33:53 +0000},
	Date-Modified = {2019-04-02 12:33:53 +0000},
	Eprint = {1606.00709},
	Title = {f-{GAN}: Training Generative Neural Samplers using Variational Divergence Minimization},
	Year = {2016}}

@misc{first-order-gans,
	Archiveprefix = {arXiv},
	Arxivid = {1802.04591},
	Author = {Seward, Calvin and Unterthiner, Thomas and Bergmann, Urs and Jetchev, Nikolay and Hochreiter, Sepp},
	Date-Added = {2019-04-02 12:33:53 +0000},
	Date-Modified = {2019-04-02 12:33:53 +0000},
	Eprint = {1802.04591},
	Title = {First Order Generative Adversarial Networks},
	Year = {2018}}

@inproceedings{fisher-gan,
	Archiveprefix = {arXiv},
	Author = {Mroueh, Youssef and Sercu, Tom},
	Booktitle = {NIPS},
	Date-Added = {2019-04-02 12:33:53 +0000},
	Date-Modified = {2019-04-02 12:33:53 +0000},
	Eprint = {1705.09675},
	Title = {{F}isher {GAN}},
	Year = {2017}}

@inproceedings{fid,
	Archiveprefix = {arXiv},
	Author = {Heusel, M. and Ramsauer, H. and Unterthiner, T. and Nessler, B. and Klambauer, G. and Hochreiter, S.},
	Booktitle = {NIPS},
	Date-Added = {2019-04-02 12:33:53 +0000},
	Date-Modified = {2019-04-02 12:33:53 +0000},
	Eprint = {1706.08500},
	Title = {{GAN}s Trained by a Two Time-Scale Update Rule Converge to a {N}ash Equilibrium},
	Year = {2017}}

@book{RasWil06,
	Address = {Cambridge, MA},
	Author = {C. E. Rasmussen and C. K. I. Williams},
	Date-Added = {2019-04-02 12:33:53 +0000},
	Date-Modified = {2019-04-02 12:33:53 +0000},
	Publisher = {MIT Press},
	Title = {Gaussian Processes for Machine Learning},
	Year = {2006}}

@inproceedings{arora:gen-equilibrium,
	Archiveprefix = {arXiv},
	Author = {Arora, S. and Ge, R. and Liang, Y. and Ma, T. and Zhang, Y.},
	Booktitle = {ICML},
	Date-Added = {2019-04-02 12:33:53 +0000},
	Date-Modified = {2019-04-02 12:33:53 +0000},
	Eprint = {1703.00573},
	Title = {Generalization and Equilibrium in Generative Adversarial Nets ({GAN}s)},
	Year = {2017}}

@inproceedings{gans,
	Archiveprefix = {arXiv},
	Author = {Goodfellow, I. and Pouget-Abadie, J. and Mirza, M. and Xu, B. and Warde-Farley, D. and Ozair, S. and Courville, A. and Bengio, Y.},
	Booktitle = {NIPS},
	Date-Added = {2019-04-02 12:33:53 +0000},
	Date-Modified = {2019-04-02 12:33:53 +0000},
	Eprint = {1406.2661},
	Title = {Generative Adversarial Nets},
	Year = {2014}}

@inproceedings{opt-mmd,
	Archiveprefix = {arXiv},
	Author = {Sutherland, D. J. and Tung, H.-Y. and Strathmann, H. and De, S. and Ramdas, A. and Smola, A. and Gretton, A.},
	Booktitle = {ICLR},
	Date-Added = {2019-04-02 12:33:53 +0000},
	Date-Modified = {2019-04-02 12:33:53 +0000},
	Eprint = {1611.04488},
	Title = {Generative Models and Model Criticism via Optimized Maximum Mean Discrepancy},
	Year = {2017}}

@inproceedings{gmmn,
	Archiveprefix = {arXiv},
	Author = {Li, Y. and Swersky, K. and Zemel, R.},
	Booktitle = {ICML},
	Date-Added = {2019-04-02 12:33:53 +0000},
	Date-Modified = {2019-04-02 12:33:53 +0000},
	Eprint = {1502.02761},
	Title = {Generative Moment Matching Networks},
	Year = {2015}}

@misc{Bottou:2017,
	Archiveprefix = {arXiv},
	Author = {Leon Bottou and Martin Arjovsky and David Lopez-Paz and Maxime Oquab},
	Date-Added = {2019-04-02 12:33:53 +0000},
	Date-Modified = {2019-04-02 12:33:53 +0000},
	Eprint = {1712.07822},
	Title = {Geometrical Insights for Implicit Generative Modeling},
	Year = {2017}}

@article{lenet,
	Author = {LeCun, Y. and Bottou, L. and Bengio, Y. and Haffner, P.},
	Date-Added = {2019-04-02 12:33:53 +0000},
	Date-Modified = {2019-04-02 12:33:53 +0000},
	Journal = {Proceedings of the IEEE},
	Title = {Gradient-Based Learning Applied to Document Recognition},
	Year = {1998}}

@article{SriGreFukLanetal10,
	Archiveprefix = {arXiv},
	Author = {B. K. Sriperumbudur and A. Gretton and K. Fukumizu and G. R. G. Lanckriet and B. Sch{\"o}lkopf},
	Date-Added = {2019-04-02 12:33:53 +0000},
	Date-Modified = {2019-04-02 12:33:53 +0000},
	Eprint = {0907.5309},
	Journal = {JMLR},
	Pages = {1517--1561},
	Title = {Hilbert Space Embeddings and Metrics on Probability Measures},
	Volume = {11},
	Year = {2010}}

@misc{how-not-to-train,
	Archiveprefix = {arXiv},
	Author = {Husz\'ar, Ferenc},
	Date-Added = {2019-04-02 12:33:53 +0000},
	Date-Modified = {2019-04-02 12:33:53 +0000},
	Eprint = {1511.05101},
	Title = {How (not) to Train your Generative Model: Scheduled Sampling, Likelihood, Adversary?},
	Year = {2015}}

@misc{Russakovsky:2014,
	Archiveprefix = {arXiv},
	Author = {Russakovsky, Olga and Deng, Jia and Su, Hao and Krause, Jonathan and Satheesh, Sanjeev and Ma, Sean and Huang, Zhiheng and Karpathy, Andrej and Khosla, Aditya and Bernstein, Michael and Berg, Alexander C. and Fei-Fei, Li},
	Date-Added = {2019-04-02 12:33:53 +0000},
	Date-Modified = {2019-04-02 12:33:53 +0000},
	Eprint = {1409.0575},
	Title = {{ImageNet} Large Scale Visual Recognition Challenge},
	Year = {2014}}

@inproceedings{improved-gen-obj,
	Archiveprefix = {arXiv},
	Author = {Poole, Ben and Alemi, Alexander A. and Sohl-Dickstein, Jascha and Angelova, Anelia},
	Booktitle = {NIPS Workshop on Adversarial Training},
	Date-Added = {2019-04-02 12:33:53 +0000},
	Date-Modified = {2019-04-02 12:33:53 +0000},
	Eprint = {1612.02780},
	Title = {Improved generator objectives for {GAN}s},
	Year = {2016}}

@inproceedings{improved-gans,
	Archiveprefix = {arXiv},
	Author = {Salimans, T. and Goodfellow, I. and Zaremba, W. and Cheung, V. and Radford, A. and Chen, X.},
	Booktitle = {NIPS},
	Date-Added = {2019-04-02 12:33:53 +0000},
	Date-Modified = {2019-04-02 12:33:53 +0000},
	Eprint = {1606.03498},
	Title = {Improved Techniques for Training {GAN}s},
	Year = {2016}}

@inproceedings{wgan-gp,
	Archiveprefix = {arXiv},
	Author = {Gulrajani, I. and Ahmed, F. and Arjovsky, M. and Dumoulin, V. and Courville, A.},
	Booktitle = {NIPS},
	Date-Added = {2019-04-02 12:33:53 +0000},
	Date-Modified = {2019-04-02 12:33:53 +0000},
	Eprint = {1704.00028},
	Title = {Improved Training of {W}asserstein {GAN}s},
	Year = {2017}}

@article{Mueller97,
	Author = {A. M{\"u}ller},
	Date-Added = {2019-04-02 12:33:53 +0000},
	Date-Modified = {2019-04-02 12:33:53 +0000},
	Journal = {Advances in Applied Probability},
	Number = {2},
	Pages = {429--443},
	Title = {Integral Probability Metrics and their Generating Classes of Functions},
	Volume = {29},
	Year = {1997}}

@inproceedings{adversarial,
	Archiveprefix = {arXiv},
	Author = {Szegedy, C. and Zaremba, W. and Sutskever, I. and Bruna, J. and Erhan, D. and Goodfellow, I. and Fergus, R.},
	Booktitle = {ICLR},
	Date-Added = {2019-04-02 12:33:53 +0000},
	Date-Modified = {2019-04-02 12:33:53 +0000},
	Eprint = {1312.6199},
	Title = {Intriguing properties of neural networks},
	Year = {2014}}

@inproceedings{kernel-choice-mmd,
	Author = {Sriperumbudur, B. K. and Fukumizu, K. and Gretton, A. and Lanckriet, G. R. G. and Sch{\"{o}}lkopf, B.},
	Booktitle = {NIPS},
	Date-Added = {2019-04-02 12:33:53 +0000},
	Date-Modified = {2019-04-02 12:33:53 +0000},
	Title = {Kernel choice and classifiability for {RKHS} embeddings of probability distributions},
	Year = {2009}}

@book{shawe-taylor-christianini,
	Author = {Shawe-Taylor, John and Cristianini, Nello},
	Date-Added = {2019-04-02 12:33:53 +0000},
	Date-Modified = {2019-04-02 12:33:53 +0000},
	Publisher = {Cambridge University Press},
	Title = {Kernel Methods for Pattern Analysis},
	Year = {2004}}

@inproceedings{sinkhorn-igm,
	Archiveprefix = {arXiv},
	Author = {Aude Genevay and Gabriel Peyr{\'e} and Marco Cuturi},
	Booktitle = {AISTATS},
	Date-Added = {2019-04-02 12:33:53 +0000},
	Date-Modified = {2019-04-02 12:33:53 +0000},
	Eprint = {1706.00292},
	Title = {Learning Generative Models with {S}inkhorn Divergences},
	Year = {2018}}

@misc{igms,
	Archiveprefix = {arXiv},
	Author = {Mohamed, Shakir and Lakshminarayanan, Balaji},
	Date-Added = {2019-04-02 12:33:53 +0000},
	Date-Modified = {2019-04-02 12:33:53 +0000},
	Eprint = {1610.03483},
	Title = {Learning in Implicit Generative Models},
	Year = {2016}}

@misc{cifar10,
	Author = {Krizhevsky, A.},
	Date-Added = {2019-04-02 12:33:53 +0000},
	Date-Modified = {2019-04-02 12:33:53 +0000},
	Title = {Learning Multiple Layers of Features from Tiny Images},
	Year = {2009}}

@misc{lsun,
	Archiveprefix = {arXiv},
	Author = {Yu, F. and Zhang, Y. and Song, S. and Seff, A. and Xiao, J.},
	Date-Added = {2019-04-02 12:33:53 +0000},
	Date-Modified = {2019-04-02 12:33:53 +0000},
	Eprint = {1506.03365},
	Title = {{LSUN}: Construction of a Large-scale Image Dataset using Deep Learning with Humans in the Loop},
	Year = {2015}}

@inproceedings{many-paths,
	Archiveprefix = {arXiv},
	Author = {W. Fedus and M. Rosca and B. Lakshminarayanan and A. M. Dai and S. Mohamed and I. Goodfellow},
	Booktitle = {ICLR},
	Date-Added = {2019-04-02 12:33:53 +0000},
	Date-Modified = {2019-04-02 12:33:53 +0000},
	Eprint = {1710.08446},
	Title = {Many Paths to Equilibrium: {GAN}s Do Not Need to Decrease a Divergence At Every Step},
	Year = {2018}}

@inproceedings{mcgan,
	Archiveprefix = {arXiv},
	Author = {Mroueh, Youssef and Sercu, Tom and Goel, V.},
	Booktitle = {ICML},
	Date-Added = {2019-04-02 12:33:53 +0000},
	Date-Modified = {2019-04-02 12:33:53 +0000},
	Eprint = {1702.08398},
	Title = {{McGan}: Mean and Covariance Feature Matching {GAN}},
	Year = {2017}}

@incollection{Bousquet:2004,
	Author = {Bousquet, Olivier and Chapelle, Olivier and Hein, Matthias},
	Booktitle = {NIPS},
	Date-Added = {2019-04-02 12:33:53 +0000},
	Date-Modified = {2019-04-02 12:33:53 +0000},
	Title = {Measure Based Regularization},
	Year = {2004}}

@inproceedings{mmd-gan,
	Archiveprefix = {arXiv},
	Author = {Li, C.-L. and Chang, W.-C. and Cheng, Y. and Yang, Y. and P{\'{o}}czos, B.},
	Booktitle = {NIPS},
	Date-Added = {2019-04-02 12:33:53 +0000},
	Date-Modified = {2019-04-02 12:33:53 +0000},
	Eprint = {1705.08584},
	Title = {{MMD GAN}: Towards Deeper Understanding of Moment Matching Network},
	Year = {2017}}

@misc{mnist,
	Author = {LeCun, Yann and Cortes, Corinna},
	Date-Added = {2019-04-02 12:33:53 +0000},
	Date-Modified = {2019-04-02 12:33:53 +0000},
	Howpublished = {http://yann.lecun.com/exdb/mnist/},
	Title = {{MNIST} handwritten digit database},
	Url = {http://yann.lecun.com/exdb/mnist/},
	Year = {2010},
	Bdsk-Url-1 = {http://yann.lecun.com/exdb/mnist/}}

@article{rosenbaum,
	Author = {Rosenbaum, S.},
	Date-Added = {2019-04-02 12:33:53 +0000},
	Date-Modified = {2019-04-02 12:33:53 +0000},
	Journal = {JRSS B},
	Pages = {405--408},
	Title = {Moments of a Truncated Bivariate Normal Distribution},
	Volume = {23},
	Year = {1961}}

@misc{munit,
	Archiveprefix = {arXiv},
	Arxivid = {1804.04732},
	Author = {Huang, Xun and Liu, Ming-Yu and Belongie, Serge and Kautz, Jan},
	Date-Added = {2019-04-02 12:33:53 +0000},
	Date-Modified = {2019-04-02 12:33:53 +0000},
	Eprint = {1804.04732},
	Title = {Multimodal Unsupervised Image-to-Image Translation},
	Year = {2018}}

@article{Brock:2016,
	Author = {Brock, Andrew and Lim, Theodore and Ritchie, J. M. and Weston, Nick},
	Date-Added = {2019-04-02 12:33:53 +0000},
	Date-Modified = {2019-04-02 12:33:53 +0000},
	Language = {en},
	Month = sep,
	Title = {Neural {Photo} {Editing} with {Introspective} {Adversarial} {Networks}},
	Url = {https://arxiv.org/abs/1609.07093},
	Urldate = {2018-06-19},
	Year = {2016},
	Bdsk-Url-1 = {https://arxiv.org/abs/1609.07093}}

@article{gan-tutorial,
	Archiveprefix = {arXiv},
	Author = {Goodfellow, I.},
	Date-Added = {2019-04-02 12:33:53 +0000},
	Date-Modified = {2019-04-02 12:33:53 +0000},
	Eprint = {1701.00160},
	Title = {{NIPS} 2016 Tutorial: Generative Adversarial Networks},
	Year = {2016}}

@misc{ipms-phi-clf,
	Archiveprefix = {arXiv},
	Author = {Sriperumbudur, B. K. and Fukumizu, K. and Gretton, A. and Sch{\"{o}}lkopf, B. and Lanckriet, G. R. G.},
	Date-Added = {2019-04-02 12:33:53 +0000},
	Date-Modified = {2019-04-02 12:33:53 +0000},
	Eprint = {0901.2698},
	Title = {On integral probability metrics, phi-divergences and binary classification},
	Year = {2009}}

@article{ipm-empirical-est,
	Author = {Sriperumbudur, B. K. and Fukumizu, K. and Gretton, A. and Sch{\"{o}}lkopf, B. and Lanckriet, G. R. G.},
	Date-Added = {2019-04-02 12:33:53 +0000},
	Date-Modified = {2019-04-02 12:33:53 +0000},
	Journal = {Electronic Journal of Statistics},
	Pages = {1550--1599},
	Title = {On the empirical estimation of integral probability metrics},
	Volume = {6},
	Year = {2012}}

@article{opt-est-probabilities,
	Archiveprefix = {arXiv},
	Author = {Sriperumbudur, Bharath},
	Date-Added = {2019-04-02 12:33:53 +0000},
	Date-Modified = {2019-04-02 12:33:53 +0000},
	Eprint = {1310.8240},
	Journal = {Bernoulli},
	Number = {3},
	Pages = {1839--1893},
	Title = {On the optimal estimation of probability mesaures in weak and strong topologies},
	Volume = {22},
	Year = {2016}}

@mastersthesis{energy-distance-gan,
	Author = {L. Liu},
	Date-Added = {2019-04-02 12:33:53 +0000},
	Date-Modified = {2019-04-02 12:33:53 +0000},
	Month = {April},
	School = {University of Princeton Senior Thesis},
	Title = {On the Two-Sample Statistic Approach to Generative Adversarial Networks},
	Url = {http://arks.princeton.edu/ark:/88435/dsp0179408079v},
	Year = {2017},
	Bdsk-Url-1 = {http://arks.princeton.edu/ark:/88435/dsp0179408079v}}

@incollection{Gretton:2012,
	Author = {Gretton, Arthur and Sejdinovic, Dino and Strathmann, Heiko and Balakrishnan, Sivaraman and Pontil, Massimiliano and Fukumizu, Kenji and Sriperumbudur, Bharath K.},
	Booktitle = {NIPS},
	Date-Added = {2019-04-02 12:33:53 +0000},
	Date-Modified = {2019-04-02 12:33:53 +0000},
	Title = {Optimal kernel choice for large-scale two-sample tests},
	Year = {2012}}

@book{Klenke:2008,
	Author = {Klenke, A.},
	Date-Added = {2019-04-02 12:33:53 +0000},
	Date-Modified = {2019-04-02 12:33:53 +0000},
	Publisher = {World Publishing Corporation},
	Title = {Probability Theory: A Comprehensive Course},
	Year = {2008}}

@inproceedings{progressive-growing,
	Archiveprefix = {arXiv},
	Arxivid = {1710.10196},
	Author = {Karras, Tero and Aila, Timo and Laine, Samuli and Lehtinen, Jaakko},
	Booktitle = {ICLR},
	Date-Added = {2019-04-02 12:33:53 +0000},
	Date-Modified = {2019-04-02 12:33:53 +0000},
	Eprint = {1710.10196},
	Title = {Progressive Growing of {GAN}s for Improved Quality, Stability, and Variation},
	Year = {2018}}

@book{dudley:analysis,
	Author = {Richard M. Dudley},
	Date-Added = {2019-04-02 12:33:53 +0000},
	Date-Modified = {2019-04-02 12:33:53 +0000},
	Edition = {2},
	Publisher = {Cambridge University Press},
	Title = {Real Analysis and Probability},
	Year = {2002},
	Bdsk-File-1 = {YnBsaXN0MDDUAQIDBAUGJCVYJHZlcnNpb25YJG9iamVjdHNZJGFyY2hpdmVyVCR0b3ASAAGGoKgHCBMUFRYaIVUkbnVsbNMJCgsMDxJXTlMua2V5c1pOUy5vYmplY3RzViRjbGFzc6INDoACgAOiEBGABIAFgAdccmVsYXRpdmVQYXRoWWFsaWFzRGF0YV8QKi4uLy4uLy4uL2xpdHRlcmF0dXJlL0R1ZGxleSgyMDAyYSlSZWFsLnBkZtIXCxgZV05TLmRhdGFPEQHwAAAAAAHwAAIAAAxNYWNpbnRvc2ggSEQAAAAAAAAAAAAAAAAAAADMMSECSCsAAAVWW7QVRHVkbGV5KDIwMDJhKVJlYWwucGRmAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAACBlUZ9huKgsAAAAAAAAAAAADAAIAAAkgAAAAAAAAAAAAAAAAAAAAC2xpdHRlcmF0dXJlAAAQAAgAAMwxEvIAAAARAAgAANhuKgsAAAABABgFVlu0BVZbfgVWWxwDOlmAAzpZfwACDfkAAgBhTWFjaW50b3NoIEhEOlVzZXJzOgBNaWNoYWVsQXJiZWw6AERvY3VtZW50czoAR2F0c2J5OgBSZXNlYXJjaDoAbGl0dGVyYXR1cmU6AER1ZGxleSgyMDAyYSlSZWFsLnBkZgAADgAsABUARAB1AGQAbABlAHkAKAAyADAAMAAyAGEAKQBSAGUAYQBsAC4AcABkAGYADwAaAAwATQBhAGMAaQBuAHQAbwBzAGgAIABIAEQAEgBOVXNlcnMvTWljaGFlbEFyYmVsL0RvY3VtZW50cy9HYXRzYnkvUmVzZWFyY2gvbGl0dGVyYXR1cmUvRHVkbGV5KDIwMDJhKVJlYWwucGRmABMAAS8AABUAAgAT//8AAIAG0hscHR5aJGNsYXNzbmFtZVgkY2xhc3Nlc11OU011dGFibGVEYXRhox0fIFZOU0RhdGFYTlNPYmplY3TSGxwiI1xOU0RpY3Rpb25hcnmiIiBfEA9OU0tleWVkQXJjaGl2ZXLRJidUcm9vdIABAAgAEQAaACMALQAyADcAQABGAE0AVQBgAGcAagBsAG4AcQBzAHUAdwCEAI4AuwDAAMgCvAK+AsMCzgLXAuUC6QLwAvkC/gMLAw4DIAMjAygAAAAAAAACAQAAAAAAAAAoAAAAAAAAAAAAAAAAAAADKg==}}

@inproceedings{inception,
	Archiveprefix = {arXiv},
	Author = {Szegedy, C. and Vanhoucke, V. and Ioffe, S. and Shlens, J. and Wojna, Z.},
	Booktitle = {CVPR},
	Date-Added = {2019-04-02 12:33:53 +0000},
	Date-Modified = {2019-04-02 12:33:53 +0000},
	Eprint = {1512.00567},
	Title = {Rethinking the {I}nception Architecture for Computer Vision},
	Year = {2016}}

@inproceedings{c2st,
	Archiveprefix = {arXiv},
	Author = {Lopez-Paz, D. and Oquab, M.},
	Booktitle = {ICLR},
	Date-Added = {2019-04-02 12:33:53 +0000},
	Date-Modified = {2019-04-02 12:33:53 +0000},
	Eprint = {1610.06545},
	Title = {Revisiting Classifier Two-Sample Tests},
	Year = {2017}}

@book{Wendland05,
	Address = {Cambridge, UK},
	Author = {H. Wendland},
	Date-Added = {2019-04-02 12:33:53 +0000},
	Date-Modified = {2019-04-02 12:33:53 +0000},
	Publisher = {Cambridge University Press},
	Title = {Scattered Data Approximation},
	Year = {2005}}

@article{scikit-learn,
	Author = {Pedregosa, F. and Varoquaux, G. and Gramfort, A. and Michel, V. and Thirion, B. and Grisel, O. and Blondel, M. and Prettenhofer, P. and Weiss, R. and Dubourg, V. and Vanderplas, J. and Passos, A. and Cournapeau, D. and Brucher, M. and Perrot, M. and Duchesnay, E.},
	Date-Added = {2019-04-02 12:33:53 +0000},
	Date-Modified = {2019-04-02 12:33:53 +0000},
	Journal = {JMLR},
	Pages = {2825--2830},
	Title = {Scikit-learn: Machine Learning in {P}ython},
	Volume = {12},
	Year = {2011}}

@misc{weed:wasserstein-rates,
	Archiveprefix = {arXiv},
	Author = {Jonathan Weed and Francis Bach},
	Date-Added = {2019-04-02 12:33:53 +0000},
	Date-Modified = {2019-04-02 12:33:53 +0000},
	Eprint = {1707.00087},
	Title = {Sharp asymptotic and finite-sample rates of convergence of empirical measures in Wasserstein distance},
	Year = {2017}}

@misc{sobolev-descent,
	Author = {Mroueh, Youssef and Sercu, Tom and Raj, Anant},
	Date-Added = {2019-04-02 12:33:53 +0000},
	Date-Modified = {2019-04-02 12:33:53 +0000},
	Month = 4,
	Note = {Private communication},
	Title = {Sobolev Descent: Variational Transport of Distributions via Advection},
	Year = {2018}}

@inproceedings{sobolev-gan,
	Author = {Mroueh, Youssef and Li, Chung-Liang and Sercu, Tom and Raj, Anant and Cheng, Yu},
	Booktitle = {ICLR},
	Date-Added = {2019-04-02 12:33:53 +0000},
	Date-Modified = {2019-04-02 12:33:53 +0000},
	Title = {Sobolev GAN},
	Year = {2018}}

@inproceedings{Miyato:2018,
	Archiveprefix = {arXiv},
	Author = {Miyato, Takeru and Kataoka, Toshiki and Koyama, Masanori and Yoshida, Yuichi},
	Booktitle = {ICLR},
	Date-Added = {2019-04-02 12:33:53 +0000},
	Date-Modified = {2019-04-02 12:33:53 +0000},
	Eprint = {1802.05927},
	Title = {Spectral Normalization for Generative Adversarial Networks},
	Year = {2018}}

@inproceedings{roth:regularization,
	Archiveprefix = {arXiv},
	Author = {Roth, Kevin and Lucchi, Aurelien and Nowozin, Sebastian and Hofmann, Thomas},
	Booktitle = {NIPS},
	Date-Added = {2019-04-02 12:33:53 +0000},
	Date-Modified = {2019-04-02 12:33:53 +0000},
	Eprint = {1705.09367},
	Title = {Stabilizing Training of Generative Adversarial Networks through Regularization},
	Year = {2017}}

@inproceedings{stacked-gans,
	Archiveprefix = {arXiv},
	Author = {Huang, X. and Li, Y. and Poursaeed, O. and Hopcroft, J. and Belongie, S.},
	Booktitle = {CVPR},
	Date-Added = {2019-04-02 12:33:53 +0000},
	Date-Modified = {2019-04-02 12:33:53 +0000},
	Eprint = {1612.04357},
	Title = {Stacked Generative Adversarial Networks},
	Year = {2017}}

@article{GneRaf07,
	Author = {T. Gneiting and A. E. Raftery},
	Date-Added = {2019-04-02 12:33:53 +0000},
	Date-Modified = {2019-04-02 12:33:53 +0000},
	Journal = {JASA},
	Number = {477},
	Pages = {359--378},
	Title = {Strictly proper scoring rules, prediction, and estimation},
	Volume = {102},
	Year = {2007}}

@book{SteChr08,
	Author = {I. Steinwart and A. Christmann},
	Date-Added = {2019-04-02 12:33:53 +0000},
	Date-Modified = {2019-04-02 12:33:53 +0000},
	Publisher = {Springer},
	Series = {Information Science and Statistics},
	Title = {Support Vector Machines},
	Year = {2008}}

@article{Zahorski:1946,
	Author = {Zahorski, Z.},
	Date-Added = {2019-04-02 12:33:53 +0000},
	Date-Modified = {2019-04-02 12:33:53 +0000},
	Journal = {Bulletin de la Soci{\'e}t{\'e} math{\'e}matique de France},
	Pages = {147--178},
	Title = {Sur l'ensemble des points de non-d{\'e}rivabilit{\'e} d'une fonction continue},
	Urldate = {2018-03-13},
	Volume = {2},
	Year = {1946}}

@article{SzeRiz04,
	Author = {G. Sz\'{e}kely and M. Rizzo},
	Date-Added = {2019-04-02 12:33:53 +0000},
	Date-Modified = {2019-04-02 12:33:53 +0000},
	Journal = {InterStat},
	Title = {Testing for Equal Distributions in High Dimension},
	Volume = {5},
	Year = {2004}}

@article{energy-distance,
	Author = {G. J. Sz{\'e}kely and M. L. Rizzo},
	Date-Added = {2019-04-02 12:33:53 +0000},
	Date-Modified = {2019-04-02 12:33:53 +0000},
	Journal = {InterStat},
	Number = {5},
	Title = {Testing for equal distributions in high dimensions},
	Year = {2004}}

@inproceedings{kfda,
	Archiveprefix = {arXiv},
	Author = {Harchaoui, Zaid and Bach, Francis R. and Moulines, \'Eric},
	Booktitle = {NIPS},
	Date-Added = {2019-04-02 12:33:53 +0000},
	Date-Modified = {2019-04-02 12:33:53 +0000},
	Eprint = {0804.1026},
	Title = {Testing for Homogeneity with Kernel Fisher Discriminant Analysis},
	Year = {2008}}

@misc{cramer-gan,
	Archiveprefix = {arXiv},
	Author = {Bellemare, M. G. and Danihelka, I. and Dabney, W. and Mohamed, S. and Lakshminarayanan, B. and Hoyer, S. and Munos, R.},
	Date-Added = {2019-04-02 12:33:53 +0000},
	Date-Modified = {2019-04-02 12:33:53 +0000},
	Eprint = {1705.10743},
	Title = {The {C}ramer Distance as a Solution to Biased {W}asserstein Gradients},
	Year = {2017}}

@article{dowson-landau,
	Author = {Dowson, D. C. and Landau, B. V.},
	Date-Added = {2019-04-02 12:33:53 +0000},
	Date-Modified = {2019-04-02 12:33:53 +0000},
	Journal = {Journal of Multivariate Analysis},
	Number = {3},
	Pages = {450--455},
	Title = {The {F}r{\'{e}}chet distance between multivariate normal distributions},
	Volume = {12},
	Year = {1982}}

@article{Piranian:1966,
	Author = {Piranian, G.},
	Date-Added = {2019-04-02 12:33:53 +0000},
	Date-Modified = {2019-04-02 12:33:53 +0000},
	Journal = {The American Mathematical Monthly},
	Number = {4},
	Pages = {57--61},
	Title = {The {Set} of {Nondifferentiability} of a {Continuous} {Function}},
	Volume = {73},
	Year = {1966}}

@inproceedings{Zhang:2018,
	Archiveprefix = {arXiv},
	Author = {Zhang, Richard and Isola, Phillip and Efros, Alexei A. and Shechtman, Eli and Wang, Oliver},
	Booktitle = {CVPR},
	Date-Added = {2019-04-02 12:33:53 +0000},
	Date-Modified = {2019-04-02 12:33:53 +0000},
	Eprint = {1801.03924},
	Title = {The Unreasonable Effectiveness of Deep Features as a Perceptual Metric},
	Year = {2018}}

@misc{Mityagin:2015,
	Archiveprefix = {arXiv},
	Author = {B. Mityagin},
	Date-Added = {2019-04-02 12:33:53 +0000},
	Date-Modified = {2019-04-02 12:33:53 +0000},
	Eprint = {1512.07276},
	Title = {The Zero Set of a Real Analytic Function},
	Year = {2015}}

@inproceedings{towards-principled-gans,
	Archiveprefix = {arXiv},
	Author = {Arjovsky, M. and Bottou, L.},
	Booktitle = {ICLR},
	Date-Added = {2019-04-02 12:33:53 +0000},
	Date-Modified = {2019-04-02 12:33:53 +0000},
	Eprint = {1701.04862},
	Title = {Towards Principled Methods for Training Generative Adversarial Networks},
	Year = {2017}}

@misc{anime-gans,
	Archiveprefix = {arXiv},
	Author = {Y. Jin and K. Zhang and M. Li and Y. Tian and H. Zhu and Z. Fang},
	Date-Added = {2019-04-02 12:33:53 +0000},
	Date-Modified = {2019-04-02 12:33:53 +0000},
	Eprint = {1708.05509},
	Title = {Towards the Automatic Anime Characters Creation with Generative Adversarial Networks},
	Year = {2017}}

@inproceedings{gen-mmd,
	Archiveprefix = {arXiv},
	Author = {Dziugaite, G. K. and Roy, D. M. and Ghahramani, Z.},
	Booktitle = {UAI},
	Date-Added = {2019-04-02 12:33:53 +0000},
	Date-Modified = {2019-04-02 12:33:53 +0000},
	Eprint = {1505.03906},
	Title = {Training generative neural networks via Maximum Mean Discrepancy optimization},
	Year = {2015}}

@article{unbiased-convex,
	Author = {Bickel, P. J. and Lehmann, E. L.},
	Date-Added = {2019-04-02 12:33:53 +0000},
	Date-Modified = {2019-04-02 12:33:53 +0000},
	Journal = {The Annals of Mathematical Statistics},
	Number = {5},
	Pages = {1523--1535},
	Title = {Unbiased Estimation in Convex Families},
	Volume = {40},
	Year = {1969}}

@article{SriFukLan11,
	Archiveprefix = {arXiv},
	Author = {B. K. Sriperumbudur and K. Fukumizu and G. R. G. Lanckriet},
	Date-Added = {2019-04-02 12:33:53 +0000},
	Date-Modified = {2019-04-02 12:33:53 +0000},
	Eprint = {1003.0887},
	Journal = {JMLR},
	Pages = {2389--2410},
	Title = {Universality, Characteristic Kernels and {RKHS} Embedding of Measures},
	Volume = {12},
	Year = {2011}}

@inproceedings{cyclegan,
	Archiveprefix = {arXiv},
	Author = {J.-Y. Zhu and T. Park and P. Isola and A. A. Efros},
	Booktitle = {ICCV},
	Date-Added = {2019-04-02 12:33:53 +0000},
	Date-Modified = {2019-04-02 12:33:53 +0000},
	Eprint = {1703.10593},
	Title = {Unpaired Image-to-Image Translation using Cycle-Consistent Adversarial Networks},
	Year = {2017}}

@inproceedings{dcgan,
	Archiveprefix = {arXiv},
	Author = {Radford, A. and Metz, L. and Chintala, S.},
	Booktitle = {ICLR},
	Date-Added = {2019-04-02 12:33:53 +0000},
	Date-Modified = {2019-04-02 12:33:53 +0000},
	Eprint = {1511.06434},
	Title = {Unsupervised Representation Learning with Deep Convolutional Generative Adversarial Networks},
	Year = {2016}}

@book{diestel-uhl,
	Address = {Providence},
	Author = {Diestel, J. and Uhl, Jr, J. J.},
	Date-Added = {2019-04-02 12:33:53 +0000},
	Date-Modified = {2019-04-02 12:33:53 +0000},
	Publisher = {American Mathematical Society},
	Title = {Vector Measures},
	Year = {1977}}

@inproceedings{wgan,
	Archiveprefix = {arXiv},
	Author = {Arjovsky, M. and Chintala, S. and Bottou, L.},
	Booktitle = {ICML},
	Date-Added = {2019-04-02 12:33:53 +0000},
	Date-Modified = {2019-04-02 12:33:53 +0000},
	Eprint = {1701.07875},
	Title = {{W}asserstein Generative Adversarial Networks},
	Year = {2017}}

@misc{censored-normal,
	Author = {Sutherland, D. J.},
	Date-Added = {2019-04-02 12:33:53 +0000},
	Date-Modified = {2019-04-02 12:33:53 +0000},
	Howpublished = {Cross Validated answer},
	Title = {What are the mean and variance of a 0-censored multivariate normal?},
	Url = {https://stats.stackexchange.com/q/326347},
	Year = {2018},
	Bdsk-Url-1 = {https://stats.stackexchange.com/q/326347}}

@misc{Mescheder:2018,
	Archiveprefix = {arXiv},
	Author = {Mescheder, Lars and Geiger, Andreas and Nowozin, Sebastian},
	Date-Added = {2019-04-02 12:33:53 +0000},
	Date-Modified = {2019-04-02 12:33:53 +0000},
	Eprint = {1801.04406},
	Title = {Which Training Methods for {GANs} do actually Converge?},
	Year = {2018}}

@incollection{Villani:2004,
	Address = {Providence, Rhode Island},
	Author = {Villani, C{\'e}dric},
	Booktitle = {Contemporary {Mathematics}},
	Date-Added = {2019-04-02 12:33:53 +0000},
	Date-Modified = {2019-04-02 12:33:53 +0000},
	Doi = {10.1090/conm/353/06434},
	Editor = {Carvalho, M. C. and Rodrigues, J. F.},
	Isbn = {978-0-8218-3278-3 978-0-8218-7943-6},
	Language = {en},
	Pages = {95--109},
	Publisher = {American Mathematical Society},
	Title = {Trend to equilibrium for dissipative equations, functional inequalities and mass transportation},
	Url = {http://www.ams.org/conm/353/},
	Urldate = {2018-05-11},
	Volume = {353},
	Year = {2004},
	Bdsk-File-1 = {YnBsaXN0MDDUAQIDBAUGJCVYJHZlcnNpb25YJG9iamVjdHNZJGFyY2hpdmVyVCR0b3ASAAGGoKgHCBMUFRYaIVUkbnVsbNMJCgsMDxJXTlMua2V5c1pOUy5vYmplY3RzViRjbGFzc6INDoACgAOiEBGABIAFgAdccmVsYXRpdmVQYXRoWWFsaWFzRGF0YV8QLC4uLy4uLy4uL2xpdHRlcmF0dXJlL1ZpbGxhbmkoMjAwNGEpVHJlbmQucGRm0hcLGBlXTlMuZGF0YU8RAfgAAAAAAfgAAgAADE1hY2ludG9zaCBIRAAAAAAAAAAAAAAAAAAAAMwxIQJIKwAABVZbtBdWaWxsYW5pKDIwMDRhKVRyZW5kLnBkZgAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAHRpqA1xtMPQAAAAAAAAAAAAMAAgAACSAAAAAAAAAAAAAAAAAAAAALbGl0dGVyYXR1cmUAABAACAAAzDES8gAAABEACAAA1xs+LQAAAAEAGAVWW7QFVlt+BVZbHAM6WYADOll/AAIN+QACAGNNYWNpbnRvc2ggSEQ6VXNlcnM6AE1pY2hhZWxBcmJlbDoARG9jdW1lbnRzOgBHYXRzYnk6AFJlc2VhcmNoOgBsaXR0ZXJhdHVyZToAVmlsbGFuaSgyMDA0YSlUcmVuZC5wZGYAAA4AMAAXAFYAaQBsAGwAYQBuAGkAKAAyADAAMAA0AGEAKQBUAHIAZQBuAGQALgBwAGQAZgAPABoADABNAGEAYwBpAG4AdABvAHMAaAAgAEgARAASAFBVc2Vycy9NaWNoYWVsQXJiZWwvRG9jdW1lbnRzL0dhdHNieS9SZXNlYXJjaC9saXR0ZXJhdHVyZS9WaWxsYW5pKDIwMDRhKVRyZW5kLnBkZgATAAEvAAAVAAIAE///AACABtIbHB0eWiRjbGFzc25hbWVYJGNsYXNzZXNdTlNNdXRhYmxlRGF0YaMdHyBWTlNEYXRhWE5TT2JqZWN00hscIiNcTlNEaWN0aW9uYXJ5oiIgXxAPTlNLZXllZEFyY2hpdmVy0SYnVHJvb3SAAQAIABEAGgAjAC0AMgA3AEAARgBNAFUAYABnAGoAbABuAHEAcwB1AHcAhACOAL0AwgDKAsYCyALNAtgC4QLvAvMC+gMDAwgDFQMYAyoDLQMyAAAAAAAAAgEAAAAAAAAAKAAAAAAAAAAAAAAAAAAAAzQ=},
	Bdsk-Url-1 = {http://www.ams.org/conm/353/},
	Bdsk-Url-2 = {https://doi.org/10.1090/conm/353/06434}}

@techreport{Villani:2009,
	Author = {Cedric Villani},
	Date-Added = {2019-04-02 12:33:53 +0000},
	Date-Modified = {2019-04-02 12:33:53 +0000},
	Title = {Optimal Transport: Old and New},
	Year = {2009},
	Bdsk-File-1 = {YnBsaXN0MDDUAQIDBAUGJCVYJHZlcnNpb25YJG9iamVjdHNZJGFyY2hpdmVyVCR0b3ASAAGGoKgHCBMUFRYaIVUkbnVsbNMJCgsMDxJXTlMua2V5c1pOUy5vYmplY3RzViRjbGFzc6INDoACgAOiEBGABIAFgAdccmVsYXRpdmVQYXRoWWFsaWFzRGF0YV8QLi4uLy4uLy4uL2xpdHRlcmF0dXJlL1ZpbGxhbmkoMjAwOWEpT3B0aW1hbC5wZGbSFwsYGVdOUy5kYXRhTxECAAAAAAACAAACAAAMTWFjaW50b3NoIEhEAAAAAAAAAAAAAAAAAAAAzDEhAkgrAAAFVlu0GVZpbGxhbmkoMjAwOWEpT3B0aW1hbC5wZGYAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAeUGwjWjfVEUERGIAAAAAAAAwACAAAJIAAAAAAAAAAAAAAAAAAAAAtsaXR0ZXJhdHVyZQAAEAAIAADMMRLyAAAAEQAIAADWjfVEAAAAAQAYBVZbtAVWW34FVlscAzpZgAM6WX8AAg35AAIAZU1hY2ludG9zaCBIRDpVc2VyczoATWljaGFlbEFyYmVsOgBEb2N1bWVudHM6AEdhdHNieToAUmVzZWFyY2g6AGxpdHRlcmF0dXJlOgBWaWxsYW5pKDIwMDlhKU9wdGltYWwucGRmAAAOADQAGQBWAGkAbABsAGEAbgBpACgAMgAwADAAOQBhACkATwBwAHQAaQBtAGEAbAAuAHAAZABmAA8AGgAMAE0AYQBjAGkAbgB0AG8AcwBoACAASABEABIAUlVzZXJzL01pY2hhZWxBcmJlbC9Eb2N1bWVudHMvR2F0c2J5L1Jlc2VhcmNoL2xpdHRlcmF0dXJlL1ZpbGxhbmkoMjAwOWEpT3B0aW1hbC5wZGYAEwABLwAAFQACABP//wAAgAbSGxwdHlokY2xhc3NuYW1lWCRjbGFzc2VzXU5TTXV0YWJsZURhdGGjHR8gVk5TRGF0YVhOU09iamVjdNIbHCIjXE5TRGljdGlvbmFyeaIiIF8QD05TS2V5ZWRBcmNoaXZlctEmJ1Ryb290gAEACAARABoAIwAtADIANwBAAEYATQBVAGAAZwBqAGwAbgBxAHMAdQB3AIQAjgC/AMQAzALQAtIC1wLiAusC+QL9AwQDDQMSAx8DIgM0AzcDPAAAAAAAAAIBAAAAAAAAACgAAAAAAAAAAAAAAAAAAAM+}}

@article{Oberman:2008,
	Author = {Adam M. Oberman},
	Date-Added = {2019-04-02 12:33:53 +0000},
	Date-Modified = {2019-04-02 12:33:53 +0000},
	Journal = {Proceedings of the American Mathematical Society},
	Title = {An Explicit Solution of the Lipschitz Extension Problem},
	Year = {2008},
	Bdsk-File-1 = {YnBsaXN0MDDUAQIDBAUGJCVYJHZlcnNpb25YJG9iamVjdHNZJGFyY2hpdmVyVCR0b3ASAAGGoKgHCBMUFRYaIVUkbnVsbNMJCgsMDxJXTlMua2V5c1pOUy5vYmplY3RzViRjbGFzc6INDoACgAOiEBGABIAFgAdccmVsYXRpdmVQYXRoWWFsaWFzRGF0YV8QMi4uLy4uLy4uL2xpdHRlcmF0dXJlL09iZXJtYW4oMjAwOGEpQW4gRXhwbGljaXQucGRm0hcLGBlXTlMuZGF0YU8RAhAAAAAAAhAAAgAADE1hY2ludG9zaCBIRAAAAAAAAAAAAAAAAAAAAMwxIQJIKwAABVZbtB1PYmVybWFuKDIwMDhhKUFuIEV4cGxpY2l0LnBkZgAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAH5O8V2AvaEAAAAAAAAAAAAAMAAgAACSAAAAAAAAAAAAAAAAAAAAALbGl0dGVyYXR1cmUAABAACAAAzDES8gAAABEACAAA2AvaEAAAAAEAGAVWW7QFVlt+BVZbHAM6WYADOll/AAIN+QACAGlNYWNpbnRvc2ggSEQ6VXNlcnM6AE1pY2hhZWxBcmJlbDoARG9jdW1lbnRzOgBHYXRzYnk6AFJlc2VhcmNoOgBsaXR0ZXJhdHVyZToAT2Jlcm1hbigyMDA4YSlBbiBFeHBsaWNpdC5wZGYAAA4APAAdAE8AYgBlAHIAbQBhAG4AKAAyADAAMAA4AGEAKQBBAG4AIABFAHgAcABsAGkAYwBpAHQALgBwAGQAZgAPABoADABNAGEAYwBpAG4AdABvAHMAaAAgAEgARAASAFZVc2Vycy9NaWNoYWVsQXJiZWwvRG9jdW1lbnRzL0dhdHNieS9SZXNlYXJjaC9saXR0ZXJhdHVyZS9PYmVybWFuKDIwMDhhKUFuIEV4cGxpY2l0LnBkZgATAAEvAAAVAAIAE///AACABtIbHB0eWiRjbGFzc25hbWVYJGNsYXNzZXNdTlNNdXRhYmxlRGF0YaMdHyBWTlNEYXRhWE5TT2JqZWN00hscIiNcTlNEaWN0aW9uYXJ5oiIgXxAPTlNLZXllZEFyY2hpdmVy0SYnVHJvb3SAAQAIABEAGgAjAC0AMgA3AEAARgBNAFUAYABnAGoAbABuAHEAcwB1AHcAhACOAMMAyADQAuQC5gLrAvYC/wMNAxEDGAMhAyYDMwM2A0gDSwNQAAAAAAAAAgEAAAAAAAAAKAAAAAAAAAAAAAAAAAAAA1I=}}

@article{Saidi:2009,
	Abstract = {Let \$G\$ be a compact subgroup of \$GL\_n({\textbackslash}R)\$ acting linearly on a finite dimensional vector space \$E\$. B. Malgrange has shown that the space \${\textbackslash}mathcal\{C\}{\textasciicircum}{\textbackslash}infty({\textbackslash}R{\textasciicircum}n,E){\textasciicircum}G\$ of \${\textbackslash}mathcal\{C\}{\textasciicircum}{\textbackslash}infty\$ and \$G\$-covariant functions is a finite module over the ring \${\textbackslash}mathcal\{C\}{\textasciicircum}{\textbackslash}infty({\textbackslash}R{\textasciicircum}n){\textasciicircum}G\$ of \${\textbackslash}mathcal\{C\}{\textasciicircum}{\textbackslash}infty\$ and \$G\$-invariant functions. First, we generalize this result for the Schwartz space \${\textbackslash}mathscr\{S\}({\textbackslash}R{\textasciicircum}n,E){\textasciicircum}G\$ of \$G\$-covariant functions. Secondly, we prove that any \$G\$-covariant distribution can be decomposed into a sum of \$G\$-invariant distributions multiplied with a fixed family of \$G\$-covariant polynomials. This gives a generalization of an Oksak result proved in ([O]).},
	Author = {Saidi, Anouar},
	Date-Added = {2019-04-02 12:33:53 +0000},
	Date-Modified = {2019-04-02 12:33:53 +0000},
	Journal = {arXiv:0902.1383 [math]},
	Keywords = {46F05, 58C46., 58C81, 58C99, Mathematics - Representation Theory},
	Month = feb,
	Note = {arXiv: 0902.1383},
	Title = {On covariant functions and distributions under the action of a compact group},
	Url = {http://arxiv.org/abs/0902.1383},
	Urldate = {2018-11-08},
	Year = {2009},
	Bdsk-File-1 = {YnBsaXN0MDDUAQIDBAUGJCVYJHZlcnNpb25YJG9iamVjdHNZJGFyY2hpdmVyVCR0b3ASAAGGoKgHCBMUFRYaIVUkbnVsbNMJCgsMDxJXTlMua2V5c1pOUy5vYmplY3RzViRjbGFzc6INDoACgAOiEBGABIAFgAdccmVsYXRpdmVQYXRoWWFsaWFzRGF0YV8QMS4uLy4uLy4uL2xpdHRlcmF0dXJlL1NhaWRpKDIwMDlhKU9uIGNvdmFyaWFudC5wZGbSFwsYGVdOUy5kYXRhTxECDAAAAAACDAACAAAMTWFjaW50b3NoIEhEAAAAAAAAAAAAAAAAAAAAzDEhAkgrAAAFVlu0HFNhaWRpKDIwMDlhKU9uIGNvdmFyaWFudC5wZGYAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAfkBcjYCkm8AAAAAAAAAAAAAwACAAAJIAAAAAAAAAAAAAAAAAAAAAtsaXR0ZXJhdHVyZQAAEAAIAADMMRLyAAAAEQAIAADYCkm8AAAAAQAYBVZbtAVWW34FVlscAzpZgAM6WX8AAg35AAIAaE1hY2ludG9zaCBIRDpVc2VyczoATWljaGFlbEFyYmVsOgBEb2N1bWVudHM6AEdhdHNieToAUmVzZWFyY2g6AGxpdHRlcmF0dXJlOgBTYWlkaSgyMDA5YSlPbiBjb3ZhcmlhbnQucGRmAA4AOgAcAFMAYQBpAGQAaQAoADIAMAAwADkAYQApAE8AbgAgAGMAbwB2AGEAcgBpAGEAbgB0AC4AcABkAGYADwAaAAwATQBhAGMAaQBuAHQAbwBzAGgAIABIAEQAEgBVVXNlcnMvTWljaGFlbEFyYmVsL0RvY3VtZW50cy9HYXRzYnkvUmVzZWFyY2gvbGl0dGVyYXR1cmUvU2FpZGkoMjAwOWEpT24gY292YXJpYW50LnBkZgAAEwABLwAAFQACABP//wAAgAbSGxwdHlokY2xhc3NuYW1lWCRjbGFzc2VzXU5TTXV0YWJsZURhdGGjHR8gVk5TRGF0YVhOU09iamVjdNIbHCIjXE5TRGljdGlvbmFyeaIiIF8QD05TS2V5ZWRBcmNoaXZlctEmJ1Ryb290gAEACAARABoAIwAtADIANwBAAEYATQBVAGAAZwBqAGwAbgBxAHMAdQB3AIQAjgDCAMcAzwLfAuEC5gLxAvoDCAMMAxMDHAMhAy4DMQNDA0YDSwAAAAAAAAIBAAAAAAAAACgAAAAAAAAAAAAAAAAAAANN}}

@article{Jourdain:2007,
	Author = {Jourdain, Benjamin and M{\'e}l{\'e}ard, Sylvie and Woyczynski, Wojbor},
	Date-Added = {2019-04-02 12:33:53 +0000},
	Date-Modified = {2019-04-02 12:33:53 +0000},
	Journal = {Latin American Journal of Probability},
	Keywords = {60K35, 35S10, 65C35, Mathematics - Probability},
	Month = jul,
	Note = {arXiv: 0707.2723},
	Title = {Nonlinear {SDEs} driven by {L}{\textbackslash}'evy processes and related {PDEs}},
	Url = {http://arxiv.org/abs/0707.2723},
	Urldate = {2018-05-10},
	Year = {2007},
	Bdsk-Url-1 = {http://arxiv.org/abs/0707.2723}}

@article{Retherford:1978,
	Author = {Retherford, J. R.},
	Date-Added = {2019-04-02 12:33:53 +0000},
	Date-Modified = {2019-04-02 12:33:53 +0000},
	Fjournal = {Bulletin of the American Mathematical Society},
	Journal = {Bull. Amer. Math. Soc.},
	Month = {07},
	Number = {4},
	Pages = {681--685},
	Publisher = {American Mathematical Society},
	Title = {Review: J. Diestel and J. J. Uhl, Jr., Vector measures},
	Volume = {84},
	Year = {1978}}

@article{durmus2018analysis,
	Author = {Durmus, Alain and Majewski, Szymon and Miasojedow, B{\l}a{\.z}ej},
	Journal = {arXiv preprint arXiv:1802.09188},
	Title = {Analysis of Langevin Monte Carlo via convex optimization},
	Year = {2018}}

@article{bernton2018langevin,
	Author = {Bernton, Espen},
	Journal = {arXiv preprint arXiv:1802.08671},
	Title = {Langevin Monte Carlo and JKO splitting},
	Year = {2018}}

@article{jordan1998variational,
	Author = {Jordan, Richard and Kinderlehrer, David and Otto, Felix},
	Journal = {SIAM journal on mathematical analysis},
	Number = {1},
	Pages = {1--17},
	Publisher = {SIAM},
	Title = {The variational formulation of the Fokker--Planck equation},
	Volume = {29},
	Year = {1998}}

@article{gretton2012kernel,
	Author = {Gretton, Arthur and Borgwardt, Karsten M and Rasch, Malte J and Sch{\"o}lkopf, Bernhard and Smola, Alexander},
	Journal = {Journal of Machine Learning Research},
	Number = {Mar},
	Pages = {723--773},
	Title = {A kernel two-sample test},
	Volume = {13},
	Year = {2012}}
