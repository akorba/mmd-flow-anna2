
\documentclass{article}
\usepackage{nips_2018,graphicx}

 \usepackage[english]{babel}   % "babel.sty" + "french.sty"
% \usepackage[english,francais]{babel} % "babel.sty"
% \usepackage{french}                  % "french.sty"
  \usepackage{times}			% ajout times le 30 mai 2003
 
%% --------------------------------------------------------------
%% CODAGE DE POLICES ?
%% Si votre moteur Latex est francise, il est conseille
%% d'utiliser le codage de police T1 pour faciliter la césure,
%% si vous disposez de ces polices (DC/EC)
\usepackage[T1]{fontenc}
\usepackage[utf8]{inputenc} 
\DeclareUnicodeCharacter{00A0}{~}

%% ==============================================================
% Packages divers (mathématiques, etc.)   
\usepackage{amssymb,amsmath,amscd,amsfonts,amsthm,bbm,mathrsfs,yhmath}
%\usepackage{enumerate}
\usepackage[shortlabels]{enumitem}
% \usepackage{showkeys}
\usepackage{hyperref}

% Example definitions.
% --------------------
\def\x{{\mathbf x}}
\def\L{{\cal L}}

\DeclareMathOperator{\var}{\mathbb Var}
\DeclareMathOperator{\cov}{cov}
\DeclareMathOperator{\rank}{rank}
\DeclareMathOperator{\dom}{dom}
\DeclareMathOperator{\zer}{zer}
\DeclareMathOperator{\aver}{av}
\DeclareMathOperator{\inter}{int}
\DeclareMathOperator{\relint}{ri}
\DeclareMathOperator{\epi}{epi}
\DeclareMathOperator{\graph}{gr}
\DeclareMathOperator{\prox}{prox}
\DeclareMathOperator{\tr}{tr}
\DeclareMathOperator{\support}{supp}
\DeclareMathOperator{\dist}{dist}
\DeclareMathOperator{\lev}{lev}
\DeclareMathOperator{\rec}{rec}
\DeclareMathOperator{\cl}{cl}
\DeclareMathOperator{\co}{co}
\DeclareMathOperator{\clo}{\overline co}
\DeclareMathOperator{\distC}{\mathsf d}
\DeclareMathOperator*{\diag}{diag}
\newcommand{\KL}{\mathop{\mathrm{KL}}\nolimits}


\newcommand{\leftnorm}{\left|\!\left|\!\left|}
\newcommand{\rightnorm}{\right|\!\right|\!\right|}

%\newcommand{\eqdef}{{\stackrel{\text{def}}{=}}} 
\newcommand{\eqdef}{:=} 

\newcommand{\1}{\mathbbm 1}
\newcommand{\bs}{\boldsymbol}

\newcommand{\itpx}{{\mathsf x}}
\newcommand{\sx}{{\mathsf x}}
\newcommand{\sy}{{\mathsf y}}
\newcommand{\sz}{{\mathsf z}}
\newcommand{\sw}{{\mathsf w}}
\newcommand{\sF}{{\mathsf F}}
\newcommand{\sH}{{\mathsf H}}

\newcommand{\ZZ}{\mathbb Z}
\newcommand{\CC}{\mathbb{C}}
\newcommand{\bP}{{{\mathbb P}}} 
\newcommand{\bE}{{{\mathbb E}}} 
\newcommand{\bV}{{{\mathbb V}}} 
\newcommand{\bN}{{{\mathbb N}}} 

% Operators, domains, etc.  
\newcommand{\mA}{{\mathcal A}} 
\newcommand{\mB}{{\mathcal B}} 
\newcommand{\mC}{{\mathcal C}} 
\newcommand{\mD}{{\mathcal D}} 
\newcommand{\mO}{{\mathcal O}} 
\newcommand{\mU}{{\mathcal U}}
\newcommand{\mX}{{\mathcal X}}
\newcommand{\mY}{{\mathcal Y}}
\newcommand{\mZ}{{\mathcal Z}} 
\newcommand{\bmD}{\cl({\mathcal D})} 

\newcommand{\sA}{{\mathsf A}}
\newcommand{\sB}{{\mathsf B}}
\newcommand{\sJ}{{\mathsf J}}
\newcommand{\sX}{{\mathsf X}}
\newcommand{\sG}{{\mathsf G}}
\newcommand{\sY}{{\mathsf Y}}

\newcommand{\maxmon}{{\mathscr M}} 
\newcommand{\Selec}{{\mathfrak S}} 

% Sigma fields
\newcommand{\mcA}{{\mathscr A}} 
\newcommand{\mcB}{{\mathscr B}} 
\newcommand{\mcN}{{\mathscr N}} 
\newcommand{\mcT}{{\mathscr T}} 
\newcommand{\mcI}{{\mathscr I}} 
\newcommand{\mcF}{{\mathscr F}} 
\newcommand{\mcG}{{\mathscr G}} 
\newcommand{\mcX}{{\mathscr X}} 
\newcommand{\cP}{{{\mathcal P}}} 
\newcommand{\cS}{{{\mathcal S}}} 
\newcommand{\cZ}{{{\mathcal Z}}} 
\newcommand{\cF}{{{\mathcal F}}} 
\newcommand{\cG}{{{\mathcal G}}} 
\newcommand{\cM}{{{\mathcal M}}} 
\newcommand{\cD}{{{\mathcal D}}} 
\newcommand{\cE}{{{\mathcal E}}} 
\newcommand{\cL}{{{\mathcal L}}}
\newcommand{\cT}{{{\mathcal T}}} 
\newcommand{\cN}{{{\mathcal N}}} 
\newcommand{\cK}{{{\mathcal K}}} 
\newcommand{\cI}{{{\mathcal I}}} 

% Spaces 
\newcommand{\R}{{{\mathbb R}}} 
\newcommand{\E}{{{\mathbb E}}} 
\newcommand{\kH}{{{\mathcal H}}} 
\newcommand{\F}{{{\mathcal F}}} 
\newcommand{\Hil}{E}                % Hilbert   
\newcommand{\Ban}{E}                % Banach   
\newcommand{\RN}{{{\mathbb R}^N}} 
\newcommand{\bR}{{{\mathbb R}}} 

\newcommand{\m}{\mathfrak{m}}
\newcommand{\toL}{\xrightarrow[]{{\mathcal L}}}
\newcommand{\toweak}{\xrightharpoonup[]{{\mathcal L}}}

\newcommand{\ps}[1]{\langle #1 \rangle}

% 
% Almost sure convergence
\newcommand{\toasshort}{\stackrel{\text{as}}{\to}}
\newcommand{\toaslong}{\xrightarrow[n\to\infty]{\text{a.s.}}}

% Convergence in probability 
\newcommand{\toprobashort}{\,\stackrel{\mathcal{P}}{\to}\,}
\newcommand{\toprobalong}{\xrightarrow[n\to\infty]{\mathcal P}}
%
% Convergence in law 
\newcommand{\todistshort}{{\stackrel{\mathcal{D}}{\to}}}
\newcommand{\todistlong}{\xrightarrow[n\to\infty]{\mathcal D}}

\usepackage[textwidth=2cm, textsize=footnotesize]{todonotes}  
\setlength{\marginparwidth}{1.5cm}               %  this goes with todonotes
\newcommand{\pbnote}[1]{\todo[color=cyan!20]{#1}}
\newcommand{\asnote}[1]{\todo[color=green!20]{#1}}
\newcommand{\whnote}[1]{\todo[color=magenta]{#1}}
\newcommand{\wh}[1]{{\color{red} #1}}

%Moreau
\newcommand{\my}{{{\nabla ^\gamma g}}}
\newcommand{\myn}{{{\nabla ^{\gamma_{n+1}} g}}}
%% ==============================================================

\theoremstyle{definition}
\newtheorem{theorem}{Theorem}
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{corollary}[theorem]{Corollary}
\newtheorem{proposition}[theorem]{Proposition}
\newtheorem{definition}{Definition}
\newtheorem{remark}{Remark}
\newtheorem{condition}{Condition}
\newtheorem{assumption}{Assumption}
\newtheorem{example}{Example}

\title{MMD Flow}

\begin{document}
\maketitle


\begin{abstract} 

\end{abstract}

\section{Notations}
Consider $\pi$ target distribution. Let $k$ a kernel and $\kH$ its RKHS. The MMD between two distributions $\rho,\pi$ is defined as:
\begin{equation}
    MMD(\rho,\pi)=\sup_{f \in \kH,  \|f\|_{\kH}\le 1} \E_{X \sim \rho}[f(X)]-\E_{Y \sim \pi}[f(Y)]=\|\E_{\rho}[k(X,.)] + \E_{\pi}[k(Y,.)]\|_{\kH}
\end{equation}
We will consider a flow $(\rho_t)_{t>0}$ and denote $f_t= \E_{\rho_t}[k(X,.)]- \E_{\pi}[k(Y,.)]$. In this case:
\begin{align}
MMD^2(\rho_t,\pi)&=\|f_t\|^2_{\kH}\\
&= \E_{\rho_t \otimes \rho_t}[k(X,X')]+\E_{\pi \otimes \pi}[k(Y,Y')] - 2\E_{\rho_t \otimes \pi}[k(X,Y)]
\end{align} 

\section{Introduction}
Endow the space of probability measures $\mathcal{P}(\Omega)$ on a domain $\Omega \subset \R^d$ with a distance (e.g, optimal transport distances), and then deal with gradient flows of suitable functionals on such a metric space.

\section{Background}

\textit{Lyapunov functional} (or "free energy" or "entropy") $\F$  \citep{villani2004trend}:
\begin{equation}
    \F(\rho)=\int U(\rho(x)) \rho(x)dx + \int V(x)\rho(x)dx + \int W(x,y)\rho(x)\rho(y)dxdy
\end{equation}
where  $U$ is the internal energy, $V$ the potential energy and $W$ the
interaction energy. The formal gradient flow equation associated to this functional can be written:
\begin{equation}
    \frac{\partial \rho}{\partial t}= div( \rho \nabla \frac{\partial \F}{\partial \rho}) \quad x \in \R^d , t>0
\end{equation}
And the dissipation of entropy is defined as %see  http://wwwf.imperial.ac.uk/~jcarrill/RICAM/CharlaRICAM2014-1.pdf
\begin{align}
    &        \frac{d \F(\rho)}{dt} =-D(\rho) \quad \text{ with } D(q)= \int |\xi|^2 \rho(x)dx\\
    &\text{ and } \xi= \nabla (U'(\rho) + V + W * \rho)= \nabla \frac{\partial \F}{\partial \rho}
\end{align}

\begin{remark} When $\F$ is the KL divergence $KL(\rho,\pi)=\int log(\frac{\rho(x)}{p(x)})\rho(x)dx$, $\F(\rho)=\int U(\rho(x))\rho(x)dx + \int V(x) \rho(x)dx$ with $U(\rho(x))=\rho(x)log(\rho(x))$ and $V(x)=-log(\pi(x))$. In this case, $\nabla \frac{\partial \F}{\partial \rho}= \nabla log(\frac{\rho}{\pi})$.
\end{remark}

\section{MMD flow}

We define the potential energy (also called confinement energy) $V$ and interaction energy $W$ as follows:
\begin{align}
    V(X)&=-\int 2 k(X,x')\pi(x')\\
    W(X,Y)&=k(X,Y)
\end{align}
We have $MMD^2(\rho,\pi)=C+ \int V(x) \rho(x)dx + \int W(x,x')\rho(x)\rho(x')$, where $C=\E_{\pi\otimes \pi}[k(Y,Y')]$. $MMD^2$ can thus be written as a \textit{Lyapunov functional} (or "free energy" or "entropy") $\F$.
In the case where $\F=MMD^2$:
\begin{align}
        \nabla \frac{\partial \F}{\partial \rho}&= \nabla \frac{\partial \|f_t\|^2_{\kH}}{\partial \rho_t}\\
        &=2 \nabla \langle \frac{\partial f_t}{\partial \rho_t}, f_t \rangle_{\kH}\\
        &=2 \nabla \langle \frac{\partial \E_{q_t}[k(X,.)]}{\partial \rho_t}, f_t \rangle_{\kH}\\
        &=2 \nabla \langle k(X,.), f_t \rangle_{\kH}\\
        &= 2 \nabla f_t(x)
\end{align}
where $\nabla f_t(Y)= \E_{X \sim \rho_t}[\nabla_{Y}k(X,Y)] -  \E_{X \sim \pi}[\nabla_{Y}k(X,Y)]$. So the dissipation of MMD is given by:  
\begin{equation}
    \frac{d MMD^2(\rho_t, p)}{dt}=-2 \E_{X \sim \rho_t}[\|\nabla f_t(X)\|^2]
\end{equation}

\begin{remark}
For $\F=KL$, we would obtain $-\E_{X \sim \rho_t}[\|\nabla log(\frac{\rho_t}{\pi}(X))\|^2]$
\end{remark}



\section{Algorithm}

The gradient flow of MMD can be written:
\begin{equation*}
\frac{\partial \rho}{\partial t}= 2 div(\rho  \nabla f_t)
\end{equation*}
which is the density of the stochastic process:
\begin{equation}\label{eq:stochastic_process}
dX_t=-2\nabla f_t(X_t) 
\end{equation}
It represents the position $X_t$ of a particle at time $t > 0$.


We can thus consider the Euler discretization of \eqref{eq:stochastic_process}:
\begin{equation}\label{eq:discretization}
X_{k+1}=X_k - \gamma_{k+1} \nabla f_k(X_k)
\end{equation}
here $\nabla f_k(X_k)= \E_{X \sim \rho_k}[\nabla_{X_k}k(X,X_k)] -  \E_{X \sim \pi}[\nabla_{X_k}k(X,X_k)]$.
In practice we estimate this gradient by:
\begin{equation*}
\widehat{\nabla f_k}(X_k)=\frac{1}{n}\sum_{i=1,\dots,n}\nabla_{X_k}k(x_i,X_k) - \frac{1}{n}\sum_{i=1,\dots,n}\nabla_{X_k}k(y_i,X_k)
\end{equation*}
where $(x_1, \dots, x_n)\sim \rho_k$ and $(y_1, \dots, y_n)\sim \pi$.

\section{Lambda displacement convexity of the MMD}


In this section, we investigate the theoretical properties of the MMD flow. Let $\mu$ and $\nu$ be two probability densities on $\mathbb{R}^{d}$,
we would like to show that $\nu\mapsto MMD^{2}(\mu,\nu)$ is a convex
functional for $\nu$ that are close enough to $\mu$ in the Wasserstein
sense. Here the notion of convexity should be understood in the following
sense:
\begin{definition}
(displacement convexity \cite{Villani:2004} Definition 1 ). Let $\mu$
and $\nu$ be two probabilities densities. There exists a $\mu-a.e.$
unique gradient of a convex function ,$\nabla\phi$, such that $\nu$
is equal to $\nabla\phi_{\#}\mu$ and one can define $\rho_{t}=((1-t)Id+t\nabla\phi)_{\#}\mu$
for $0\leq t\leq1$. We say that a functional $\nu\mapsto\mathcal{F}(\nu)$
is displacement convex if 
\[
t\mapsto\mathcal{F}(\rho_{t})
\]
 is convex for any $\mu$ and $\nu$.Moreover, we say that $\mathcal{F}$
is convex in a neighborhood of $\mu$ if there exists a radius $r>0$
such that the above property holds for any $\nu$ with $W_{2}(\mu,\nu)\leq r$
.
\end{definition}
%
\begin{definition}
($\Lambda$-convexity \cite{Villani:2009} Definition 16.4). Let $(\mu,v)\mapsto\Lambda(\mu,v)$
be a function that defines for each probability distribution $\mu$
a quadratic form on the set of square integrable vectors valued functions
$v$ , i.e: $v\in L_{2}(\mathbb{R}^{d},\mathbb{R}^{d},\mu)$ . We
further assume that:
\[
\inf_{\mu,v}\frac{\Lambda(\mu,v)}{\Vert v\Vert_{L_{2}(\mu)}^{2}}>-\infty.
\]

We say that a functional $\mu\mapsto\mathcal{F}(\mu)$ is $\Lambda$-convex
if for any $\mu$ and $\nu$ and a minimizing geodesic $\text{\ensuremath{\rho_{t}}}$
between $\mu$ and $\nu$ with velocity vector field $v_{t}$, i.e:
$\partial_{t}\rho_{t}+div(\rho_{t}v_{t})=0;\rho_{0}=\mu;\rho_{1}=\nu$
the following holds:
\end{definition}
\[
\frac{d^{2}\mathcal{F}(\rho_{t})}{dt^{2}}\geq\Lambda(\rho_{t},v_{t})\qquad\forall t\in[0,1].
\]

\begin{proposition}
\label{prop:lambda_convexity}$\nu\mapsto MMD^{2}(\mu,\nu)$ is $\text{\ensuremath{\Lambda_{\mu}}}$-convex
with $\Lambda_{\mu}$ given by:
\begin{equation}
\Lambda_{\mu}(\rho,v)=\langle v,(C_{\rho}-\lambda MMD(\mu,\rho)I)v\rangle_{L_{2}(\rho)}\label{eq:Lambda}
\end{equation}

where $C_{\rho}$ is the operator defined by:
\[
(C_{\rho}v)(x)=\int\nabla_{x}\nabla_{x'}k(x,x')v(x')d\rho(x')
\]

and $\lambda=\sup_{x,x'\in\mathbb{\mathbb{R}^{d}}}\Vert Hk(x,x')\Vert_{op}$
where $Hk(x,x')$ is an $\mathbb{R}^{d^{2}}\times\mathbb{R}^{d^{2}}$
matrix whose entries are given by $\partial_{x_{i}}\partial_{x_{j}}\partial_{x'_{i}}\partial_{x'j}k(x,x')$.
\end{proposition}
%
\begin{proof}
To prove that $\nu\mapsto MMD^{2}(\mu,\nu)$ $\Lambda_{\mu}$-convex
we need to compute the second derivative $\frac{d^{2}}{dt^{2}}MMD^{2}(\mu,\rho_{t})$
where $\rho_{t}$ is a minimizing geodesic between two probability
distributions $\nu_{0}$ and $\nu_{1}$. When $\nu_{0}$ and $\nu_{1}$
both have a density, there exists a convex function such that $\rho_{t}=(1-t)Id+t\nabla\phi)_{\#}\nu_{0}:=(\pi_{t})_{\#}\nu_{0}$
.We start by computing the first derivative:
\[
\frac{dMMD^{2}(\mu,\rho_{t})}{dt}=2\langle f_{t},\frac{df_{t}}{dt}\rangle_{\mathcal{H}}
\]
where $f_{t}=\rho_{t}(k(x,.))-\mu(k(x,.))$. Using the definition
of $\rho_{t}=(1-t)Id+t\nabla\phi)_{\#}\nu_0$ it follows that:
\[
\frac{df_{t}}{dt}=\int(\nabla\phi(x)-x).\nabla k(\pi_{t}(x),.)\nu_{0}(x)dx
\]
hence:
\[
\frac{dMMD^{2}(\mu,\rho_{t})}{dt}=2\int(\nabla\phi(x)-x).\nabla f_{t}(\pi_{t}(x))\nu_{0}(x)dx
\]
Now the second derivative is given by:
\begin{align*}
\frac{d^{2}MMD^{2}(\mu,\rho_{t})}{dt^{2}}= & \int(\nabla\phi(x)-x).Hf_{t}(\pi_{t}(x))(\nabla\phi(x)-x)\nu_{0}(x)dx\\
 & +\int(\nabla\phi(x)-x).\nabla_{1}\nabla_{2}k(\pi_{t}(x),\pi_{t}(x'))(\nabla\phi(x')-x')\nu_{0}(x)\nu_{0}(x')dxdx'
\end{align*}
Here $\nabla_{1}\nabla_{2}k(x,x')$ is the matrix whose components
are given by $\langle\partial_{i}k(x,.),\partial_{j}k(x,.)\rangle$
for $1\leq i,j\leq d$, and $Hf_{t}$ is the hesssian of $f_{t}$
and its components are also given by:
\[
(Hf_{t}(x))_{i,j}=\langle f_{t},\partial_{i}\partial_{j}k(x,.)\rangle.
\]
Denoting by $h(x):=\nabla\phi(x)-x$ it follows that:
\begin{align*}
\frac{d^{2}MMD^{2}(\mu,\rho_{t})}{dt^{2}}= & \langle f_{t},\int\sum_{i,j}h_{i}(x)h_{j}(x)\partial_{i}\partial_{j}k(\pi_{t}(x),.)\nu_{0}(x)dx\rangle\\
 & +\Vert\int\sum_{i}h_{i}(x)\partial_{i}k(\pi_{t}(x),.)\nu_{0}(x)dx\Vert^{2}
\end{align*}
Now we use Cauchy-Schwarz inequality for the first term to get:
\begin{align*}
\frac{d^{2}MMD^{2}(\mu,\rho_{t})}{dt^{2}}\geq & -\Vert f_{t}\Vert_{\mathcal{H}}\Vert\int\sum_{i,j}h_{i}(x)h_{j}(x)\partial_{i}\partial_{j}k(\pi_{t}(x),.)\nu_{0}(x)dx\Vert_{\mathcal{H}}\\
 & +\Vert\int\sum_{i}h_{i}(x)\partial_{i}k(\pi_{t}(x),.)\nu_{0}(x)dx\Vert^{2}.
\end{align*}
After aplying a change of variables $x=\pi_{t}(y)$ one recovers the
velocity vector $v_{t}$ instead of $h$: 
\begin{align*}
\frac{d^{2}MMD^{2}(\mu,\rho_{t})}{dt^{2}}\geq & -\Vert f_{t}\Vert_{\mathcal{H}}\Vert\int\sum_{i,j}v_{t}^{i}(x)v_{t}^{j}(x)\partial_{i}\partial_{j}k(x,.)\rho_{t}(x)dx\Vert_{\mathcal{H}}\\
 & +\Vert\int\sum_{i}v_{t}^{i}(x)\partial_{i}k(x,.)\rho_{t}(x)dx\Vert^{2}.
\end{align*}

One can further note that:
\[
\Vert\int\sum_{i,j}v_{t}^{i}(x)v_{t}^{j}(x)\partial_{i}\partial_{j}k(x,.)\rho_{t}(x)dx\Vert_{\mathcal{H}}\leq\lambda\Vert v_{t}\Vert_{L_{2}(\rho_{t})}^{2}
\]

and that 
\begin{align*}
\Vert\int\sum_{i}v_{t}^{i}(x)\partial_{i}k(x,.)\rho_{t}(x)dx\Vert^{2} & =\int v_{t}(x)^{T}\int\nabla_{1}\nabla_{2}k(x,x')v_{t}(x')\rho_{t}(x')dx'dx.\\
 & =\langle v_{t},C_{\rho_{t}}v_{t}\rangle_{L_{2}(\rho_{t})}
\end{align*}

Hence we have shown that 
\[
\frac{d^{2}MMD^{2}(\mu,\rho_{t})}{dt^{2}}\geq\langle v_{t},(C_{\rho_{t}}-\lambda MMD(\mu,\rho_{t})I)v_{t}\rangle_{L_{2}(\rho_{t})}=\Lambda_{\mu}(\rho_{t},v_{t})
\]
\end{proof}
%
It is worth noting that $\nu_{0}=\mu$ and at time $t=0$ we have
that $MMD(\mu,\rho_{0})=0$ and hence we get:
\[
\frac{d^{2}MMD^{2}(\mu,\rho_{t})}{dt^{2}}\vert_{t=0}=\langle v_{t},C_{\rho_{t}}v_{t}\rangle_{L_{2}(\rho_{t})}\geq0.
\]
This shows that $\nu\mapsto MMD^{2}(\mu,\nu)$ has a non-negative
hessian at $\mu$ which is not surprising since $\mu$ is the global
minimum of this functional.
\begin{corollary}
For any geodesic $\rho_{t}$ between two probability distributions
$\rho_{0}$ and $\rho_{1}$ the following holds:
\begin{equation}
MMD^{2}(\mu,\rho_{t})\leq(1-t)MMD^{2}(\mu,\rho_{0})+tMMD^{2}(\mu,\rho_{1})-\int_{0}^{1}\Lambda_{\mu}(\rho_{s},v_{s})G(s,t)ds\label{eq:integral_lambda_convexity}
\end{equation}

where $\Lambda_{\mu}$ is given by \ref{eq:Lambda} and $G$ is given
by:
\[
G(s,t)=\begin{cases}
s(1-t) & s\leq t\\
t(1-s) & s\geq t
\end{cases}
\]
\end{corollary}
%
\begin{proof}
This is a direct consequence of the general identity (\cite{Villani:2009},
proposition 16.2). Indeed, for any continuous function $\phi$ on
$[0,1]$ with second derivative $\ddot{\phi}$ that is bounded below
in distribution sense the following identity holds:
\[
\phi(t)=(1-t)\phi(0)+t\phi(1)-\int_{0}^{1}\ddot{\phi}(s)G(s,t)ds
\]
Hence, one can choose $\phi(t)=MMD^{2}(\mu,\rho_{t})$ therefore,
it follows that:
\[
MMD^{2}(\mu,\rho_{t})=(1-t)MMD^{2}(\mu,\rho_{0})+tMMD^{2}(\mu,\rho_{1})-\int_{0}^{1}\frac{d^{2}MMD^{2}(\mu,\rho_{s})}{ds^{2}}G(s,t)ds
\]
Now using the inequality from \ref{prop:lambda_convexity}, \ref{eq:integral_lambda_convexity}
follows directly. 
\end{proof}
%
\begin{corollary}
\label{cor:loser_bound}Assume the distributions are supported on
$\mathcal{X}$ and the kernel is bounded, i.e: $\sup_{x,y\in\mathcal{X}}\vert k(x,y)\vert<\infty$.
Then the following holds:

\[
MMD^{2}(\mu,\rho_{t})\leq(1-t)MMD^{2}(\mu,\rho_{0})+tMMD^{2}(\mu,\rho_{1})+t(1-t)K
\]
\end{corollary}
%
\begin{proof}
Recall the expression of $\Lambda_{\mu}(\rho_{s},v_{s}):$

\[
\Lambda_{\mu}(\rho_{s},v_{s})=\langle v_{t},(C_{\rho_{t}}-\lambda MMD(\mu,\rho_{t})I)v_{t}\rangle_{L_{2}(\rho_{t})}\geq-\lambda MMD(\mu,\rho_{t})\Vert v_{t}\Vert_{L_{2}(\rho_{t})}^{2}
\]
However, $MMD(\mu,\rho_{t})\leq4C$ where $C=\sup_{x,y\in\mathcal{X}}\vert k(x,y)\vert$
. Moreover, if $\rho_{t}$ is a constant speed geodesic then $\Vert v_{t}\Vert_{L_{2}(\rho_{t})}^{2}=W_{2}^{2}(\rho_{0},\rho_{1})$,
hence: 
\[
-\int_{0}^{1}\Lambda_{\mu}(\rho_{s},v_{s})G(s,t)ds\leq4\lambda CW_{2}^{2}(\rho_{0},\rho_{1})\leq2t(1-t)\lambda Cdiam(\mathcal{X})^{2}
\]
where $diam(\mathcal{X})$ is the diameter of $\mathcal{X}$. The
rest of the proof follows by directly using \ref{cor:integral_lambda_convexity}
and by setting $K=2\lambda Cdiam(\mathcal{X})$.
\end{proof}
%
\ref{cor:loser_bound}, is a loser bound and doesn't account for local
convexity of the MMD. However, it allows to state the following result,
which is inspired from (\cite{Bottou:2017}, Theorem 6.3) but generalizes
it to the case of 'almost convex' functionals.
\begin{proposition}

(Almost convex optimization). Let $\mathcal{P}$ be a closed subset
of $\mathcal{P}(\mathcal{X})$ which is displacement convex. Then
for all $M>\inf_{\rho\in\mathcal{P}}MMD^{2}(\mu,\rho)+K$, the following
holds:
\end{proposition}
\begin{enumerate}
\item The level set $L(\mathcal{P},M)=\{\rho\in\mathcal{P}:MMD^{2}(\mu,\rho)\leq M\}$
is connected
\item For all $\rho_{0}\in\mathcal{P}$ such that $MMD^{2}(\mu,\rho_0)>M$
and all $\epsilon>0$, there exists $\rho\in\mathcal{P}$ such that
$W_{2}(\rho,\rho_{0})=\mathcal{O}(\epsilon)$ and
\[
MMD^{2}(\mu,\rho)\leq MMD^{2}(\mu,\rho_{0})-\epsilon(MMD^{2}(\mu,\rho_{0})-M).
\]
\end{enumerate}
%
\begin{remark}
This results means it is possible to optimize the cost function $\rho\mapsto MMD^{2}(\mu,\rho)$
on $\mathcal{P}$ as long as the barrier $\inf_{\rho\in\mathcal{P}}MMD^{2}(\mu,\rho)+K$
is not reached. We provide now a simple proof of this result:
\end{remark}
%
\begin{proof}
The proof is very similar to (\cite{Bottou:2017}, Theorem 6.3 and
Theorem 6.9): 
\end{proof}
\begin{enumerate}
\item First choose $\rho_{1}\in\mathcal{P}$ such that $MMD^{2}(\mu,\rho_{1})<M-K$.
For any $\rho_{0},\rho_{0}'\in L(\mathcal{P},M)$ there exist a displacement
geodesic joining $\rho_{1}$ and $\rho_{0}$ without leaving $\mathcal{P}$,
since $\mathcal{P}$ is by assumption discplacement convex. By \ref{cor:loser_bound}
we have:
\begin{align*}
MMD^{2}(\mu,\rho_{t}) & \leq(1-t)MMD^{2}(\mu,\rho_{0})+tMMD^{2}(\mu,\rho_{1})+t(1-t)K\\
 & \leq(1-t)M+t(M-K)+t(1-t)K\leq M-t^{2}K\leq M
\end{align*}
Hence $\rho_{t}\in L(\mathcal{P},M)$. The same can be done for a
path joining $\rho_{0}'$ and $\rho_{1}$. Hence we can find a path
in $L(\mathcal{P},M)$ joining $\rho_{0}$ and $\rho_{0}'$ , which
means that the level set $L(\mathcal{P},M)$ is connected.
\item Consider now $\rho_{1}\in L(\mathcal{P},M-K)$, note that such an
element exists since $M>\inf_{\rho\in\mathcal{P}}MMD^{2}(\mu,\rho)+K$.
By convexity of $\mathcal{P}$ there exists a constant speed geodesic
$\rho_{t}$ connecting $\rho_{0}$ and $\rho_{1}$. Since it is a
constant speed curve then one has:
\[
W_{2}(\rho_{0},\rho_{t})\leq tW_{2}(\rho_{0},\rho_{1}).
\]
But we also have by \ref{cor:loser_bound}:
\begin{align*}
MMD^{2}(\mu,\rho_{t}) & \leq(1-t)MMD^{2}(\mu,\rho_{0})+tMMD^{2}(\mu,\rho_{1})+t(1-t)K\\
 & \leq MMD^{2}(\mu,\rho_{0})-t(MMD^{2}(\mu,\rho_{0})-M+tK)\\
 & \leq MMD^{2}(\mu,\rho_{0})-t(MMD^{2}(\mu,\rho_{0})-M)
\end{align*}
Here we simply used the fact that $\rho_{1}\in L(\mathcal{P},M-K)$. 
\end{enumerate}
%

\begin{remark}
	A possible direction would be to directly leverage the tighter inequality in \ref{eq:integral_lambda_convexity} to get a better description of the loss landscape.
\end{remark}



\subsubsection*{References}
\renewcommand\refname{\vskip -1cm}
\bibliographystyle{apalike}
\bibliography{biblio}

\newpage
\section{Appendix}

\subsection{SDE and stochastic processes}

Consider the Itô process, i.e. the stochastic process:
\begin{equation}
dX_t=g(X_t)dt
\end{equation}
Let $f \in \mathcal{C}^2(\Omega)$, Itô's formula can be written:
\begin{equation*}
df(X_t)=\nabla f(X_t).g(X_t)dt
\end{equation*}
Let $\rho_t$ be the distribution of the process $X_t$. We have:
\begin{align*}
\E[\frac{df}{dt}(X_t)]&= \E[\nabla f(X_t).g(X_t)]\\
\Longleftrightarrow \int f(X) \frac{d \rho_t}{dt}(X)&=-\int f(X)div(g(X)\rho_t(X))
\end{align*}
where the second line is obtained by integrating by parts on both sides of the equality. Finally, the distribution $\rho_t$ verifies: 
\begin{equation*}
\frac{d\rho_t}{dt}=div(g\rho_t)
\end{equation*}

\end{document}